<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:discourse="http://www.discourse.org/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Optimization - AltRebol</title>
    <link>https://forum.rebol.info/c/development/optimization/53</link>
    <description>Topics in the &#39;Optimization&#39; category This is a category for discussing performance and optimization ideas.</description>
    
      <lastBuildDate>Tue, 21 Nov 2023 05:02:05 +0000</lastBuildDate>
      <atom:link href="https://forum.rebol.info/c/development/optimization/53.rss" rel="self" type="application/rss+xml" />
        <item>
          <title>Executable Size circa 2023...and tweaking INLINE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>A modern Ren-C non-debug executable on Linux--with https and the libuv filesystem and networking code (which supports asynchronous file I/O etc.) is about 1.7 MB when it is built at an O2 level of optimization (optimize for speed).</p>
<p>When built at Os optimization it's about 1.2 MB, sacrificing 40% of the speed to get the compression.  (In the modern era, most people would say that the extra size isn't a big deal to get that much of a speed improvement.)</p>
<p>By comparison, an R3-Alpha Linux executable is about 0.56 MB at O2.  And a Red CLI-only binary on Linux is about 1.0 MB.</p>
<h2>
<a name="why-has-size-gone-up-1" class="anchor" href="https://forum.rebol.info#why-has-size-gone-up-1"></a>Why Has Size Gone Up?</h2>
<p>I've looked under the hood at the differences with R3-Alpha to see what accounts for the disparity with modern Ren-C.  libuv accounts for a couple 100k, and is worth it--it would be especially so if taking advantage of things like the async file I/O.</p>
<p>But the rest just generally comes down to the fact that it's about twice as much code.  If you enjoy using ADAPT or ENCLOSE or SPECIALIZE, well, there's code that implements it.  And it's a deeper, safer, far more advanced codebase that just does more.</p>
<h2>
<a name="i-actually-pared-out-about-600k-by-tweaking-inlining-2" class="anchor" href="https://forum.rebol.info#i-actually-pared-out-about-600k-by-tweaking-inlining-2"></a>I Actually Pared Out About 600K By Tweaking Inlining</h2>
<p>When I started looking at size, the O2 binary was like 2.4 MB.  That was more than I expected, so I decided to look under the hood into why.</p>
<p>I used Google's tool <a href="https://github.com/google/bloaty">Bloaty McBloatface</a> to get some insight, and to my surprise...some rather small functions had a disproportionate amount of code attributed to them.</p>
<p>It turned out that this was due to putting functions in header files and inlining them with <code>static inline</code>.  When I moved 5 of these functions into the .c files instead of the .h files, that saved 400k in one blow... and the executable only got 0.4% slower (four tenths of a percent) as a result.</p>
<p>Then I managed to make it so the C++ build was about 140K lighter by changing the <code>static inline</code> on the remaining functions to a macro of INLINE that's either <code>inline</code> in the C++ build, or <code>static inline</code> in the C build.</p>
<p>I guess the takeaway here is that even if you notice that something is getting bigger due to good reasons of having more code, it always pays to look under the hood a bit when you can.  A few hours of work can get some low-hanging fruit.</p>
<p>(Another takeaway is that being able to build a C codebase as C++--if you want to--continuously pays dividends...)</p>
<p>Here's some notes on the INLINE macro:</p>
<pre><code class="lang-auto">
//=//// INLINE MACRO FOR LEVERAGING C++ OPTIMIZATIONS /////////////////////=//
//
// "inline" has a long history in C/C++ of being different on different
// compilers, and took a long time to get into the standard.  Once it was in
// the standard it essentially didn't mean anything in particular about
// inlining--just "this function is legal to appear in a header file and be
// included in multiple source files without generating conflicts."  The
// compiler makes no particular promises about actually inlining the code.
//
// R3-Alpha had few inline functions, but mostly used macros--in unsafe ways
// (repeating arguments, risking double evaluations, lacking typechecking.)
// Ren-C reworked the code to use inline functions fairly liberally, even
// putting fairly large functions in header files to give the compiler the
// opportunity to not need to push or pop registers to make a call.
//
// However, GCC in C99 mode requires you to say `static inline` or else you'll
// get errors at link time.  This means that every translation unit has its
// own copy of the code.  A study of the pathology of putting larger functions
// in headers as inline with `static inline` on them found that about five
// functions were getting inlined often enough to add 400K to the executable.
// Moving them out of .h files and into .c files dropped that size, and was
// only about *0.4%* slower (!) making it an obvious win to un-inline them.
//
// This led to experimentation with C++ builds just using `inline`, which
// saved a not-insignificant 8% of space in an -O2 build, as well as being ever
// so slightly faster.  Even if link-time-optimization was used, it still
// saved 3% on space.
//
// The long story short here is that plain `inline` is better if you can use
// it, but you can't use it in gcc in C99 mode (and probably not other places
// like TinyC compiler or variants). So this clunky INLINE macro actually
// isn't some pre-standards anachronism...it has concrete benefits.
//
#if CPLUSPLUS_11
    #define INLINE inline
#else
    #define INLINE static inline
#endif
</code></pre>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061</link>
          <pubDate>Tue, 21 Nov 2023 05:02:05 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2061</guid>
          <source url="https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061.rss">Executable Size circa 2023...and tweaking INLINE</source>
        </item>
        <item>
          <title>Intrinsics: Functions without Frames</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Redbol's historical type system really had only one design point: <em>be fast</em>.  There were 64 fundamental datatypes, and parameters of a function could either accept each datatype or not.  So a simple bitset of 64 bits was stored alongside each parameter, and checked when the function was called.  That was it.</p>
<p>Ren-C's richer design explodes the number of "types" in the system.  Isotopes like <strong><code>~null~</code></strong> are variations on WORD!, but you don't want every function that takes a WORD! to take nulls...and you don't want to have the type checking be so broad as to take <strong><code>[isotope!]</code></strong> just because you want to be able to take nulls (because that would include splices, packs, etc.)</p>
<p>It's not just this reason that Redbol's type checking was too simple, but it forced my hand in coming up with some sort of answer.  <em>I couldn't think of any better idea than Lisp, which does type checking via functions ("predicates").</em>  So I rigged it up where if you want to say a function can take an integer or null, you can write <strong><code>[null? integer!]</code></strong>  You can freely mix LOGIC-returning functions with fundamental types, and we're no longer stuck with the 64 fundamental type limit.</p>
<h2>
<a name="isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1" class="anchor" href="https://forum.rebol.info#isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1"></a>Isn't It Slow To Call A List of Functions For Typechecking?</h2>
<p>It can be.  And in particular, it can be if you have to go through calling those functions twice.</p>
<p>Why twice?  Because of "coercion".  For example, if you pass a pack to a function that expects packs, you'll get the meta-pack:</p>
<pre><code>&gt;&gt; foo: func [^x [pack?]] [probe x]

&gt;&gt; foo pack [1 "hi"]
~['1 '"hi"]~
</code></pre>
<p>But if your function didn't want packs, but wanted the type the pack decays to, it has to work for that as well:</p>
<pre><code>&gt;&gt; bar: func [^x [integer?]] [probe x]

&gt;&gt; bar pack [1 "hi"]
'1 
</code></pre>
<p><em>Did the function want the meta form or the meta-decayed form?</em>  There's no way of knowing for sure in advance.  The method chosen is to offer the meta form first, and if that doesn't match then the decayed form is offered.</p>
<p>It didn't know before walking through the block of functions to typecheck that a pack wouldn't have been accepted.  So it had to go through offering the pack, and then offering the integer.</p>
<h2>
<a name="but-i-noticed-something-about-these-functions-2" class="anchor" href="https://forum.rebol.info#but-i-noticed-something-about-these-functions-2"></a>But I Noticed Something About These Functions...</h2>
<p>Typically these functions are very simple:</p>
<ul>
<li>
<p>They take one argument.</p>
</li>
<li>
<p>They can't fail.</p>
</li>
<li>
<p>They don't require recursive invocations of the evaluator.</p>
</li>
</ul>
<p>This led me to wonder how hard it would be to define a class of actions whose implementations were a simple C function with an input value and output value.  If you weren't in a scenario where you needed a full FRAME!, you could reach into the ACTION!'s definition and grab the simple C function out of it.  All these functions would use the same dispatcher--that would be a simple matter of proxying the first argument of a built frame to pass it to this C function.</p>
<p>I decided to call these <strong>"intrinsics"</strong>, which is named after a <a href="https://en.wikipedia.org/wiki/Intrinsic_function">trick compilers use</a> when they see certain function calls that they implement those functions via direct code inlining.  It's not a perfect analogy, but it's similar in spirit.</p>
<h2>
<a name="it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3" class="anchor" href="https://forum.rebol.info#it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3"></a>It Wasn't All That Hard To Implement (relatively speaking <img src="https://forum.rebol.info/images/emoji/twitter/roll_eyes.png?v=12" title=":roll_eyes:" class="emoji" alt=":roll_eyes:" loading="lazy" width="20" height="20"> )</h2>
<p>All of the native function implementations were assumed to have the same type signature, taking a frame as an argument.  I took away that assumption and added an /INTRINSIC refinement to the NATIVE function generator.  If it was an intrinsic, then the C function in the native table would take a single value argument and an output slot to write to.</p>
<p>So it's still one C function per native.  But if it's an intrinsic, then the function is not a dispatcher... the Intrinsic_Dispatcher() is used, and the C function is poked into the properties of the function.</p>
<p>Callsites that want to optimize for intrinsics just look to see if an action has the Intrinsic_Dispatcher(), and if so they have to take responsibility for procuring an argument and type checking it.  But if they do, they can just call the C function directly with no frame overhead.</p>
<p><strong>This helps make the switchover to functions in type spec blocks much more palatable.</strong>  It's never going to be as fast as the bitset checking, but it's fast enough to allow things to make progress.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/intrinsics-functions-without-frames/2050">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/intrinsics-functions-without-frames/2050</link>
          <pubDate>Sun, 15 Oct 2023 17:25:32 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2050</guid>
          <source url="https://forum.rebol.info/t/intrinsics-functions-without-frames/2050.rss">Intrinsics: Functions without Frames</source>
        </item>
        <item>
          <title>Is The Script Compression Feature Necessary?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>R3-Alpha introduced the option that when you SAVE a script, you can ask that it be compressed.</p>
<p>It doesn't compress the header...just the body of the script.  There were two options for how this body could be compressed after the header: either as a Base64 BINARY literal ("script compression"), or directly as gzip'd bits ("raw compression").</p>
<p>As an example:</p>
<pre><code>&gt;&gt; data: save/compress blank [1 &lt;two&gt; "three"] 'script
== #{
    5245424F4C205B0A202020204F7074696F6E733A205B636F6D70726573735D0A
    5D0A3634237B483473494141414141414141436A4E5573436B707A3764545543
    724A4B45704E56654943414E425746325951414141417D
}

&gt;&gt; print as text! data
REBOL [
     Options: [compress]
]
64#{H4sIAAAAAAAACjNUsCkpz7dTUCrJKEpNVeICANBWF2YQAAAA}

&gt;&gt; [body header]: load data
== [1 &lt;two&gt; "three"
]

&gt;&gt; body
== [1 &lt;two&gt; "three"
]

&gt;&gt; header
== make object! [
    Title: "Untitled"
    File: ~null~
    Name: ~null~
    Type: 'script
    Version: ~null~
    Date: ~null~
    Author: ~null~
    Options: [compress]
    Description: ~null~
]
</code></pre>
<h2>
<a name="rebol2-didnt-have-it-red-doesnt-have-it-1" class="anchor" href="https://forum.rebol.info#rebol2-didnt-have-it-red-doesnt-have-it-1"></a>Rebol2 Didn't Have It, Red Doesn't Have It...</h2>
<p>Arguments that it helps with transmitting over networks don't hold up much these days, because the HTTP protocol itself does compression.</p>
<p>Plus, keeping scripts in compressed form is an annoying form of opaqueness on a language that's supposed to be about simplicity.</p>
<p>I've kept it around just because there were tests for it, and it exercised compression code (including showcasing a really bad design method of trying to decompress garbage to see if it was the raw compressed form, causing a crazy memory allocation).  But I'm not sure what the compelling use case for this feature is.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044</link>
          <pubDate>Thu, 27 Jul 2023 23:20:14 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2044</guid>
          <source url="https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044.rss">Is The Script Compression Feature Necessary?</source>
        </item>
        <item>
          <title>Boot Footprint: Giant String Literal vs. Encap?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>One thing you can do with C is embed literal data.  This is how R3-Alpha ships with its mezzanine functions "built in", the prep process stores everything in a big compressed array of bytes called (misleadingly) <strong><code>Native_Specs</code></strong>:</p>
<p><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/b-init.c#L166">https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/b-init.c#L166</a></p>
<p>The name being <code>Native_Specs</code> might suggest it was the contents of <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/natives.r">%natives.r</a>.  But it's actually a lot more, with glued-together source code... including all of the contents of the <a href="https://github.com/rebol/rebol/tree/master/src/mezz">%base-xxx.r, %sys-xxx.r, and %mezz-xxx.r</a> files.  So I renamed it to <code>Boot_Block_Compressed</code>.</p>
<p>But it doesn't embed the files as-is... it LOADs them and SAVEs them using an already-built version of R3.  This round-tripping removes the comments and normalizes the spacing.  It also actually scrambled it with CLOAK for whatever reason--a waste of time because you could read all the code with SOURCE if you felt like it.  :-/</p>
<p><em>(Ren-C doesn't use an old-R3's LOAD+SAVE to strip out comments, because it would lock down the format.  Your hands would be tied on adding or changing lexical forms in the sys/base/mezzanine.  So it has its own STRIPLOAD function that does a light stripping out of comments and spaces for this glue-files-together purpose)</em></p>
<h2>Is Embedding Big Fat C Constants Supported By The Standard?</h2>
<p>C compilers are only <em>required</em> to allow you to build in string literals that are <a href="https://stackoverflow.com/a/11488687">509 characters in C89, and 4095 characters in C99</a>.  They can allow more, but don't have to.</p>
<p>So I recall R3-Alpha having problems when you turn up <code>--pedantic</code> warning levels by using a syntax like:</p>
<pre><code>const char Native_Specs[] = "\x01\x02\x03...";
</code></pre>
<p>That warning went away when I changed it to:</p>
<pre><code>const unsigned char Boot_Block_Compressed[] = { 0x01, 0x02, 0x03 ...};
</code></pre>
<p>Regarding the problem of hitting length limits, Ren-C actually breaks things up a bit more...because each extension has its own constant declaration like this for its Rebol portion.</p>
<p>Because this code is decompressed and scanned once--and then tossed--there's probably a number of experiments that could be done.  What if the blob were loaded as mutable data, and then used as some kind of buffer for another purpose?  Is there some way to help hint to the OS that you really are only going to use the information only once so it will throw out the page from memory?  Or will the right thing happen to scan it and use it just once?</p>
<p>Long story short--it hasn't been a problem, even with the TCC build.  So it has been taken for granted that it works acceptably.</p>
<h2>But Would Encapping Be Better?</h2>
<p>One vision of how the boot would work is that it would only load enough to get de-encapping working.  Then the de-encapping would be how all the blobs for the "built-in" extensions were extracted.</p>
<p><em>This seems like an interesting vision,</em> because if someone gave you a big fat Ren-C and you wanted any skinnier version, you could basically ask it to cut everything out you don't want and give you a new EXE.  You could roll it up with any customizations you like.</p>
<p>But if you're using any "real" form of encapping (e.g. manipulating the resource portions of a Linux ELF file or a Windows PE file) this gets complicated.  And Ren-C's encap facilities are <a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb">written in usermode</a>...so that expects things like file I/O and PARSE of a BINARY!, etc.  I also assume that unzip facilities would be part of encapping.  So you need a reasonably runnable system just to get to that point.</p>
<p><strong>I've punted on worrying too much about this, because of the focus on the web build.</strong></p>
<p>It would be a bad investment of limited resources to handwrite and maintain encapping code in C, just so that encapping can be the means by which more of the bootstrap can be done with encap.</p>
<h2>Script Code Is Easy to Encap, EXE/DLL Code Is Not</h2>
<p>So the "easy" part would be changing the build to go in two steps.</p>
<p>The first step would make an r3-one.exe that is capable of augmenting itself with encapped data.  The second step would ask that r3 to fold in various scripts and resources to make an r3-two.exe that had more things in it...such as a console.</p>
<p>This isn't that far out to accomplish.  <strong>The hard part is when what you're encapping isn't script data, but compiled and executable C code...like bits from a DLL.</strong>  e.g. encapping "extensions".</p>
<p>What some people do in this situation is to actually glue the DLL file into the executable, but extract it to the filesystem and load the extracted version.  If you Google around for "using a DLL as an embedded resource" you'll find people who've done such things...but the answers you find will be from over a decade ago, because no one cares about how they ship such things anymore.</p>
<h2>Making Encap A Dependency Is Probably Unwise...</h2>
<p>It isn't going to be a terribly big win for bootstrap if it can't be used to pull out or put in extensions.</p>
<p>I don't think it's wise to pursue handcrafted C de-encapping.  In fact there's no way I'd be writing any kind of encap code right now if it weren't already made.  Kind of the only reason we have the usermode encapping around is because Atronix was using it, but I was trying to keep the feature but cut it out of the C.  It hasn't been tossed entirely because it functions as test code.</p>
<p>We <em>could</em> make a token two-step build (the phase one executable, that uses the phase one to build a phase two with encapped data in it).</p>
<p>But it seems what we might want more is an easy option to not build in encapping whatsoever, and have more control over options at build time than the current list of extensions.</p>
<p>For the limited audience looking at desktop builds--I imagine the answer will be that if you want a differently-sized r3.exe, you do it with a C compiler and ticking different boxes.  Or you build everything as a DLL and accept it's not all one file.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977</link>
          <pubDate>Sat, 24 Sep 2022 01:57:39 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1977</guid>
          <source url="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977.rss">Boot Footprint: Giant String Literal vs. Encap?</source>
        </item>
        <item>
          <title>Influences On Startup Time And Memory Use</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Right now it's not ideal to be focusing on things like startup time and memory use.  There are a lot of fundamental features being rethought--and recall that rules of optimizing code at the cost of clarity and flexibility of design are:</p>
<ul>
<li>
<p>Rule <span class="hashtag">#1:</span> Don't Do It</p>
</li>
<li>
<p>Rule <span class="hashtag">#2</span> (Experts Only) Don't Do It... Yet.</p>
</li>
</ul>
<p>...BUT, the issues can't be ignored forever.  And it's reasonable for one to ask why there's been a dramatic increase in boot time and memory use between the build being used for bootstrap and a current commit.</p>
<p>So it's worth having a thread here to track some of what's involved.</p>
<h2>ENCAP Detection</h2>
<p>By default we still run encap detection on all desktop builds, scanning the executable.  On Windows I think Shixin's version loads the whole binary into memory, and on Linux it still does quite a lot.</p>
<p>You can skip the detection by using <code>--no-encap</code>.</p>
<p><a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb">https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb</a></p>
<p>But the encap and de-encapping tools will still be bundled in the executable.  They're not an extension, so if you don't want to pay for that...you need to entirely remove <a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977">early-boot modules like encap and unzip</a> which are <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/src/main/prep-main.reb#L45">built in another way</a></p>
<p>Obviously platform-specific C code would be faster and lighter than PARSE.  And there was some before, but it entangled things in the core with FILE I/O...and it was dedicated finicky C for a purpose we're not really focusing on, especially in the web build.</p>
<p>The decision to move encapping to userspace tools was mine, and not something I regret.  But since we're not using it, all it's really doing is acting as a test.  I've made a separate thread to talk about the fate of Encap, and whether we should depend on it more or distance from it further:</p>
<p><a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977" class="inline-onebox">Boot Footprint: Giant String Literal vs. Encap?</a></p>
<h2>A Big Cost Is Going To Come From UPARSE</h2>
<p>UPARSE right now is an elaborate exercise of the ability to build complex feature-filled dialects in userspace.  And it does so at great cost to the evaluator.</p>
<p>Of course the plan is to cut that down, because COMBINATORs are just functions.  They could be written as natives.  And even more importantly, the process of <em>combinating itself</em> needs to be native.</p>
<p>I have done some experiments with this:</p>
<p><a href="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636" class="inline-onebox">Progress on Nativizing Parser Combinators</a></p>
<p>But those experiments are currently inactive, because the design needed more work.  And it's easier to churn through that work with userspace code.</p>
<p>What can we do about it? Well until UPARSE goes through an optimization phase, we can just use PARSE3 in boot...or at least for whatever subsetted codebase is in this metric.  The main thing is just to get it measured so we know how much of this is known UPARSE-ism vs. other unknowns.  I'm going to bet it's a lot...even though it's not used all that terribly much in boot, it's going to be big.</p>
<p>Cutting it out for the moment would at least help focus on the next bigger things.</p>
<h2>Another Pain Point Is Going to be GET+SET Atop PICK+POKE</h2>
<p>I spent quite a while working through what a GET and SET and PICK and POKE actually were.  Ultimately I concluded:</p>
<ul>
<li>
<p>GETs are just sequences of individual PICK steps (where a GET of a WORD! starts the chain with the binding of the word, and PICKs the word out of that object)</p>
</li>
<li>
<p>SETs are a sequence of PICK steps which are kept track of...followed by POKE.  That POKE can return nothing (in which case you're done) or it can return an adjusted value.  If the value needed to be adjusted that means it then gets POKE'd back into the cell back in the chain, and this ripples back so long as the bitpattern in cells need to be adjusted.</p>
</li>
</ul>
<p>I haven't gone back to this prototype and optimized it.  That means it quite literally is building evaluation chains of PICK and POKE every time it does tuple processing (what would be "path picking", e.g. variables out of objects).  I wasn't sure if this was the answer or not, so it seemed best to keep it general to be able to play with it.</p>
<p>It's tough to know how much "hardening" should be done on this.  It's nice to be able to hijack and hook and bend things.  I think I still want to consider it to be calls to PICK and POKE, but we can do those calls via frames built just for those functions...and not generic evaluation.  I'll have to look at it.</p>
<h2>Each Extension Adds Memory Use, But Also Has Startup Code</h2>
<p>By default the desktop includes every extension, even for making animated GIFs...as well as currently</p>
<p>If one wants to make a non-kitchen-sink test build of Ren-C...obviously use <code>debug: none</code>, and <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/configs/default-config.r#L23">chopping extensions out with <strong><code>-</code></strong> instead of <strong><code>+</code></strong></a>, for starters.  Note that extensions can be <a href="https://github.com/metaeducation/ren-c/actions/runs/3056527403/jobs/4930783888#step:22:3">built as separate DLL/.so with <strong><code>*</code></strong></a></p>
<h2>Other Factors Need Managing On a Case-by-Case Basis</h2>
<p>Those would be among the only things that can be done without <em>some</em> attention to the C, which hasn't been vetted for this metric in years.  But it isn't a priority right at this exact moment--there are much more important things.</p>
<p><em>(If you want some of my general philosophy about why Ren-C will be competitive with R3-Alpha despite "increased complexity", then seeing some <a href="https://forum.rebol.info/t/the-now-even-more-special-specialize/588">old stats on SPECIALIZE might be illuminating</a>)</em></p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972</link>
          <pubDate>Sun, 18 Sep 2022 22:11:11 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1972</guid>
          <source url="https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972.rss">Influences On Startup Time And Memory Use</source>
        </item>
        <item>
          <title>Rethinking The Stale Bit: Invisibility In The Isotopic Age</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Every evaluation step is asked to target an output cell.  Before the evaluation, a single bit is set on that cell to say it is "stale".  So if the evaluation doesn't write to it--and confirms that wasn't a mistake--then the old result is left around to recover.</p>
<p>The ability to recover the previous result with the flip of a bit is used for invisibility.  It's used not just in the evaluator when it goes step by step, but also in things like ANY and ALL.</p>
<pre><code>&gt;&gt; 1 + 2 comment "in the evaluator"
== 3

&gt;&gt; all [1 + 2 comment "here too"]
== 3
</code></pre>
<p><em>Doing it this way has sacrificed some features.</em>  For instance, you can't make an "invisible" enfix function:</p>
<pre><code>&gt;&gt; foo: enfix func [left] [
       print ["Left was" left]
       return void
   ]

&gt;&gt; 304 1020 foo
Left was 1020
== 1020
</code></pre>
<p>The 1020 from the previous evaluation was used as an argument.  But after "consuming all its arguments" the product of FOO could not leave the 304 there.  If each evaluation step was to a temporary cell, that temporary cell could be used to fill the enfix slot of FOO... and 304 could be left.</p>
<p><em>Doing it this way has also required acrobatics to accomplish non-negotiable features.</em>  The related problem of making it possible for an enfix function to perceive voidness on the left hand side requires stale bit mechanics that aren't for the faint of heart... e.g. to differentiate these two cases:</p>
<pre><code>&gt;&gt; (else [print "Won't work"])
** Error

&gt;&gt; () else [print "Will work"]
Will work
</code></pre>
<h2>Is All The Bit-Fiddling Worth It Vs. Copying?</h2>
<p>Considering the small size of cells (4 platform pointers), the logic to test and clear the "stale" bit may seem to add overhead and complexity that isn't saving that much.  Instead, every evaluation could be done into a temporary slot...and then if not invisible, the 4 pointers could be moved.</p>
<p>This is actually a bit misleading--because copying cells is actually a bit more expensive in the general case.  Cell format flags have to be checked, bindings may need to be managed, and if a reference count mechanic is implemented this could make it all worse.</p>
<p>Less copying is desirable, and it seems neat to have achieved invisibility thus far without needing an extra eval per-eval-step.</p>
<blockquote>
<p><em>"So if the evaluation doesn't write to it--and confirms that wasn't a mistake..."</em></p>
</blockquote>
<p>This is one of the main reasons I've stuck with the current method.  It's useful for debug purposes to know if a native just forgot to write an output cell anyway.  So I figured: <em>"so long as the output cell is going to have a flag on it saying it hasn't been written to yet, why not make that flag able to coexist with the previous value...and hence avoid a mechanic of needing to copy every time?"</em></p>
<h2>But Isotopes Mean It's Time For Change</h2>
<p>Early on I observed that there was no way to get this to work:</p>
<pre><code>&gt;&gt; 1000 + 20 if true [comment "hi"]
== 1020  ; not possible
</code></pre>
<p>The IF had to produce something as a proxy for VOID that wasn't void... in order to signal a taken branch (we want THEN to run).</p>
<p>But even if that proxy was able to <em>decay</em> to a void state, it was too late.  It had overwritten the output.  Today that proxy is a parameter pack with a meta-void in it: <strong><code>~[~]~</code></strong>.</p>
<p>There's more stuff with parameter packs that <em>should</em> work, like this:</p>
<pre><code> &gt;&gt; 1000 + 20 [x @y]: pack [304 void]
 == 1020
</code></pre>
<p><em>And isotopic objects that represent lazy evaluations should be able to produce void, too.</em>  They're a proxy for behavior, and if you pick and choose behaviors that could be accomplished with a normal result that a REIFY method on a lazy object can't, you're saying they're not as powerful.</p>
<p><strong>These features tip the scales.</strong>  And really, the circuitous nature of void enfix handling was already tipping them.</p>
<p>The concerns over copying are mostly addressed by something I'm calling "cell movement"; this means we can really get closer to the 4 platform pointer copies, because you're destroying the old cell in the process.  So if techniques like reference counting came along, you're not adding and removing them--you're just letting the new cell take over the resources of the old.</p>
<p>Plus, detecting whether a cell has been written to or not is a generic debug feature now that has easy coverage.</p>
<p><em>The stale bit is thus on the chopping block.</em>  So expect more robust void-related behavior coming soonish.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963</link>
          <pubDate>Thu, 08 Sep 2022 21:34:36 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1963</guid>
          <source url="https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963.rss">Rethinking The Stale Bit: Invisibility In The Isotopic Age</source>
        </item>
        <item>
          <title>Incomplete TRANSCODEs: Actually an Optimization Problem</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p><strong>Ren-C has a very slick multi-return interface for TRANSCODE.</strong>  The mere request of a "remainder" of data left indicates you're not trying to do a full scan.</p>
<p>Without the request, you get the whole thing:</p>
<pre><code>&gt;&gt; transcode "abc def"
== [abc def]

; ...or...

&gt;&gt; value: transcode "abc def"
== [abc def]

; ...or...

&gt;&gt; [value]: transcode "abc def"
== [abc def]
</code></pre>
<p>With the next position request, just one item and a remainder:</p>
<pre><code>&gt;&gt; [value pos]: transcode "abc def"
== abc

&gt;&gt; pos
== " def"

; ...or...

&gt;&gt; transcode/next "abc def" 'pos
== abc

&gt;&gt; pos
== " def"
</code></pre>
<p>You also know that you're at the end of the input when it returns null, with all the benefits of easy reactions to NULL with IF and ELSE and friends:</p>
<pre><code>&gt;&gt; [value pos]: transcode ""
; null
</code></pre>
<p>Writing foolproof loops to process items are a breeze:</p>
<pre><code>while [true]
    [item utf8]: transcode utf8 else [break]
    print mold item
 ]

; or for the THEN/ELSE haters out there (you know who you are :-P)

while [true]
    if null? [item utf8]: transcode utf8 [
        break
    ]
    print mold item
 ]
</code></pre>
<p><img src="https://forum.rebol.info/images/emoji/twitter/+1.png?v=9" title=":+1:" class="emoji only-emoji" alt=":+1:"></p>
<h2>This Runs Circles Around Red and R3-Alpha</h2>
<p>For starters: neither support strings as input--because the scanner is built for reading UTF-8 files...and both R3-Alpha and Red unpack strings into fixed-width encodings.  So if you have string input, you have to pay for a copy encoded as UTF-8 via TO BINARY!.  (<a href="https://forum.rebol.info/t/realistically-migrating-rebol-to-utf8-everywhere/374">Ren-C's UTF-8 Everywhere</a> wins again!)</p>
<p>R3-Alpha unconditionally returns a block with the last element as a remainder, whether you ask for one item via /NEXT or not:</p>
<pre><code>r3-alpha&gt;&gt; transcode to binary! "abc def"
== [abc def #{}]

r3-alpha&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

r3-alpha&gt;&gt; transcode/next to binary! ""
== [#{}]
</code></pre>
<p>So if you were transcoding an entire input, you have to TAKE/LAST an always-empty binary off of the result.</p>
<p>But you are using /NEXT you have to PICK out the element from the start of the array and the remainder from the end.  But you need to notice the exception of no-value-produced where the block is length 1 instead of 2.</p>
<p>That's awkward, but as usual... <em>Red somehow manages to make an incompatible interface that is as much worse as it is better:</em></p>
<p>The better part is that if you don't ask for /NEXT you just get the block back, like in Ren-C:</p>
<pre><code>red&gt;&gt; transcode to binary! "abc def"
== [abc def]
</code></pre>
<p>But the /NEXT interface is outright broken:</p>
<pre><code>red&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

red&gt;&gt; transcode/next to binary! ""
== [[] #{}]
</code></pre>
<p>It might look better because you don't have to guess about which position to find the remainder in--it's always in the second slot.  But it has a fatal flaw: you can't distinguish the result state of scanning <code>"[]"</code> and any string with nothing but comments and whitespace.</p>
<p>Consider this very basic loop to scan one item at a time and print it:</p>
<pre><code>red&gt;&gt; utf8: to binary! "abc def"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print mold item
]
abc
def
</code></pre>
<p>You get two items.  But what if you had something that was--say--a comment:</p>
<pre><code>red&gt;&gt; utf8: to binary! "; I'm just a comment"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print mold item
]
[]
</code></pre>
<p>You get one spurious item.  (They chose BLOCK! for the item, but it wouldn't matter what it was--a NONE! would be just as bad, you're just losing the distinction between empty strings and <code>"#[none]"</code> then.)</p>
<p>If I were prescribing a solution for Red I'd say:</p>
<ul>
<li>
<p>Make /NEXT take a variable to write the next position into</p>
</li>
<li>
<p>Error on <code>#{}</code> input, so anyone doing a TRANSCODE/NEXT knows they are responsible for testing for TAIL? before they call (if they're not sure their input is non-empty)</p>
<ul>
<li>This way an empty remainder returned in the /NEXT variable will uniquely signal the reached-end state</li>
</ul>
</li>
<li>
<p>Make the synthesized product at the tail something ugly but assignable (so not an unset!)</p>
<ul>
<li>an ERROR! saying "end of input" is at least informative in case it winds up getting treated as an actual value somewhere</li>
</ul>
</li>
</ul>
<p>That would at least give them patterns like:</p>
<pre><code>if not tail? utf8 [  ; needed if you're not sure it's non-empty
    while [true] [
        item: transcode/next utf8 'utf8
        if tail? utf8 [break]
        print mold item
    ]
]
</code></pre>
<p><em>(Having NULL is clearly better as a non-valued state (with isotope states fleshing out the picture)...and my <a href="https://forum.rebol.info/t/why-or-why-not-have-unset-in-rebol-like-languages/113/2">bafflement at Nenad's blindness</a> when confronted with what should be clear as day is as relevant as it was 6-plus-years ago.  And of course we see they'd actually need SET/ANY to do this right if you were allowing #[unset!]...but Ren-C's design has no need of that.)</em></p>
<h2>Ren-C Also <em>Thrashes</em> R3-Alpha and Red In Error Handling</h2>
<p>Ren-C TRANSCODE has these potential behaviors:</p>
<ul>
<li>
<p>RETURN a BLOCK! (if plain TRANSCODE)</p>
</li>
<li>
<p>RETURN an ANY-VALUE! or NULL (if TRANSCODE/NEXT)</p>
</li>
<li>
<p>It can do a "hard FAIL"</p>
<ul>
<li>
<p>This would happen if you asked something fundamentally incoherent...like asking to TRANSCODE a with input that was non-UTF-8...like a GOB!, or something like that</p>
</li>
<li>
<p>Such errors are only interceptible by a special SYS.UTIL.ENTRAP method--they are not supposed to be easy to gloss over and unlikely to have meaningful mitigation.  So only special sandboxing situations (like writing consoles that print out the error) are supposed to trap them.</p>
</li>
</ul>
</li>
<li>
<p>It can RETURN an <em>isotopic ERROR!</em> ("raised error") if something went wrong in the transcoding process itself</p>
<ul>
<li>
<p>This would be something like a syntax error, like  if you asked <strong>transcode "a bc 1&amp;x def"</strong></p>
</li>
<li>
<p>These will be promoted to a hard FAIL if the immediate caller doesn't do something to specially process them.</p>
</li>
<li>
<p>You can casually ignore or intercept these, because you can be confident that it was a formal return result of the thing you just called--not some deeper problem like a random typo or other issue.</p>
</li>
</ul>
</li>
</ul>
<p>I won't rehash the entire <a href="https://forum.rebol.info/t/fail-vs-return-raise-the-new-age-of-definitional-failures/1852">"why definitional errors are foundational"</a> post, but TRANSCODE was one of the first functions that had to be retrofitted to use them.</p>
<pre><code>&gt;&gt; transcode "a bc 1&amp;x def" except e -&gt; [print ["Error:" e.id]]
Error: scan-invalid
</code></pre>
<p><strong>The definitionality is extremely important!</strong>  I spent a long time today because in the bootstrap shim I had a variation of transcode...parallel to this in R3-Alpha:</p>
<pre><code>r3-alpha&gt;&gt; transcode: func [input] [
               prnit "My Transcode Wrapper"  ; oops, typo
               return transcode input
           ]

r3-alpha&gt;&gt; if not attempt [transcode to binary! "abc def"] [print "Bad input"]
Bad input
</code></pre>
<p><em><strong>But the input isn't bad!!!</strong></em>  This leads to a nightmare of trying to figure out what was going wrong.  I had just one of those nightmares today in the bootstrap executable when tinkering with the shim implementation of TRANSCODE.  A bug in the shim was leading to silently skipping work that should have been done, because the caller wanted to be tolerant of bad transcode input.</p>
<p>There's simply no practical way of working on code of any complexity without something like definitional failures, and experience has proven this day after day.</p>
<h2>Getting Incomplete Results Via R3-Alpha's /ERROR</h2>
<p>R3-Alpha offered this feature:</p>
<pre><code>/error -- Do not cause errors - return error object as value in place
</code></pre>
<p>The intended use is that you might want the partial input of what had been successfully scanned so far.  If the code went and raised an error, you could trap that error.  But you wouldn't have any of the scanned items.</p>
<p>It would put it any ERROR! as the next-to-last item in the block, with the remainder after that:</p>
<pre><code>&gt;&gt; transcode/error to binary! "a bc 1&amp;x def"
== [abc make error! [
    code: 200
    type: 'Syntax
    id: 'invalid
    arg1: "pair"
    arg2: "1&amp;x"
    arg3: none
    near: "(line 1) a bc 1&amp;x def"
    where: [transcode]
] #{20646566}]

&gt;&gt; to string! #{20646566}
== " def"  ; wait...why isn't 1&amp;x part of the "remainder"
</code></pre>
<p>It's clumsy to write the calling code (or to read it...testing to see if the next-to-last-item is an ERROR! and reacting to that.</p>
<p><em>(Also: What if there was some way to represent ERROR! values literally in source?  This would conflate with such a block that was valid...but just incidentally had an ERROR! and then a BINARY! in the last positions.)</em></p>
<p>But the thing that had me most confused about it was the remainder.  Notice above you don't get  <code>1&amp;x</code> as the start of the stuff it couldn't understand.</p>
<p>Was it trying to implement some kind of recoverable scan?  What would that even mean?  <img src="https://forum.rebol.info/images/emoji/twitter/thinking.png?v=9" title=":thinking:" class="emoji" alt=":thinking:"></p>
<p><strong>Ultimately I think this was just a leaking of an implementation detail as opposed to any reasonable attempt at recoverable scanner.</strong>  It only didn't tell you where the exact tail of the successfully scanned material was because it did not know.</p>
<p>The scanning position is based on token consumptions, and so if you started something like a block scan and it saw a <strong>[</strong> then it forgets where it was before that.  Then if something inside the block goes bad, it will just give you a remainder position somewhere inside that--<em>completely forgetting about how many nesting levels it was in</em>.</p>
<p>So what you were getting was a crappier implementation of scanning one by one, and remembering where you were before the last bad scan:</p>
<pre><code>pos: input
error: null
block: collect [
   while [true] [
       keep [# pos]: transcode pos else [
           break
       ] except e -&gt; [
           error: e
           break
       ]
   ]
]
</code></pre>
<p>That gives you a proper version, setting error if something happened and giving you the block intact.</p>
<h2>So Finally... We See It's An Optimization Problem</h2>
<p>Question is if there's some way of folding this into TRANSCODE, so it's doing the looping and collecting efficiently for you.</p>
<p>But this interface wants to get back a "remainder".  And I kind of hate to sacrifice the property that TRANSCODE's asking for a remainder means scan one element.  :-/</p>
<p>I guess we <em>could</em> say that there's a logical process you follow:</p>
<ul>
<li>
<p>The output parameter is called REST (instead of NEXT)</p>
</li>
<li>
<p>An additional output parameter is added for ERROR</p>
</li>
<li>
<p>If you ask for the REST and <em>don't</em> ask for an ERROR, that suggests you want to encode a single item</p>
<ul>
<li>
<p>You could have just intercepted the error if you wanted it</p>
</li>
<li>
<p>Nothing is lost because there wouldn't be any partial results to miss (if you're only doing one item, there will always be zero items completed before it)</p>
</li>
</ul>
</li>
<li>
<p>If you ask for the REST and <em>do</em> ask for an error, then it assumes you must not want the one-item-only semantics after all.</p>
</li>
</ul>
<p>It's a little bit awkward because it conflates partial output with fully successful output</p>
<pre><code>&gt;&gt; [block rest error]: transcode "a bc"
== [a bc]

&gt;&gt; error
; null

&gt;&gt; [block rest error]: transcode "a bc 1&amp;x def"
== [a bc]  ; no indication something failed

&gt;&gt; error? error  ; you'd have to remember to check this
== #[true]
</code></pre>
<p>That's not a deal breaker, and Ren-C makes it easy to work with, using <a href="https://forum.rebol.info/t/circling-multi-returns-choose-which-is-expression-result/1587">circling an output in multi-return</a> to make it the primary return result:</p>
<pre><code>&gt;&gt; [block rest @error]: transcode "a bc"
; null

&gt;&gt; [block rest @error]: transcode "a bc 1&amp;x def"
== make error! [...]
</code></pre>
<p><strong>What's much more jarring to me is the flipping back and forth of whether you're asking for a full transcode or not.</strong></p>
<pre><code>&gt;&gt; x: transcode "abc def"
== [abc def]

&gt;&gt; [x y]: transcode "abc def"
== abc

&gt;&gt; [x y z]: transcode "abc def"
== [abc def]
</code></pre>
<p>Ick.  Should I be willing to bend on the transcode "requested parameter" behavior in this case, by adding a /ONE refinement?</p>
<pre><code>&gt;&gt; [block rest]: transcode "abc def"
== [abc def]

&gt;&gt; rest
== #{}  ; kind of useless, but honest

&gt;&gt; [block rest]: transcode/one "abc def"
== abc

&gt;&gt; rest
== " def"
</code></pre>
<p>That would make me feel grief, as it loses one of the first showcases of return value sensitivity.  <em>And it irks me to think that the beauty is ultimately being given up for the sake of what amounts to an optimization.</em></p>
<h2>Answer For Now: Kill Off /ERROR</h2>
<ul>
<li>
<p>The answer /ERROR has been giving back in error cases for the remainder is sketchy, and I don't want to figure out how to fix it.</p>
</li>
<li>
<p>You can get the behavior reliably just by intercepting errors going one transcode item at a time.</p>
</li>
<li>
<p>This is a good opportunity to write tests of item-by-item scanning with error handling</p>
</li>
<li>
<p>Red added a bunch of refinements on transcode [/next /one /prescan /scan /part /into /trace], and they didn't pick up /error themselves</p>
</li>
</ul>
<p>Speaking of adding lots of refinements: I also want to get away in general from investments in weird C scanner code and hooks (<em>especially</em> if it's just an optimization).</p>
<p>What we should be investing in is more fluid mixture of PARSE of strings/binary with the scanner.  e.g. we should have ways of knowing what line number you're at during the parse for any combinator, and just generally pushing on that.  Adding TRANSCODE parameters up the wazoo isn't a winning strategy.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940</link>
          <pubDate>Mon, 22 Aug 2022 15:09:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1940</guid>
          <source url="https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940.rss">Incomplete TRANSCODEs: Actually an Optimization Problem</source>
        </item>
        <item>
          <title>Optimizing TRANSCODE Usage in String/Binary PARSE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>As written, the DATATYPE! combinator in UPARSE may do wasteful value loading when operating on string input.</p>
<p>Consider this case.</p>
<pre><code>&gt;&gt; parse "[some big block ...] 10" [collect some [keep integer! | block!]]
== [10]
</code></pre>
<p><em>Pretty impressive that it works.</em>  (Red will only do this on BINARY! input, but Ren-C's UTF-8 everywhere allows it to do it on strings too!)</p>
<p>But at the combinator level, it's wasteful.  What happens is:</p>
<ul>
<li>
<p>Hitting the INTEGER! combinator, causing it to scan the next element, loading <strong><code>[some big block ...]</code></strong> as a series into memory.</p>
<ul>
<li>It then checks the type, notices it's not an integer, and the INTEGER! combinator gives back a rejection...so the BLOCK! combinator goes to the next alternate.</li>
</ul>
</li>
<li>
<p>It hits the BLOCK! combinator and scans the block again.</p>
<ul>
<li>
<p>This time it matches, so the parser returns success and the synthesized block</p>
</li>
<li>
<p><em>But the block isn't actually desired</em>, so it is thrown away</p>
</li>
</ul>
</li>
<li>
<p>The next iteration scans the INTEGER! and keeps it.</p>
</li>
</ul>
<h2>Why Does It Work This Way?</h2>
<p>It's based on TRANSCODE, and does basically exactly what I said:</p>
<pre><code>[item remainder]: transcode input except e -&gt; [return raise e]

if datatype != type of item [
    return raise ["Could not TRANSCODE" datatype "from input"]
]
return item
</code></pre>
<p>If we could pass in a datatype to TRANSCODE when using the /NEXT option (e.g. requesting a remainder, as we are above) then it could short-circuit and we wouldn't need that test.</p>
<h2>Red Has Looked At This Kind of Problem</h2>
<p>There are a bunch of new arguments to Red's TRANSCODE function:</p>
<pre><code>USAGE:
     TRANSCODE src

DESCRIPTION: 
     Translates UTF-8 binary source to values.
     Returns one or several values in a block. 

ARGUMENTS:
     src          [binary! string!]
     {UTF-8 input buffer; string argument will be UTF-8 encoded.}

REFINEMENTS:
     /next        =&gt; Translate next complete value (blocks as single value).
     /one         =&gt; Translate next complete value, returns the value only.
     /prescan     =&gt; Prescans only, do not load values. Returns guessed type.
     /scan        =&gt; Scans only, do not load values. Returns recognized type.
     /part        =&gt; Translates only part of the input buffer.
         length       [integer! binary!] "Length in bytes or tail position."
     /into        =&gt; Optionally provides an output block.
        dst          [block!] 
     /trace       =&gt; 
        callback     [function! [
                        event [word!]
                        input [binary! string!]
                        type [word! datatype!]
                        line [integer!]
                        token
                        return: [logic!]
                      ]] 

RETURNS:
    [block!]
</code></pre>
<p>I'm not sure exactly how useful the /PRESCAN option is (what good is a "guess" of the type?)  But the /SCAN option would offer some bit of efficiency.</p>
<p>It would mean instead of one call to TRANSCODE followed by a datatype test, there'd be two calls</p>
<ul>
<li>
<p>The first as TRANSCODE/SCAN to get the datatype (but not synthesize a value from it)</p>
</li>
<li>
<p>A second call to scan again and get the value</p>
</li>
</ul>
<p>We assume the idle mode of scanning without producing anything can be fast.</p>
<p>I would suggest the scan feature be <strong>transcode/types</strong> so it worked more generally, not just with /NEXT.</p>
<pre><code>&gt;&gt; transcode/types [1 a [b]]
== [#[datatype! integer!] #[datatype! word!] #[datatype! block!]]
</code></pre>
<p><sub><em>(When I figure out the story of datatypes, there are going to be a lot of forum posts fixing up the above ugly notation.)</em></sub></p>
<h2>But What About The Synthesis Of Unused Values?</h2>
<p>This is a bit of a pickle.  <em>We don't know if you're going to use the product or not.</em></p>
<p>UPARSE's design has values bubbling out the top, and no line of communication to be aware of whether what it produces will be used:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! group!] 
== (b)
</code></pre>
<p>You might think that when the block! rule is going to be run, UPARSE could notice it wasn't at the end and send some kind of signal to the BLOCK! combinator that it doesn't have to synthesize an output.  But there's no a-priori psychic power saying that GROUP! hasn't been configured to evaluate to void.  Until the combinator gets looked up and run, it's potentially the same situation as this:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! void] 
== [a]
</code></pre>
<h2>It Seems We Have Two Choices</h2>
<ol>
<li>
<p>We can assume that a plain DATATYPE! intends to synthesize a value, and use a different combinator to say you only want to match the type:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [scan block!]
== #[datatype! block!]  ; cheap (but useful) return value, no series synthesis

&gt;&gt; uparse "[a b c]" [block!]
== [a b c]
</code></pre>
</li>
<li>
<p>We can reverse it and say that by default it does the cheap thing, and you have to explicitly ask to get the expensive thing:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
== #[datatype! block!]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Looked at in isolation, it might seem like (2) would be the obvious winner.</p>
<p>The thorn is that this would be a pretty notable divergence from how array parsing works, which I would basically call non-negotiable:</p>
<pre><code>&gt;&gt; uparse [[a b c]] [x: block!]

&gt;&gt; x
== [a b c]
</code></pre>
<p>So is there actually an option 3?</p>
<ol start="3">
<li>
<p>Make lone datatype! an error, and have two distinct operations for transcoding:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
** Error: On string input, use either TRANSCODE BLOCK! or SCAN BLOCK!

&gt;&gt; uparse "[a b c]" [transcode block!]
== [a b c]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Urg.  That kind of sucks.</p>
<p><strong>I think the answer is to accept option (1) being suboptimal performance, allowing those who are performance-minded to tune it.</strong>  There's no overt harm by scanning things you throw away, it's just wasteful.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939</link>
          <pubDate>Sun, 21 Aug 2022 19:10:10 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1939</guid>
          <source url="https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939.rss">Optimizing TRANSCODE Usage in String/Binary PARSE</source>
        </item>
        <item>
          <title>Simple Objects vs. What The People Want</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C has a more streamlined version of how R3-Alpha implemented simple OBJECT!s, but it's really mostly the same <em>(though MODULE! has changed significantly)</em></p>
<p>An OBJECT! is just two parallel lists, which I have called the <strong>"keylist"</strong> and the <strong>"varlist"</strong>.</p>
<p>So if you say something like:</p>
<pre><code>obj: make object! [
    x: 1 + 2
    y: 10 + 20
]
</code></pre>
<p>You will get:</p>
<pre><code>keylist: {symbol(x) symbol(y)}
varlist: [*V0* 3 30]
</code></pre>
<p>The first slot in a varlist is used for some tracking information.  So:</p>
<ul>
<li>
<code>keylist[0]</code> is the key for <code>varlist[1]</code>
</li>
<li>
<code>keylist[1]</code> is the key for <code>varlist[2]</code>
</li>
</ul>
<h2>You Get A New Keylist With Every MAKE OBJECT!</h2>
<p>Nothing in the system goes around looking for common patterns in your object creation to notice that you've made several objects with the same keys.</p>
<pre><code>collect [
    count-up i 1000 [
        keep make object! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>You just made 1000 objects, and all of them have their own copy of the keylist <code>{symbol(X) symbol(Y)}</code>.  Ren-C made this overhead cost less than 1/4 as much as R3-Alpha, but it's still kind of lame.</p>
<p><strong>The only way you avoid making a new keylist is if you do object inheritance.</strong></p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>This time, there's 1000 objects all sharing a single keylist.</p>
<p><strong>If you expand keys at all, that will result in a new keylist...</strong></p>
<p>You spoil the optimization if you put anything additional in your derived object:</p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20, z: i * 30]
    ]
]
</code></pre>
<p>There's no inheritance mechanism that makes use of the common sublist.  So this puts you at <em>1001</em> keylists, because your keylist for the original point! never gets used.</p>
<p><strong>Object Expansion via APPEND disconnects shared keylists</strong></p>
<p>R3-Alpha allowed you to add fields to an object.  If you did so, you would lose any sharing that it had taken advantage of before.</p>
<pre><code>p: make point! [x: 10 y: 20]  ; reuses point!'s keylist
append p [z: 30]  ; oop, not anymore...gets its own keylist.
</code></pre>
<p><strong>Comparisons Are Difficult</strong></p>
<p>Because there's no global mechanism of canonization of keylists, you get entirely different-looking objects by creating the fields in different orders.</p>
<pre><code>obj1: make object! [x: 10 y: 20]
obj2: make object! [y: 20 x: 10]
</code></pre>
<p>These objects have been considered to be not equal historically.  Because comparisons are done by walking the fields in order.  So obj1 &lt;&gt; obj2 in this case.</p>
<p>However, if you create an object via inheritance so it shares a keylist, that will standardize the order of the fields:</p>
<pre><code>point1: make point! [x: 10 y: 20]
point2: make point! [y: 20 x: 10]
</code></pre>
<p>Here we will have point1 = point2, since their shared keylist forces the order of x and y to whatever it was in POINT!.</p>
<h2>There Are Fancier Ways Of Dealing With This</h2>
<p><strong>If you're willing to say that the order of keys in objects shouldn't matter...</strong> then you can rethink the data structures to exploit commonalities in the patterns of keys that are created.</p>
<p>The V8 JavaScript engine approaches this with <strong><a href="https://richardartoul.github.io/jekyll/update/2015/04/26/hidden-classes.html">Hidden Classes</a></strong>.</p>
<p>But there's really always some other way of approaching the problem.  The way modules work in "Sea of Words" is an example of a structure that seems to work reasonably well for modules--but wouldn't work as well for lots of little objects.</p>
<h2>Today's FRAME! Depends On This Non-Fancy Way</h2>
<p>Right now, when a native runs it does so with a concept of the order of the arguments and refinements that gets baked into the C code directly.  IF knows that the condition is argument 1 and that the branch is argument 2, and it looks directly in slots 1 and 2 of the varlist of the frame to find those variables.</p>
<p>This is pretty foundational to the idea of the language, and is part of what gives it an appealing "simple-ness".</p>
<p>Ren-C has come along and permitted higher level mechanisms like specialization and adaptation, but everything is always getting resolved in a way that each step in a function's composition works on putting information into the exact numbered slot that the lower levels expect it to be in.</p>
<h2>Binding Has Depended On This Non-Fancy Way</h2>
<p>A premise in Rebol has been that you can make a connection between a variable and an object that has a key with the name of that variable, and once that connection is made it will last.  This rule is why there's been dodginess about deleting keys in objects or rearranging them...and why R3-Alpha permits adding new variables but not removing any.</p>
<pre><code> obj: make object! [x: 10 y: 20]
 code: [x + y]
 bind code obj
</code></pre>
<p>If you write something like the above, you are annotating the X inside of CODE with (obj field <span class="hashtag">#1</span>), and the Y inside of CODE with (obj field <span class="hashtag">#2</span>).  So nothing can happen with obj that can break that.</p>
<p><strong>This isn't strictly necessary.</strong>  It could have annotated X and Y with just (obj) and then gone searching each time it wanted to find it.  This would permit arbitrary rearrangement of OBJ, inserting and removing keys.  It could even remove X or Y and then tell you it couldn't find them anymore.</p>
<p>There are compromises as well.  The binding could be treated as a potentially fallible cache...it could look in that slot position (if it's less than the total keylist size) and see if the key matched.  If not, it could fall back on searching and then update with the slot where it saw the field.</p>
<p>(Of course this means you have to look at the keylist instead of just jumping to where you want to be in the varlist, and locality is such that they may not be close together; so having to look at the keylist <em>at all</em> will bring you a slowdown.)</p>
<h2>But What Is The Goal, Here?</h2>
<p>I've mentioned how the FRAME! design pretty much seems to go along well with the naive ordering of object fields.</p>
<p>I guess this is where your intuition comes in as to what represents "sticking to the rules of the game".  <em>And I think that hardcoding of positions into the executable of where to find the argument cells for natives is one of the rules.</em></p>
<p>This suggests that all functions hardcode the positions of their arguments--even usermode functions.  I'm okay with this.</p>
<p>So then we get to considering the question about OBJECT!.</p>
<ul>
<li>
<p>A lot of languages force you to predefine the structure of an object before creating instances.  And defining that structure is a good place to define its interfaces.  If Rebol wants to go in a more formal direction (resembling a Rust/Haskell/C++) then you might suggest you <em>always</em> make a base structure...and you can only have the fields named in it.</p>
</li>
<li>
<p>Other languages (like JavaScript) are more freeform, and as mentioned can look for the relationships after-the-fact.  Order of fields does not matter.</p>
</li>
</ul>
<p>It's clear that Rebol's userbase so far are people who would favor better implementation of the JavaScript model over going to more strictness.  I think there'd be a pretty good reception of a model where you could create objects with <strong>{...}</strong> and where fields could be added or removed as people saw fit.  If behind-the-scenes the system was optimizing access to those objects, that would presumably be preferable to this idea that you had to be responsible for declaring prototypes to get efficiencies (that would instantly disappear if you added another field).</p>
<p>But the mechanics definitely get more complicated.  :-/</p>
            <p><small>8 posts - 3 participants</small></p>
            <p><a href="https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745</link>
          <pubDate>Mon, 18 Oct 2021 06:45:29 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1745</guid>
          <source url="https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745.rss">Simple Objects vs. What The People Want</source>
        </item>
        <item>
          <title>Changing Strategies on Avoiding Stdio Inclusion</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Among the battles that Rebol picked, one was to not become dependent on the IO and formatting constructs of libc.  So you could build an interpreter without the logic that is behind <strong>printf("Hello %s, your Score is %d\n");</strong></p>
<p>Ren-C embraced this and tried to enforce it by causing compile-time errors when stdio.h was included in release builds.  Over time this has turned out to be a non-viable strategy for accomplishing the intent.</p>
<p>Modern C compilers have more or less assumed that if you include one header you want to include them all.  So if you <strong><code>#include &lt;string.h&gt;</code></strong> you're likely to get all of <strong><code>&lt;stdio.h&gt;</code></strong> too.</p>
<p>And as it happens, while we don't need printf(), we now do need some definitions out of stdio in some files.</p>
<p><strong>We still should keep an eye on included functions, but that oversight needs to shift from the compiler level to the linkage level</strong>.</p>
<p>Here was some of the trickery used to try and keep stdio.h out of release builds:</p>
<pre><code>//
// DISABLE STDIO.H IN RELEASE BUILD
//
// The core build of Rebol published in R3-Alpha sought to not be dependent
// on &lt;stdio.h&gt;.  Since Rebol has richer tools like WORD!s and BLOCK! for
// dialecting, including a brittle historic string-based C "mini-language" of
// printf into the executable was a wasteful dependency.  Also, many
// implementations are clunky:
//
// http://blog.hostilefork.com/where-printf-rubber-meets-road/
//
// To formalize this rule, these definitions will help catch uses of &lt;stdio.h&gt;
// in the release build, and give a hopefully informative error.
//
#if defined(NDEBUG) &amp;&amp; !defined(DEBUG_STDIO_OK)
    //
    // `stdin` is required to be macro https://en.cppreference.com/w/c/io
    //
    #if defined(__clang__)
        //
        // !!! At least as of XCode 12.0 and Clang 9.0.1, including basic
        // system headers will force the inclusion of &lt;stdio.h&gt;.  If someone
        // wants to dig into why that is, they may...but tolerate it for now.
        // Checking if `printf` and such makes it into the link would require
        // dumping the library symbols, in general anyway...
        //
    #elif defined(stdin) and !defined(REBOL_ALLOW_STDIO_IN_RELEASE_BUILD)
        #error "&lt;stdio.h&gt; included prior to %sys-core.h in release build"
    #endif

    #define printf dont_include_stdio_h
    #define fprintf dont_include_stdio_h
#else
    // Desire to not bake in &lt;stdio.h&gt; notwithstanding, in debug builds it
    // can be convenient (or even essential) to have access to stdio.  This
    // is especially true when trying to debug the core I/O routines and
    // unicode/UTF8 conversions that Rebol seeks to replace stdio with.
    //
    // Hence debug builds are allowed to use stdio.h conveniently.  The
    // release build should catch if any of these aren't #if !defined(NDEBUG)
    //
    #include &lt;stdio.h&gt;

    // NOTE: F/PRINTF DOES NOT ALWAYS FFLUSH() BUFFERS AFTER NEWLINES; it is
    // an "implementation defined" behavior, and never applies to redirects:
    //
    // https://stackoverflow.com/a/5229135/211160
    //
    // So when writing information you intend to be flushed before a potential
    // crash, be sure to fflush(), regardless of using `\n` or not.
#endif
</code></pre>
<p>Here are some comments on how the C++ <strong><code>&lt;string&gt;</code></strong> header on MSVC pulled in formatting that pulled in string.h (aka. <strong><code>&lt;cstring&gt;</code></strong> in C++ terms).</p>
<pre><code>/*
 * If using C++, variadic calls can be type-checked to make sure only
 * legal arguments are passed.  It also means one can pass literals
 * and have them coerced (e.g. integer =&gt; INTEGER! or bool =&gt; LOGIC!).
 *
 * Note: In MSVC, `#include &lt;string&gt;` will pull in `&lt;xstring&gt;` that
 * then sucks in `&lt;iosfwd&gt;` which brings in `&lt;cstdio&gt;`.  This means
 * that including the release version of %sys-core.h will see an
 * inclusion of stdio that it doesn't want.  Bypass the assertion
 * for this case, and hope the C build maintains dependency purity.
 */
#ifdef TO_WINDOWS
    #define REBOL_ALLOW_STDIO_IN_RELEASE_BUILD  // ^^-- see above
#endif
</code></pre>
<h2>Again, not giving up, just changing tactics...</h2>
<p>In fact, it's really better to be looking at the binary for bloat anyway.  You can write innocuous lines of source and find that brought in all kinds of things in the compiler.  So going by the source isn't the best idea.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685</link>
          <pubDate>Thu, 26 Aug 2021 13:24:52 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1685</guid>
          <source url="https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685.rss">Changing Strategies on Avoiding Stdio Inclusion</source>
        </item>
        <item>
          <title>Sea of Words is now in Beta (or something?)... Some Numbers</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I've gotten Sea of Words through the test suite and Bootstrap...and running the scenarios that have GitHub Actions (rebol-httpd, rebol-odbc, rebol-whitespacers.)  And I got it working in the web console too, of course!</p>
<p><em>(Note: If the web console seems sluggish these days don't blame Sea of Words...I started using UPARSE in it, and right now UPARSE is in full-on experimental mode.  It's a beast, so using it at all--even on trivial samples--will be resource intensive.)</em></p>
<h1>Some Easy-To-Get Numbers For The Moment</h1>
<p><strong>These numbers should be taken with a grain of salt...</strong> they don't measure everything, and some things shift around in ways that are hard to quantify.  But they're better than nothing.</p>
<p>(Note: I actually had to fix a bug in the evaluation count that was giving wild answers.  R3-Alpha lacked a double-check on its optimized method of incrementing the total evaluation count without needing to so every time in the loop...)</p>
<h2>Prior to Sea of Words</h2>
<p>Here is a report from a freshly booted desktop build on Windows, which avoids trying to read the executable into memory:</p>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 123702
    series-made: 53630
    series-freed: 27122
    series-expanded: 728
    series-bytes: 3460161
    series-recycled: 25447
    made-blocks: 33403
    made-objects: 191
    recycles: 3
]
</code></pre>
<h2>After Sea of Words</h2>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 138386
    series-made: 72860
    series-freed: 39697
    series-expanded: 706
    series-bytes: 3270669
    series-recycled: 20856
    made-blocks: 52054
    made-objects: 221
    recycles: 3
]
</code></pre>
<p>On the bright side, <strong>Sea of Words is not only a watershed moment in binding/modules, it's also saving 189K or so of memory</strong>, even just here in its first debut.</p>
<p>You may be wondering why there are so many more blocks.  The answer is that they're very <em>tiny</em> optimized stub blocks, used to hold individual variables that are floating in the "sea".  This is expected and purposeful.  As shown, the total memory use went down...</p>
<h2>This is Really Only A Beginning</h2>
<p>While the abilities that just came into play are a tremendous step for making a "real" and usable module system, there is significantly more left.  I'll be posting more on those issues after some <img src="https://forum.rebol.info/images/emoji/twitter/zzz.png?v=9" title=":zzz:" class="emoji" alt=":zzz:"></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678</link>
          <pubDate>Sun, 22 Aug 2021 11:45:46 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1678</guid>
          <source url="https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678.rss">Sea of Words is now in Beta (or something?)... Some Numbers</source>
        </item>
        <item>
          <title>Beating REPEND: A New Parameter Convention?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>When you do an <strong>append a reduce b</strong>, the REDUCE generates a new series... let's call it <strong>rb</strong>.  Then <strong>rb</strong> is spliced into <strong>a</strong>.  And then <strong>rb</strong> needs to be GC'd.</p>
<p>The idea behind <strong>repend a b</strong> is that you never make <strong>rb</strong>.  Instead, expressions are evaluated one by one and put onto <strong>a</strong> as you go.  The savings are twofold...reduced memory overhead and reduced tax on the GC by not making extra series nodes.</p>
<p>That might sound like a great savings, but here is a heated debate in Red about the questionable benefit of REPEND (as well as /INTO):</p>
<p><a href="https://github.com/red/red/issues/3340">https://github.com/red/red/issues/3340</a></p>
<p>I guess I'm halfway on DocKimbel's side there...in that if REPEND isn't showing a benefit it's probably more to do with a bug in REPEND vs. that the idea doesn't represent a savings.</p>
<p>But I <em>hate</em> the word REPEND.  Things like REMOLD are double monstrous, and REFORM?  Give me a break.  These make a terrible impression.</p>
<p>More generally, I don't like the idea that every function would have to come in two flavors and create anxiety on the part of the caller as to if they're using the optimized one or not.  I'd like any optimization to be more "under the hood" so the caller doesn't have to fret about it.</p>
<p>This got me to thinking...</p>
<h2>A New Voodoo for GET-params!</h2>
<p>Let's imagine that we have a new rule for params that look like GET-WORD!:</p>
<ul>
<li>
<p>If the argument is a GET-XXX!, it is passed literally</p>
</li>
<li>
<p>If the argument is anything else, it is evaluated normally and the product is passed in with one quoting level added.</p>
</li>
</ul>
<p>Here's an example definition</p>
<pre><code>appender: func [
    block [block!]
    :value [any-value!]
][
   print ["Block is" mold block]
   print ["Value is" mold value]
   if get-block? value [
       append block reduce as block! value
   ] else [
       append block unquote value
   ]
]
</code></pre>
<p>Let's look at some concrete examples:</p>
<pre><code>&gt;&gt; appender [1 2 3] 2 + 2
Block is [1 2 3]
Value is '4
== [1 2 3 4]

&gt;&gt; data: [[a b c] [d e f]]
&gt;&gt; appender [1 2 3] second data
Block is [1 2 3]
Value is '[d e f]
== [1 2 3 d e f]

&gt;&gt; appender [1 2 3] :[10 + 20 100 + 200]
Block is [1 2 3]
Value is :[10 + 20 100 + 200]  ; not quoted!
== [1 2 3 30 300]
</code></pre>
<p>At the source level, the user doesn't really have to worry about the parameter convention.  They get the same outcome as if the REDUCE had been done by the evaluator, but the APPENDER becomes complicit.</p>
<p>And look what happens if the GET-BLOCK! is in a variable...</p>
<pre><code>&gt;&gt; data: the :[10 + 20 100 + 200]
&gt;&gt; appender [1 2 3] data
Block is [1 2 3]
Value is ':[10 + 20 100 + 20]
** Error: Cannot append evaluative items...
</code></pre>
<p><strong>A ha!</strong> We could tell that this was an evaluative get-block product, and not meant to participate in our little trick.  <em>(Erroring is actually the right answer here, you would need to use <strong>only data</strong> or <strong>^data</strong> or <strong>quote data</strong> etc. to APPEND an evaluative GET-BLOCK! under the new rules.)</em></p>
<p>This is neat, because it means users can express intention to reduce at the callsite...and it is something that you can optimize on an as-needed basis.</p>
<h2>As One Would Expect, There Are Some Glitches...</h2>
<p>There are some seeming semantic glitches when a function takes these and they're not the last parameter, where you might see variations along the lines of:</p>
<pre><code> &gt;&gt; takes-first-args-normally :[elide print "A", 1 + 2] (print "B", &lt;x&gt;)
 A
 B
 3
 &lt;x&gt; 

&gt;&gt; takes-first-arg-specially: :[elide print "A", 1 + 2] (print "B", &lt;x&gt;)
A
B
&lt;x&gt;
3
</code></pre>
<p>Basically: <strong>If you somehow relied on side effects happening in left-to-right parameter order at the callsite, then moving the REDUCE of any parameters other than the last one into the body of the operation will change that order.</strong></p>
<p>This is nothing new for this line of thinking in optimization: imagine if APPEND and REPEND took their arguments in the reverse order, so that the block wasn't the last item.  You couldn't just blindly substitute APPEND REDUCE for REPEND in that case, if you were dependent on argument-ordering effects...if there was an evaluation in the first parameter's reduction that was needed for the second parameter.</p>
<p>But the difference is that the person editing APPEND REDUCE =&gt; REPEND made  a change at the callsite.  If you change the parameter convention and don't touch the callsites--with the intent that they stay working and you're just adding an optimization--it starts to matter.</p>
<p>We have some control here, though!  We can define how GET-BLOCK!s act as arguments to function calls, and we can say that they don't actually perform their REDUCE until the function executes.  That leaves breathing room for people who wish to add the optimization later...knowing they won't break the expectations.</p>
<p><em>Whew, that solves that problem!  Good thing it's the only one!  Oh, no, wait...</em>  <img src="https://forum.rebol.info/images/emoji/twitter/face_with_head_bandage.png?v=9" title=":face_with_head_bandage:" class="emoji" alt=":face_with_head_bandage:"></p>
<h2>Not All REPEND Operations Take Literal Blocks</h2>
<p>You see <strong>repend data [...]</strong> a lot of the time, but there's also <strong>repend block1 block2</strong>.</p>
<p>So <strong>append data :[...]</strong> can be finessed as an optimization for the first case, but doesn't solve the second.</p>
<p>To shore it up, we'd have to say that <strong><code>:(...)</code></strong> means "reduce the result of what's in the expression".</p>
<pre><code>&gt;&gt; :(reverse [1 + 2 10 + 20])  ; -&gt; :[20 + 10 2 + 1]
== [30 3]
</code></pre>
<p>This way, we could actually pass the APPEND an expression to reduce the product of.  We'd need to do the evaluation at the moment we passed the parameter (I think), and then alias it as a GET-BLOCK!, so:</p>
<pre><code>&gt;&gt; appender [1 2 3] :(reverse [1 + 2 10 + 20])
Block is [1 2 3]
Value is :[20 + 10 2 + 1]
== [1 2 3 3 30]
</code></pre>
<h2>Where Are GET-WORD!, GET-PATH!, GET-TUPLE! in all of this?</h2>
<p>We don't have GET-WORD! mean "reduce the product of fetching the word":</p>
<pre><code>&gt;&gt; block: [1 + 2]

&gt;&gt; :block
== [1 + 2]  ; not [3]
</code></pre>
<p>But it seems it would be inconsistent to not put these other GET-XXX! types into the family of parameters that are captured as-is.  So the above code would get this behavior:</p>
<pre><code>&gt;&gt; appender [1 2 3] :foo
Block is [1 2 3]
Value is :foo
** Error: Cannot append evaluative items...
</code></pre>
<p>Instead of a REDUCE it would need a GET.  But this makes a good argument for why REDUCE of a GET-WORD! should work as a word fetch, for generality... it makes routines like this easier to write correctly.</p>
<p>I don't think it's worth shuffling the symbols around so that <strong>:foo</strong> does a reduce and we pick something else for GET.  It seems to me that <strong>:(foo)</strong> is fine enough.</p>
<p>But even though GET-WORD! won't run arbitrary code, you can be impacted by ordering problems, where someone might pass a <strong>:foo</strong> argument and then in the next parameter change the value of foo.  Hence for consistency, we'd be saying that normal parameters would likely have to delay their get of foo until all the parameters were given...this way you could change the parameter convention without affecting callsites.</p>
<p>But likely the best way to go about that would be to protect the word from modification:</p>
<pre><code>&gt;&gt; some-func :foo (foo: 20, &lt;arg&gt;)
** Error: FOO captured by GET-WORD! in parameter slot, can't modify
      while gathering arguments
</code></pre>
<h2>I'm Probably Over-Worrying About It</h2>
<p>...these protection mechanisms I mention in order to make it painless to change a parameter convention are not likely suited to being the kind of concern that applies.</p>
<p>But it's good to articulate what the limits of a design are...</p>
            <p><small>4 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673</link>
          <pubDate>Thu, 19 Aug 2021 22:29:50 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1673</guid>
          <source url="https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673.rss">Beating REPEND: A New Parameter Convention?</source>
        </item>
        <item>
          <title>Paring Down the Boot Block Symbol Table</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Rebol has historically had a file called %words.r, that points out words that the C code would like to be able to recognize quickly by ID numbers.</p>
<p>So if you want to write something like C code for PARSE that recognizes keywords, you might write something like:</p>
<pre><code>switch (VAL_WORD_ID(word)) {
  case SYM_SOME:
      // code for implementing a some rule..
     break;

 case SYM_WHILE:
     // code for implementing a while rule...
    break;
}
</code></pre>
<p>etc.  In C you can only switch() on integers, not pointers.  So these SYM_XXX values have to be agreed upon by the C code and the symbol-loading subsystem.</p>
<p>Some tricks depend on actual ordering of these symbols, or ranges of them.  But most of the time, it doesn't really matter.</p>
<p>If something doesn't have a symbol ID, then you have to do slower creations and comparisons by string... or create your own instance of a symbol and then compare to that symbol by pointer.</p>
<h2>Another Idea: Nix the Table And Trust Determinism</h2>
<p>Right now the way the loading process goes, you have your list of words in %words.r and they count up.  Let's imagine:</p>
<pre><code>apple
banana
orange
...
</code></pre>
<p>So apple becomes SYM_APPLE = 1, banana becomes SYM_BANANA = 2, orange becomes SYM_ORANGE = 3, etc.</p>
<p>The beginning of the boot block has these words in a block, and then stuff using them</p>
<pre><code>[
    [apple banana orange ...]
    [foo: func [] [eat 'apple] peel orange/banana ...]
    ...
]
</code></pre>
<p>Each of those word cells takes up 4 platform pointers, so 32 bytes apiece.  Which is a fair amount to pay to convey the contract between the C code and the interpreter that APPLE needs to have an associated shorthand of 1, BANANA needs a shorthand of 2, etc.</p>
<p><strong>But if you don't care about the values, why not use whatever the value was organically?</strong></p>
<p>Imagine that list at the beginning wasn't there:</p>
<pre><code>[
    [foo: func [] [eat 'apple] peel orange/banana ...]
    ...
]
</code></pre>
<p>The scanner can still give a number to basically every unique word that's in the boot block if it wants to.  It would just come out in a different order... FOO would be 1, FUNC would be 2, EAT would be 3, APPLE would be 4 etc.</p>
<p>You don't necessarily want a giant C file of SYM_XXX for absolutely every word used in the mezzanine.  But what could be done here would be that %words.r would be an indication of <em>registering interest</em> in what value a loaded word ultimately got.</p>
<p><strong>But how do you know what order the scanner is going to visit words in?</strong>  Well, you don't...and you get a chicken and an egg problem.  You can't build the executable to scan without the SYM_XXX numbers.</p>
<p><strong>...unless the scanner was a separate library that could be linked and run standalone...</strong>  If the scanner was factored you could have one compile step that built it, and then linked it into a small executable just for the purposes of generating an enum of SYM_XXX values for a particular boot block.</p>
<p>Not something likely to happen this year (or this lifetime), but... I thought it was interesting to think that if the code were a little more self-aware, the array of words in the boot block could be cut way back to only words that required having sequential integer numbers for some optimization.  <em>(Or <em>specific</em> numbers for some optimization...the only case of that is that the spelling of datatype words line up with the enum value of the datatype in the system.)</em></p>
<h2>Another Related Idea: An Internet Registry for WORD &lt;=&gt; ID</h2>
<blockquote>
<p>Note: This concept is actually contentious with the above...</p>
</blockquote>
<p>If we really wanted to (and weren't concerned about size), we could get a dictionary off the Internet of the 65535 most common words, and number them all in advance.  Then we could tell people who write C extensions that they can use those numbers in their code, so their extensions would be faster if they happened to want to deal with that spelling of that word.</p>
<p>Except then the r3.exe would have a big fat dictionary inside it with a list of strings that extensions may never use.</p>
<p>But putting the dictionary in isn't actually necessary.  If you trust extension authors to be true to the string table, then just publish the table on the Internet in an agreed upon place.  All an extension has to when it gets loaded is to supply the list of strings and numbers out of that table it wants to use.  You only pay for the entries in the table you need...and it doesn't cost any more than having those strings would anyway.</p>
<p>The system could then reconcile and notice if one extension said "banana" is 1020 and another said "banana" is 304.  It could just say one of those extensions is wrong and refuse to load them.  It can do so without r3.exe needing to store the string "banana" or information about it being 304 intrinsically.</p>
<p><em>The reason I say it's contentious is because changes in the boot block would shuffle the symbol IDs around.  This means the deterministic (but changing) approach would create symbol values that would force extensions to be recompiled, while a committed database of numbers would not.</em></p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671</link>
          <pubDate>Thu, 19 Aug 2021 00:59:26 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1671</guid>
          <source url="https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671.rss">Paring Down the Boot Block Symbol Table</source>
        </item>
        <item>
          <title>About the Optimization category</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>This is a category for discussing performance and optimization ideas.</p>
<p>Though remember the very important <strong>Rules For Optimizations (at least, the Ones Make Code Less Clear)</strong>:</p>
<h1>Rule <span class="hashtag">#1:</span> Don't do it.</h1>
<h1>Rule <span class="hashtag">#2:</span> (Experts only!) Don't do it...yet.</h1>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/about-the-optimization-category/1670">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/about-the-optimization-category/1670</link>
          <pubDate>Thu, 19 Aug 2021 00:15:43 +0000</pubDate>
          <discourse:topicPinned>Yes</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1670</guid>
          <source url="https://forum.rebol.info/t/about-the-optimization-category/1670.rss">About the Optimization category</source>
        </item>
        <item>
          <title>Progress on Nativizing Parser Combinators</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>So... let's start with a virtual machine I have...where r3-alpha gets this time for a rather simple parse operation:</p>
<pre><code>r3-alpha&gt;&gt; delta-time [
     parse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some [opt "c" opt "b" opt "a"]
     ]
 ]
 == 0:00:00.000020  ; averages around here on 
</code></pre>
<p>UPARSE was written with design consideration only; it wasn't even optimized usermode code.  Performance was no object.  And of course, that shows:</p>
<pre><code>uparse-on-day-zero&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]  ; here UPARSE needs FURTHER
     ]
 ]
 == 0:00:00.014000  ; averages around here
</code></pre>
<p>The performance varies a lot because there's so much stuff happening the GC gets triggered, so you have to eyeball it.</p>
<p>But it shows we're talking a ballpark of around 700x slower.  This didn't surprise me at all...running usermode code for all parts of the operation...specializing functions and making frames on so many steps... in a completely general architecture.  <strong>I'm actually surprised it wasn't even slower!</strong></p>
<p>I began chipping away at the infrastructure for combinators to make more of it native.  Generally not the combinators themselves yet (actually only OPT has been changed here...)</p>
<pre><code>basics-plus-opt&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]
     ]
]
== 0:00:00.006000  ; averages around here
</code></pre>
<p>So that cuts it from 700x down to around 300x slower.  This is actually not bad for a beginning!</p>
<p><em>In fact</em>, right here in real-time I'm going to use the techniques I've established to make the combinator that matches TEXT! native and see how much that moves the needle.</p>
<pre><code>with-text&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]
     ]
]
== 0:00:00.005000  ; averages around here
</code></pre>
<p>Just then I'd estimate an hour of work just took it from 300x slower to 250x slower.</p>
<p><a href="https://github.com/metaeducation/ren-c/blob/4a17795c153ff45d8477387987fe8e15e7a2480b/src/mezz/uparse.reb#L846">Here is the usermode form of the original TEXT! combinator</a></p>
<p><a href="https://github.com/metaeducation/ren-c/blob/4a17795c153ff45d8477387987fe8e15e7a2480b/src/core/functionals/c-combinator.c#L367">Here is the native form that I compiled, tested, and measured in about an hour on a slow-compiling machine...</a></p>
<p>Since I just succeeded so quickly, <a href="https://github.com/metaeducation/ren-c/commit/608286c941889a36c70fbdec984f9d5ccd0d00a7">I just hacked up SOME and FURTHER as natives...they're easier than TEXT!</a></p>
<p>And with that it dips down to around 0:00:00.004000.  <em>A total of an hour and a half of work for three combinators and we went from 300x slower down to a mere 200x slower!</em>  <img src="https://forum.rebol.info/images/emoji/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>It might seem for this example we've run out of combinators to make native.  But there's one combinator you're missing that might not be obvious...that's the BLOCK! combinator which orchestrates the sequencing of the OPT clauses.  It's quite a high value case to optimize!</p>
<p>Doing that optimization would mean that with any luck, we'd get to 150x slower than R3-Alpha PARSE for this (or any other) apples-to-apples comparison task.  But I have some things I need to do first, and it would take longer than I want before I finish this post.  But I think you got the idea.</p>
<h2>
<a name="even-with-the-high-multiplier-im-optimistic-about-the-endgame-1" class="anchor" href="https://forum.rebol.info#even-with-the-high-multiplier-im-optimistic-about-the-endgame-1"></a>Even With The High Multiplier, I'm Optimistic About The Endgame</h2>
<p>There's still a bunch of infrastructure besides the combinators to attack where much of the cost exists.  Most of the combinators themselves are pretty simple, but the logic that does the "combinating" itself is not!</p>
<p>But let's imagine that at the end of optimizing <em>all</em> the usermode pieces into native pieces it hits a wall of being 20x or so slower than traditional PARSE.  What then?</p>
<p>Let me give you several reasons why I'm optimistic:</p>
<ul>
<li>
<p><strong>The UPARSE concept is built around fundamental mechanics that are used everywhere by the system.</strong>  For example: there's no special "parse stack", it's using function calls on the same stack as everything else.  It uses clever mechanisms to hook which levels are parsing so you can get UPARSE trace output which doesn't mix up regular function calls with the combinators, but those mechanisms are generic too.</p>
<p>This has plenty of good implications.  Improvements to the function call stack become improvements to the "UPARSE stack" automaticlly (e.g. stacklessness).  Any work we do on making parse recursions faster are likely to make all function calls faster.  Think about all the other aspects this applies to as well.</p>
<p><em>(Of course I'll just restate that all of this is in service of one of the big goals...of letting users pick their own mixes of combinators and write their own.  It wasn't just about reusing work, it was about designing the protocol in a way that the native code wouldn't be locked into a monolithic blob just to save on optimizing some switch() statement.  Everything can be teased out and reconfigured as a mash-up of natives and usermode code.)</em></p>
</li>
<li>
<p><strong>UPARSE is vastly more powerful, so chasing performance parity with any given laborious piece of historical parse code may not be the point.</strong>  Let's say you can express something briefly and eloquently as an idiomatic UPARSE expression and that code runs in 1 second.  Then does it matter that when you write it the convoluted way in historical PARSE it takes 3 seconds, when that convoluted code would run in UPARSE in 5?</p>
</li>
<li>
<p><strong>People with performance-sensitive scenarios who hit a bottleneck can attack that with a combinator specific to their purpose.</strong>  If you write <strong><code>opt some ["a" | "b"]</code></strong> so often that it's bothering you to pay for all the generalized protocols where OPT talks to SOME talks to BLOCK! talks to TEXT!... you could natively write the OPT-SOME-A-OR-B combinator and plug it in.</p>
<p>In addition: there could be ways to make a semi-optimized version of an OPT-SOME-A-OR-B by just asking to pre-combinate those things together.  This would cost you some flexibility... in the past I've talked about <a href="https://forum.rebol.info/t/when-should-parse-notice-changes/1528/3">when PARSE notices rule changes"</a> and that's the kind of phenomenon that might come into play.</p>
<p><em>(Note: Building a CHAIN of functions like <strong>negated-multiply: chain [:multiply | :negate]</strong> have a similar aspect.  They are faster but if what's assigned to the word MULTIPLY or NEGATE change they won't see it...as they commit to the definitions from the time of the CHAIN.)</em></p>
</li>
<li>
<p><strong>The code is organized so much better with responsibility isolated so clearly that I think clever optimizations will be much easier to try.</strong>  There was little you could do with R3-Alpha PARSE without worrying about breaking it.</p>
</li>
</ul>
<h2>
<a name="i-hope-im-right-2" class="anchor" href="https://forum.rebol.info#i-hope-im-right-2"></a>I Hope I'm Right</h2>
<p>It's a challenge but an interesting one to make UPARSE perform.  Let's see where this goes...</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636</link>
          <pubDate>Mon, 19 Jul 2021 05:07:33 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1636</guid>
          <source url="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636.rss">Progress on Nativizing Parser Combinators</source>
        </item>
        <item>
          <title>Fundamental Changes Needed for GC (Reference Counting)</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>So I'd gone ahead with the implementation of virtual binding and LET, because I don't see any real future for the language without it...at least not for the kinds of distinguishing features that I think would make it notable.</p>
<p>But it means we're creating a lot of garbage.  I've brought up pathological cases, like:</p>
<pre><code>count-up x 1000000 [
   let y: x + 1
   print ["Y is" y]
]
</code></pre>
<p>Creating a million tiny tracking entities for each time through the loop is a lot of junk for the GC to have to crunch through.</p>
<p>But the problem runs much deeper than this, because even without LETs you get issues with nested loops and their virtual binding information.  It's one example of many.</p>
<p>It isn't allocating and freeing memory that kills us.  We have memory pools and the layouts of everything are tuned fairly well.  It's having to sweep through all of memory to clear out things that aren't used.</p>
<h2>
<a name="reference-counting-cant-replace-gc-but-would-help-1" class="anchor" href="https://forum.rebol.info#reference-counting-cant-replace-gc-but-would-help-1"></a>Reference Counting Can't Replace GC, But Would Help</h2>
<p>If we had room in each series node for a reference counter, we could notice when that counter reached zero...and free the series without allowing it to accumulate and tax the GC.</p>
<p>That won't get everything, because blocks and objects can have cyclical references.  But a lot of the time, it would let us rapidly reclaim memory to reuse...leading to far less accumulation.</p>
<p>So in the example of the tight COUNT-UP loop above, a FRAME! would be allocated that would have a "specifier chain".  That chain would get the entry for the LET, and so that would count as a reference.  When PRINT runs, the BLOCK! <strong>["Y is" y]</strong> fills into its argument slot...and that instance of the block cell is coupled with the specifier chain...adding another reference.  But when PRINT finished, it would release its hold on the frame where that block cell lived...in this case nothing is holding that frame (it's a native, no debugger, etc.)  That means no one is seeing the cells, so they could all be blanked out...releasing their references.  This would drop the reference <strong>["Y is" y]</strong>'s derelativization has on the specifier chain, bringing it down to 1 reference.  And then, when the frame finished that iteration of the body, it would drop the reference on the specifier chain...reducing its references to 0.  That would free the LET.</p>
<p><em>Or at least the theory is something like that.</em></p>
<h2>
<a name="how-hard-would-it-be-2" class="anchor" href="https://forum.rebol.info#how-hard-would-it-be-2"></a>How Hard Would It Be?</h2>
<p>Offhand, I'd say very hard.</p>
<p>With a C++ build to draw on, it becomes easier to check.  Though I'd definitely say this kind of change would be one of those moments where I'd start to seriously question the sanity of trying to keep on building a sophisticated system in C89.</p>
<p>Doing anything with low-level mechanics is harder the more low-level "core" code you have.  Anything written to higher-levels of abstraction like libRebol wouldn't have to change, but everything that assumes lower access gets a lot hairier.</p>
<p>It's better at the moment to write the code how it's supposed to look...and tackle big challenges, tolerating the slowness.  But I just wanted to bring this up because I don't think the slowness can be beaten unless we do better bookkeeping to know how to reclaim memory.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527</link>
          <pubDate>Fri, 26 Feb 2021 08:53:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1527</guid>
          <source url="https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527.rss">Fundamental Changes Needed for GC (Reference Counting)</source>
        </item>
        <item>
          <title>Web Build Performance Stats</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I resurrected the "stats" function to get some metrics.  It's actually a good example of how nicely Ren-C can improve things:</p>
<ul>
<li>
<p><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/n-system.c#L129">Here's the code for stats in R3-Alpha</a> (which references an object prototype <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/sysobj.r#L255">defined elsewhere in sysobj.h</a>, and you can also see that all you see in this file of the function spec is REBNATIVE(stats))</p>
</li>
<li>
<p><a href="https://github.com/metaeducation/ren-c/blob/de62515f95ce335c07622ef51218d5da9e938a12/src/core/d-stats.c#L56">Here's that in Ren-C</a>, and the maintainability advantages should be obvious.  The distinction of counting natives didn't exist in the same fashion as before, so it was deleted, but we could do that kind of thing another way.</p>
</li>
</ul>
<p>In any case, running the statistics between R3-Alpha and Ren-C are going to show <em>a lot</em> more series and memory use in Ren-C.  The main reasons are:</p>
<ul>
<li>
<p>There's a Windows encapping issue that it reads the whole executable into memory to probe it for resource sections.  This is especially crazy for debug builds.  I'd raised this as an issue for Shixin to look at but forgot about it.</p>
</li>
<li>
<p><strong>Function frames do not use the data stack, and instead the arguments of functions are stored in individual arrays.</strong>  While there are some optimizations to mean this doesn't require an allocation on quite every function call, it means a good portion of function calls do allocate series.  This stresses the GC, but, I've mentioned how it was important for many reasons (including that the data stack memory isn't stable, and that meant the previous approach had bugs passing pointers to arguments around.  It's a given that this is how things are done now--especially with stackless--so it just needs to be designed around and tuned.</p>
</li>
<li>
<p><strong>WORD!s are special cases of string series.</strong>  Things like the word table and binding didn't count in series memory before, and wasn't tabulated in R3-Alpha in the series count.  There are some other examples of this.</p>
</li>
<li>
<p><strong>ACTION!s create more series and contexts.</strong>  The HELP information for most actions that have help information has two objects linked to it...one mapping parameter names to datatypes, and one mapping parameter names to descriptions.  I'm hoping that the one mapping parameter names to datatypes can be covered by the parameter information that the interpreter also sees...but for today, there's a difference because one contains TYPESET!s and the other contains human-readable BLOCK!s.</p>
</li>
<li>
<p><strong>So Much More Is Done In Usermode.</strong>  Ranging from console code to command-line argument processing, there's more source code (which counts as series itself) and more code running.</p>
</li>
</ul>
<p>I see it as good--not bad--that a ton of things run in the boot process.  Although I think you should be able to build an run a minimal system...even one that doesn't waste memory on HELP strings (it's now easier to make such things, since the spec isn't preserved).</p>
<p>But for today, the closest we have to a "minimal build" is the web build.  It's a bit more comparable to R3-Alpha in terms of how much startup code it runs.</p>
<h2>The Current State</h2>
<p>Starting up R3-Alpha on Linux, I get the following for <strong>stats/profile</strong>:</p>
<pre><code>r3-alpha&gt;&gt; stats/profile
== make object! [
    timer: 0:00:02.639939
    evals: 20375
    eval-natives: 3340
    eval-functions: 369
    series-made: 8393
    series-freed: 2597
    series-expanded: 70
    series-bytes: 2211900
    series-recycled: 2526
    made-blocks: 5761
    made-objects: 64
    recycles: 1
]
</code></pre>
<p>Ren-C on the web is considerably heavier, at least when it comes to evals + series made + GC churn <em>(a little less overall series bytes...probably mostly owed to optimizations that fit small series into the place where tracking information would be stored if it were a larger one)</em>:</p>
<pre><code>ren-c/web&gt;&gt; stats/profile
== make object! [
    evals: 65422
    series-made: 28569
    series-freed: 11160
    series-expanded: 419
    series-bytes: 1731611
    series-recycled: 8669
    made-blocks: 16447
    made-objects: 109
    recycles: 229  ; !!! see update, this is now 1
]
</code></pre>
<p>The increased number of evals just goes with the "a lot more is done in usermode" bit.  There's lots of ways to attack that if it's bothersome.</p>
<p>The series-made number is much bigger.  8393 v. 28569.  I mentioned how a lot of this is going to come from the fact that many evals need to make series, but we don't really have a breakdown of that number here to be sure that's accounting for them.  Anyway, this number isn't all that bothersome to me given that knowledge...but it should be sanity-checked.</p>
<p>What does bother me is the 229 recycles.  That's a lot.  Despite making 3-4x as many series, I don't see how exactly that's translating into 200x the recycling.</p>
<p><strong>UPDATE: This was the result of accidentally committed debug code.  It's back to 1.</strong></p>
<h2>Writing Down The Current State is Better Than Nothing</h2>
<p>Ideally we'd have some kind of performance regression chart that plotted some of these numbers after each build.  Though really it's not too worth doing that unless the numbers carried more information that was more actionable.</p>
<p>But...lacking an automated method, writing it down now and having a forum thread to keep track of findings and improvements is better than nothing.</p>
<p>There's likely a lot that could be done to help the desktop build (such as obviously tending to that encap-reading issue).  But I'd like to focus principally on improvements to the internals that offer benefit to the web build, where I think the main relevance is.  And:</p>
<ul>
<li>
<p><strong>Having a system built from rigorously understood invariants is the best plan for optimization over the long-term.</strong>  If you don't have a lot of assertions and confidence about what is and isn't true around your codebase, you can't know if a rearrangement will break it or not.  So I spend a lot of time focusing on defining these invariants and making sure they are true.</p>
</li>
<li>
<p><strong>Avoid optimizing things before you're sure if they're right.</strong>  I'm guilty as anyone of fiddling with things for optimization reasons just because it's cool or I get curious of whether something can work or not.  Programmers are tinkerers and that's just how it is.  But it's definitely not time to go over things with a fine-toothed comb when so many design issues are not worked out.</p>
</li>
</ul>
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/web-build-performance-stats/1468">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/web-build-performance-stats/1468</link>
          <pubDate>Mon, 18 Jan 2021 06:51:16 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1468</guid>
          <source url="https://forum.rebol.info/t/web-build-performance-stats/1468.rss">Web Build Performance Stats</source>
        </item>
        <item>
          <title>Moving Away From &quot;NULL termination&quot; (END!) of BLOCK!s</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C preserved an idea from R3-Alpha...which was that there would be a cell type byte reserved to signal the end of an array.  This is a bit like how null terminators are used with C strings.  However, arrays also tracked their length.  So it was redundant information.</p>
<p>In R3-Alpha, the special cells were given the END! datatype.  Sometimes you would see bugs that would leak the existence of this internal type to the user.  Ren-C hid it more effectively, by not making it an actual "type".</p>
<p>On the plus side, this provides a clean-looking way to walk through the values in an array:</p>
<pre><code>Cell* item = Array_Head(array);  // first cell pointer in the array
for (; Not_End(item); ++item) {
    ...
}
</code></pre>
<p>However, there are several downsides:</p>
<ul>
<li>
<p><strong>You have to pay for a dereference on each step.</strong>  item is a pointer, and you have to follow that pointer to its memory location to read a byte there to see if you've reached the end.  This probably isn't <em>that</em> bad, because odds are you are going to be working with that memory inside the loop anyway.  But maybe you aren't...and you certainly aren't going to be for the last cell.</p>
</li>
<li>
<p><strong>You typically wind up paying a cell's worth of cost for this convenience.</strong>  If your array is empty, it still needs space for at least one cell.  If your array has one cell, it needs space for two.  If it has two it needs space for three, etc.  This isn't just an extra byte (as in C '\0' termination)...it's 4 platform pointers.  So 32 bytes of oft-wasted space on 64-bit platforms for a mostly empty cell.</p>
</li>
<li>
<p><strong>But rounding up by 1 is even worse than wasting one cell...</strong> because it propagates to rounding up in the memory pool block size, and memory pools are sized in multiples of 2 (2, 4, 8, 16, etc).  So if what you really want is a two-cell array--e.g. enough for <strong>a/b</strong>, you move up to the next size and take a chunk from the 4-cell pool.  A 4-cell array needs to come from the 8-cell pool.  Etc.</p>
</li>
</ul>
<h2>
<a name="should-we-scrap-this-idea-1" class="anchor" href="https://forum.rebol.info#should-we-scrap-this-idea-1"></a>Should We Scrap This Idea?</h2>
<p>It's bothered me for a while, but since it might make enumeration faster in some cases I've let it hang around.  Having a terminator has helped catch out of bounds cases more easily.</p>
<p><strong>But I think the time has come to demote termination to a debug-build-only practice.</strong>  It's gotten in the way of too many interesting optimizations.</p>
<p>Data point: Red doesn't do it.  They just store the pointer to the tail of the data (in the slot where R3-Alpha stored the length).  It works either way since you can calculate the length by subtracting the head from the tail...or calculate the tail by adding the length to the head.  I'd imagine the tail is needed more often.</p>
<p>The code isn't usually that much worse:</p>
<pre><code>Cell* item = Array_Head(array);
Cell* tail = Array_Tail(array);
for (; item != tail; ++item) {
    ...
}
</code></pre>
<p>But sometimes there were cases that a function would be passed a Cell* resident in an array, without passing the array also.  And then it would enumerate that value forward until it reached an end.  Such routines aren't all that common, but a few do exist.  They'd need to be revisited.</p>
<p>It's not that huge a deal, and kind of trivial in the scheme of things.  But it would touch a lot of code.  <img src="https://forum.rebol.info/images/emoji/twitter/frowning.png?v=12" title=":frowning:" class="emoji" alt=":frowning:" loading="lazy" width="20" height="20">  But, as usual in Ren-C...the asserts can keep it running.</p>
<h2>
<a name="end-signals-would-still-exist-2" class="anchor" href="https://forum.rebol.info#end-signals-would-still-exist-2"></a>END signals would still exist</h2>
<p>The END cell type is important for other reasons.  It's used in rebEND as a terminator for C va_list arguments, and that's not going away.  There are other applications which are beyond the scope of this post to explain.</p>
<p>And as I say, termination of some kind would probably continue in debug builds.  So they might over-allocate to have enough room at the tail to put an end cell, just to get errors to trigger if you went past the limit.</p>
<p>So let's not malign the END marker too much.  It has been a valuable contributor.  <img src="https://forum.rebol.info/images/emoji/twitter/medal_sports.png?v=12" title=":medal_sports:" class="emoji" alt=":medal_sports:" loading="lazy" width="20" height="20"></p>
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445</link>
          <pubDate>Wed, 30 Dec 2020 10:29:18 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1445</guid>
          <source url="https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445.rss">Moving Away From &quot;NULL termination&quot; (END!) of BLOCK!s</source>
        </item>
        <item>
          <title>Idea: Agreed Upon Symbol Number for Extensions</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>A concept in the R3-Alpha codebase is that there are a certain number of built-in words...which come from a file called %words.r</p>
<p><a href="https://github.com/rebol/rebol/blob/master/src/boot/words.r">https://github.com/rebol/rebol/blob/master/src/boot/words.r</a></p>
<p>This is done so you can switch on a numeric code for these words, and not bother with needing to do a string comparison in C.  Some words (like PARSE keywords) are strategically chosen to be in a sequential range, to make testing for them faster.</p>
<p>If you write an extension in C that operates at the internal level API and want the performance of a native, you might want to talk about a word that's not in that list.  You can get a bit close to the performance for a single test by caching a pointer to the canonized version of that word, and comparing to that canon pointer.  But it won't be quite as fast, and since that won't be a constant...C can't use it in switch statements.</p>
<p>To be more concrete, imagine you have some words not in %words.r like OVERLOAD, MULTIPLE, INHERITANCE.  You couldn't write:</p>
<pre><code> switch (VAL_WORD_SYM(some_word)) {  ; small 16-bit # can be cached in word
     case SYM_OVERLOAD: ...  ; ...but these weren't in %words.r!
     case SYM_MULTIPLE: ...
     case SYM_INHERITANCE: ...
     default: ...
}
</code></pre>
<p>Can't do that for those new terms.  You'd have to do case-insensitive string comparisons, or something like this pseudocode:</p>
<pre><code> REBSTR *canon_overload;
 REBSTR *canon_multiple;
 REBSTR *canon_inheritance;

 void On_Module_Load() {
      canon_overload = Register_Word("overload");
      canon_multiple = Register_Word("multiple");
      canon_inheritance = Register_Word("inheritance");
 }

 void On_Module_Shutdown() {
     Unregister_Word(canon_overload);
     Unregister_Word(canon_multiple);
     Unregister_Word(canon_inheritance);
 }
</code></pre>
<p>So imagine this gives you word series pointers that are guarded from GC for as long as your module is loaded.  Then you could say:</p>
<pre><code> REBSTR *canon = VAL_WORD_CANON(some_word);
 if (canon == canon_overload) { ... }
 else if (canon == canon_multiple) { ... }
 else if (canon == canon_inheritance) { ... }
 else { ... }
</code></pre>
<p>It's less elegant than the switch(), and since the numbers are runtime pointers and not fixed at compile-time, there's no way to optimize as in a switch() by repeatedly bisecting the range of values...if you have N words, you will do N comparisons.</p>
<h2>Weird idea: Agree on a list of words and numbers, commit on Internet</h2>
<p>It would be pretty heinous to make a much bigger %words.r and ship it in every executable...inflating the size of Rebol to include a dictionary.</p>
<p>But there's a possibility that doesn't go that far yet still gets the benefit.  Make the word list and commit it somewhere on the internet that developers can look.  Give every common word a number.  Then, the extension ships with just the spellings and numbers it needs.  All extensions agree to use the same numbers:</p>
<pre><code> #define SYM_OVERLOAD 15092
 #define SYM_MULTIPLE 32091
 #define SYM_INHERITANCE 63029

 void On_Module_Load() {
      Register_Word("overload", SYM_OVERLOAD);
      Register_Word("multiple", SYM_MULTIPLE);
      Register_Word("inheritance", SYM_INHERITANCE);
 }

 void On_Module_Shutdown() {
     Unregister_Word(SYM_OVERLOAD);
     Unregister_Word(SYM_MULTIPLE);
     Unregister_Word(SYM_INHERITANCE);
 }
</code></pre>
<p>Your switch() statements can work just fine, and you're only out of luck if you use a sequence of characters that wasn't committed to in the database.  But the database can grow, so long as it grows centrally and not inconsistently.  (In fact, it's probably better to do it that way, where extension authors ask for the words they want and get them approved before shipping the extension.)</p>
<p>The worst that can happen is you load two extensions that disagree, and it refuses to load them.  It could print out the disagreeing numbers and you could consult the internet to decide who was the culprit using the wrong number.</p>
<p>It's a weird idea but kind of interesting--not in particular because of the performance aspect, but because of enabling the C switch()es.  Since there's only 16 bits of space in the word available for the symbol trick, it's an exhaustible resource.  But maybe still worth doing.  This really isn't difficult, outside of the administrative headache of deciding the policy on giving out <span class="hashtag">#s</span></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188</link>
          <pubDate>Thu, 25 Jul 2019 18:44:16 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1188</guid>
          <source url="https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188.rss">Idea: Agreed Upon Symbol Number for Extensions</source>
        </item>
        <item>
          <title>PATH! and TUPLE! compression, explained</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Since I ever saw them, <strong>I wanted to unify PATH! and REFINEMENT!</strong></p>
<ul>
<li>
<p>It did not make sense to me that there was <strong>foo/bar</strong> and <strong>/bar</strong> but not <strong>/foo/bar</strong> or <strong>bar/</strong></p>
</li>
<li>
<p>I pretty quickly landed on the idea that if you saw something like <strong>/bar</strong> it would be a 2-element PATH!, where the first element was simply BLANK!.</p>
</li>
</ul>
<p>As simple as that sounds, changing to this design the way R3-Alpha was written would make the PATH! form of <strong>/bar</strong> cost <em>SEVEN TIMES</em> as much!!!</p>
<ul>
<li>
<p>When REFINEMENT! was an ANY-WORD! class it cost 4 pointers resident in a block</p>
<ul>
<li>That's 16 bytes on 32-bit platforms, 32 bytes on 64-bit platforms</li>
</ul>
</li>
<li>
<p>The initial implementation in NewPath cost those pointers <em>plus</em> 8 pointers for a series stub <em>plus</em> a pool element that was 4 units long of 4 pointers each...to be big enough for 2 elements and a terminator.</p>
<ul>
<li>That's 112 bytes on 32-bit platforms, 224 bytes on 64-bit platforms</li>
</ul>
</li>
</ul>
<p>Not only do you get a factor of 7 cost increase, you now have 3 separate locations in memory to jump around at to process what you have (in addition to the locality issue of having to go find the symbol itself, which WORD!s had to do as well).</p>
<p>In the "keeping it Amish" spirit, it felt like too much when the difference between <strong>/FOO</strong> and <strong>FOO</strong> jumped like that.  It only takes a few factors of 10 to get to the kinds of software situations we see today.  :-/</p>
<h2>Before Going Further: Let's Recap CELLs and STUBs</h2>
<p>For those who've looked at Rebol sources, you know that a Rebol cell is the size of <em>four platform pointers</em>.</p>
<p>Ren-C keeps the general gist, with some important adjustments.  It breaks down as:</p>
<ul>
<li>
<p><strong>header:</strong> one pointer-sized integer (uintptr_t).  This tells you the cell's type (e.g. REB_BLOCK, REB_INTEGER), among other things.  Only 32 bits are used of this (operated on through <code>uint_fast32_t</code> alias field, in case 32-bit masking operations are faster on 64-bit platforms than a 64-bit <code>uintptr_t</code>).  This allows the system to function identically on 32 and 64 bit systems...though the extra 32 bits could be applied to some kind of 64-bit optimization purpose.</p>
</li>
<li>
<p><strong>"extra"</strong>: one pointer or pointer-sized integer.  It's a union, and which of the union fields is active depends on the type of the cell.  For "bindable" types, this holds a <em>binding</em>...and that's a fairly deep topic.  But if a type isn't bindable--let's say a MONEY! or a DATE!, then it can just use this for some spare bits to help put more data in the cell without needing to do a dynamic allocation.</p>
</li>
<li>
<p><strong>"payload"</strong>: Also a union that depends on the type of cell, but this one is the size of <em>two</em> platform pointers.  That makes it sufficient to hold something like a 64-bit floating point number on 32-bit platforms.  It comes after the "extra" on purpose--so that as long as the cell is on a 64-bit boundary, then on 32-bit platforms this payload will be on a 64-bit boundary as well.  (That's important.)</p>
</li>
</ul>
<p>Beyond just alignment, there's a lot of nuance to cells, and staying on the right side of the standard.  Being able to assign different datatype payloads but still use generic routines on the binding, being able to assign the header in one instruction without breaking strict aliasing.  There's comments in the code if people want to go down a rabbit hole.</p>
<p>But the main thing to take away is that you're not paying a catastrophic cost for something like a 64-bit integer in a Rebol array.  It's 2x the size it would be otherwise.  Fatter than a low-level C type, sure...but all the bits are right there with good locality...you don't have to go through a dereference to some malloc()'d entity.  Not much of a big deal.</p>
<h2>Despite Optimizations, <em>Arrays Cost Notably More</em>
</h2>
<p>When we try to understand the difference between <strong>[1 2 3]</strong> and <strong>[[1] [2] [3]]</strong>, just how much of cost is that in bytes or processing overhead?  If you're designing a dialect, should you fear BLOCK!s, GROUP!s, and PATH!s?</p>
<p>Well, when you've got that cell and the header says it's a REB_BLOCK, the "extra" field is used for stuff I'm not going to explain here.  But the Reb_Block_Payload union contains to two things: a <em>series stub</em> pointer and the index that series value has in the block.</p>
<p>Series stubs are fixed-size tracking entities.  Though they're pretty small, they're still <em>eight platform pointers</em>.  To get to the stub from the cell you have to hop through the pointer to another memory location, and so that's going to affect caching.</p>
<p>If you have a block of length 1 or 0, then Ren-C has a very neat optimization called "singular arrays".  The payload for the array lives <em>directly in the series stub</em>.  Careful mechanics allow this to work, not breaking any alignment or aliasing rules, and even building some safety in via bit patterns to help catch attempts to wander off the edge of the data into neighboring stubs.</p>
<p>So in this case--if you're lucky--you've gone from taking 4 platform pointers for just the REB_INTEGER cell, to a REB_BLOCK cell of 4 platform pointers...and a series stub of 8 pointers.  3x the size for <strong>[1]</strong> vs. just <strong>1</strong>.</p>
<p>But let's say instead of a block, you were doing a PATH!, like the proposed 2-element path for <strong>/refinement</strong> (a BLANK! that's not rendered, and then the WORD! "refinement").  What would a 2-element array cost?</p>
<p>You've still got the 4 pointer cell and the 8 pointer series stub.  But now you need a dynamic allocation to hold the 2 cells, so that would be 8 more platform pointers.</p>
<blockquote>
<p>Note: It would have needed to be 4 cells when a terminator was needed...but <a href="https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445">the terminator no longer applies</a>!  So arrays of length 2 can really just use 8 pointers in their allocation now.  <img src="https://forum.rebol.info/images/emoji/twitter/partying_face.png?v=9" title=":partying_face:" class="emoji" alt=":partying_face:"></p>
</blockquote>
<p><em>Grand Total:</em> 4 + 8 + 8 =&gt; <strong>20 platform pointers</strong>...for something that took only 4 before!  So <strong>[1 1]</strong> is 5x as big as <strong>1</strong>, and on a 64-bit platform we're talking 160 bytes.  For a refinement like <strong>[_ refine]</strong> that's not even counting the storage for the UTF-8 name of the refinement and overhead for that...this is how much it costs just to <em>point</em> to it!</p>
<p>I'm neglecting all the overhead that dynamic allocation systems have to keep for their internal bookkeeping.  Plus, remember that locality...spreading this data out is about more than just how many bytes you use, it's <em>where</em> the bytes are.</p>
<h2>Major Realization: PATH!s and TUPLE!s Should Be Immutable</h2>
<p>Once I realized paths and tuples could be immutable, it began to make sense that they could avoid the problem of being like arrays.</p>
<p>I called them "sequences", and dropped the assumption that every sequence had an array backing it.  Instead, some would...and some would not.</p>
<p>So you wouldn't ask for a pointer to the head cell of a sequence directly.  Instead, you would ask for the Nth element of a sequence...and <em>maybe</em> you would get a pointer to a cell that was part of an array.  Or you could get a pointer back to a temporary cell you passed in where the item would be written.</p>
<p>Then, something like a refinement-type PATH! would not point at an array stub, but to a word spelling stub.  It's possible to tell the difference because in Ren-C all Stubs have "flavor bytes" (which are a parallel to the heart byte in a Cell that tells you if it's a BLOCK! or WORD! etc.)</p>
<p>Since 1-element paths are illegal, we can assume that if a path points to a word spelling it must represent either <strong>/a</strong> or <strong>a/</strong>.  The distinction is encoded with a single bit in the cell.</p>
<h2>It's All Been Working Great</h2>
<p>Immutable PATH! and TUPLE! are clearly good, and they permit this optimization.</p>
<p>This has tied down <a href="http://forum.rebol.info/t/taming-the-pathology-of-path/1006">some of the more egregious ambiguities in PATH!</a>, making it a solid and reliable part for dialecting--something it has certainly not been in the past.  If we can kill off things like <strong><code>a/:b:</code></strong> in favor of <strong><code>a/(b):</code></strong> and pay no more for it, we very well may should.</p>
            <p><small>2 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/path-and-tuple-compression-explained/1008">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/path-and-tuple-compression-explained/1008</link>
          <pubDate>Fri, 11 Jan 2019 05:13:22 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1008</guid>
          <source url="https://forum.rebol.info/t/path-and-tuple-compression-explained/1008.rss">PATH! and TUPLE! compression, explained</source>
        </item>
  </channel>
</rss>
