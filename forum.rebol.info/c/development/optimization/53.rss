<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:discourse="http://www.discourse.org/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Optimization - AltRebol</title>
    <link>https://forum.rebol.info/c/development/optimization/53</link>
    <description>Topics in the &#39;Optimization&#39; category This is a category for discussing performance and optimization ideas.</description>
    
      <lastBuildDate>Fri, 29 Mar 2024 06:47:47 +0000</lastBuildDate>
      <atom:link href="https://forum.rebol.info/c/development/optimization/53.rss" rel="self" type="application/rss+xml" />
        <item>
          <title>Speed of UPARSE</title>
          <dc:creator><![CDATA[bradrn]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Iâ€™m just wondering if any benchmarking has been done on UPARSE. How does it compare to PARSE in Red or in Rebol? Or to parser combinators in Haskell? For that matter, how does it compare to an ordinary recursive-descent parser handwritten in Ren-C?</p>
<p>(Yes, I know UPARSE is unoptimised and slow. But it would be interesting to know <em>how</em> slow.)</p>
            <p><small>6 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/speed-of-uparse/2177">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/speed-of-uparse/2177</link>
          <pubDate>Fri, 29 Mar 2024 06:47:47 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2177</guid>
          <source url="https://forum.rebol.info/t/speed-of-uparse/2177.rss">Speed of UPARSE</source>
        </item>
        <item>
          <title>Mapping from Series =&gt; Series By Co-Opting The Key Series</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>There was an unfinished idea in an old version of the interpreter.  It related to how to deal with problems like trying to make a copy of a block, and make sure any series with the same identity are only copied once in the new structure, and point to that one copied identity.</p>
<p>Rebol2 did not have this behavior:</p>
<pre><code>rebol2&gt;&gt; block: [a]
== [a]

rebol2&gt;&gt; original: reduce [block block]
== [[a] [a]]

rebol2&gt;&gt; append block 'b
== [a b]

rebol2&gt;&gt; original
== [[a b] [a b]]  ; both aliases see the append

rebol2&gt;&gt; duplicate: copy/deep original
== [[a b] [a b]]

rebol2&gt;&gt; append first duplicate 'c
== [a b c]

rebol2&gt;&gt; duplicate
== [[a b c] [a b]]  ; considered by many to be wrong: independent copies
</code></pre>
<p>This post isn't about whether that is right or wrong (and having such questions may seem to some as an indication of <em>"this language is madness! get me to Haskell"</em>, etc. But as I've said this is the game we're playing here so we roll with it.)</p>
<p>But to not get independent copies, you need a way to map series nodes to copies you've already created...so you can consult that mapping before making new copies.  And the direction that was being pursued by the old interpreter I am looking at was to actually do surgery on the originating series nodes, to alter them so they shifted out some of their content, such that they could be their own keys in the mapping.</p>
<p>Generally speaking, all the bits in a series stub are spoken for.  So it would seem there's nowhere to stow a pointer to the new series you are creating in it.  What the implementation was doing was pushing a 4 pointer cell on the data stack, writing one pointer's worth of information from the stub into that cell, then replacing that pointer slot in the stub with the stack index.  Then it wrote the new series into the cell...so the cell contained one stowed pointer from the original series and one pointer for the new series.</p>
<p>This meant the original series was now in a "weird" state, that things like the GC had to know about and tolerate.  Other operations looking for the missing information in the stub needed to be caught if they tried to get at it without following the stack index through to the stack cell.</p>
<p>Having the cells on the data stack meant it was not necessary to enumerate all the series stubs after a copy to "clean them up".  Otherwise, I'd imagine it may be possible to make some kind of guarantee that for any series appearing in source, the union of the bits in the source series and the bits of the copied series can hold all the information necessary to construct two valid series... e.g. one pointer's worth of information is always redundant in those two copies.  If you can get <em>two</em> pointers' worth of information redundant, the second could be used to chain a linked list as you go...removing the need for the stack cells to enumerate.</p>
<p>Though having the stack cells and no particular requirement of information redundancy in source series with their copies offers another benefit: being applicable for creating mappings that aren't copying-related.</p>
<p>Anyway, it was a little unfinished idea I ran across that I wanted to document.  I'm cleaning up the bootstrap executable to refresh it with something that will help <a href="https://forum.rebol.info/t/rethinking-braces-as-an-array-type/1727">in the FENCE! migration</a>, and mercilessly deleting any code in the bootstrap executable that does not specifically benefit bootstrap... to reduce the instability surface, speed things up, and make it easier to debug the 6-year old executable if worst comes to worst.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/mapping-from-series-series-by-co-opting-the-key-series/2166">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/mapping-from-series-series-by-co-opting-the-key-series/2166</link>
          <pubDate>Wed, 06 Mar 2024 15:24:07 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2166</guid>
          <source url="https://forum.rebol.info/t/mapping-from-series-series-by-co-opting-the-key-series/2166.rss">Mapping from Series =&gt; Series By Co-Opting The Key Series</source>
        </item>
        <item>
          <title>Optimizing Environment Lookup</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <h2>
<a name="object-storage-object-frame-port-error-1" class="anchor" href="https://forum.rebol.info#object-storage-object-frame-port-error-1"></a>Object Storage (OBJECT!, FRAME!, PORT!, ERROR!)</h2>
<p>Rebol objects were designed as two parallel arrays, which we can call the "keylist" and the "varlist".  Originally these were entire cells, like this:</p>
<pre><code>obj: make object! [a: 10 b: 20]

            0     1     2
KEYLIST  [     |  a  |  b  ]   ; 4 platform pointers per cell

            0     1     2
VARLIST  [self |  10 |  20 ]   ; 4 platform pointers per cell
</code></pre>
<p>The idea is that the <code>[0]</code>th cell of the varlist contains an instance of the object itself.  This means if the implementation has a pointer to the varlist in its hand, it also has a cell instance of the object.  This also means you can find out from just the varlist what subtype it is (ERROR!, FRAME!, PORT!, etc.)</p>
<p>R3-Alpha used full 4-platform-pointer-sized WORD! cells for each element in the keylist, and left the [0]th cell empty.</p>
<p>Ren-C optimized this to just point to symbols.  So keylists are arrays of single pointers, and there is no [0]th element.</p>
<pre><code>                  0     1 
KEYLIST        [  a  |  b  ]   ; 1 platform pointer per cell

            0     1     2
VARLIST  [self |  10 |  20 ]   ; 4 platform pointers per cell
</code></pre>
<p>Keylists are shared among objects that are used as prototypes for each other, e.g. <strong>obj2: make obj [...]</strong>.  They will become un-shared if any of the objects are expanded.</p>
<p>(Object expansion is allowed in R3-Alpha and Ren-C, but not Rebol2 or Red).</p>
<h2>
<a name="module-storage-module-2" class="anchor" href="https://forum.rebol.info#module-storage-module-2"></a>Module Storage (MODULE!)</h2>
<p>R3-Alpha used the same layout for modules containing hundreds of items as it did for objects.</p>
<p>Ren-C instead allocates small variable "stubs" for each variable in a module.  Each stub is 8 platform pointers in size.</p>
<ul>
<li>4 of those platform pointers are for the cell of the variable's value</li>
<li>1 pointer is for the symbol of the variable</li>
<li>1 pointer is to the module the variable is for</li>
<li>1 pointer to the next stub with the same symbol for another module</li>
<li>1 pointer-sized slot unused at this time</li>
</ul>
<p>These form a linked list of all the same-named variable instances in modules.  This list is pointed to by the symbol itself.</p>
<p>If we want to check if a WORD! cell has a variable in a module, the cell contains a pointer to the word's symbol.  We follow that, and get to the list of variables.  We can walk that list and see if there is an instance matching the module we are looking for.</p>
<h2>
<a name="let-variables-3" class="anchor" href="https://forum.rebol.info#let-variables-3"></a>LET Variables</h2>
<p>At the moment, LET variables are similar to the stubs holding variables for a module... except they don't have an associated module.</p>
<h2>
<a name="specifier-chains-are-linked-lists-of-contexts-or-containers-4" class="anchor" href="https://forum.rebol.info#specifier-chains-are-linked-lists-of-contexts-or-containers-4"></a>Specifier Chains Are Linked Lists Of Contexts -or- Containers</h2>
<p>Things like FRAME! or OBJECT! or MODULE! have one pointer for their "parent specifier".  So when you do something like:</p>
<pre><code> let x: 10
 obj: make object! [y: x + 10, z: x + 20]
</code></pre>
<p>The BLOCK! that object receives has a specifier put onto it... in this case, it will be a LET variable.  That LET variable presumably points up to something else (an enclosing function frame, or a module, or whatever).</p>
<p>The object creates its varlist, and then that varlist has a pointer to the LET.  It uses this as the edited specifier when running the body block of the object.</p>
<p>But if you later try to leverage that object elsewhere e.g. with <strong>overbind obj [...]</strong>, it wants to chain that object onto some other specifier.  However its parent link is already in use for the other chain.  So this means a little stub USE container is needed... which points at the object and provides a new slot to put a pointer in.</p>
<h2>
<a name="looking-up-an-unbound-word-walks-this-chain-5" class="anchor" href="https://forum.rebol.info#looking-up-an-unbound-word-walks-this-chain-5"></a>Looking Up An Unbound Word Walks This Chain</h2>
<aside class="quote no-group quote-modified" data-username="bradrn" data-post="12" data-topic="1751">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://forum.rebol.info/user_avatar/forum.rebol.info/bradrn/40/365_2.png" class="avatar"><a href="https://forum.rebol.info/t/rebol-and-scopes-well-why-not/1751/12">Rebol And Scopes: Well, Why Not?</a>
</div>
<blockquote>
<p>Though I do spy one low-hanging fruitâ€¦</p>
<aside class="quote no-group" data-username="hostilefork" data-post="11" data-topic="1751">
<div class="title">
<div class="quote-controls"></div>
<img loading="lazy" alt="" width="20" height="20" src="https://forum.rebol.info/user_avatar/forum.rebol.info/hostilefork/40/26_2.png" class="avatar"><a href="https://forum.rebol.info/t/rebol-and-scopes-well-why-not/1751/11">Rebol And Scopes: Well, Why Not?</a>
</div>
<blockquote>
<p>a linked list of objects</p>
</blockquote>
</aside>
<p>Might it not be quicker to use a hashmap or similar data structure?</p>
</blockquote>
</aside>
<p>It's not entirely obvious what to hash, here.  And it's not so much that any particular lookup is all that slow.  It's just that there's lots of them, and you can't reliably cache the answer between runs.</p>
<p><a href="https://forum.rebol.info/t/semantics-and-optimization-of-copying-function-bodies/2119/2">One thing I cited to exploit</a> was the fact that when you copy a function body, you tend to wind up with elements that look up either in a module or in the frame of that function.</p>
<ul>
<li>
<p>Module lookup is relatively fast because there aren't all that many redundant names (e.g. there's typically only one APPEND and it's in LIB.)</p>
</li>
<li>
<p>Function frames are not allowed to expand.</p>
</li>
<li>
<p>You can use the space in the unbound elements to give an answer to something knowable--like "this isn't defined in the frame for function X" or "this is defined in the frame for function X at offset Y", that can let you skip along to searching in the module or beeline for the pointer to what you want in the frame.</p>
</li>
</ul>
<p>I'm sure this will help.  Will have to see how much.</p>
<h2>
<a name="gc-load-is-a-big-problem-6" class="anchor" href="https://forum.rebol.info#gc-load-is-a-big-problem-6"></a>GC Load Is A Big Problem</h2>
<p>Ren-C's garbage collector has some interesting points, but it's still a mark-and-sweep strategy.</p>
<p>These specifier chains are being allowed to leak out, with every function call producing tons of them... and function frames have to be GC'd because you can't assume there are no extant references.  (Natives are an exception, they will free their frames when they end, but you can't do that with usermode functions because they use frames as specifiers in the blocks they run... and you don't know what happens to that block).</p>
<p>LETs are pretty bad too... a LET inside a loop creates a little piece of junk each time that needs to get cleaned up.</p>
<p>I think reference counting would be helpful, because most of these aren't referenced very long and aren't involved in cycles.  So reaching a 0 refcount would be a moment the memory could be reclaimed.  My guess is it would outweigh the cost of the reference counting by a fair bit.  But it's difficult to do reference counting correctly in C-like code (although having a C++ build variant it could be double-checked).</p>
            <p><small>4 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/optimizing-environment-lookup/2134">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/optimizing-environment-lookup/2134</link>
          <pubDate>Thu, 25 Jan 2024 03:27:25 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2134</guid>
          <source url="https://forum.rebol.info/t/optimizing-environment-lookup/2134.rss">Optimizing Environment Lookup</source>
        </item>
        <item>
          <title>Semantics and Optimization of Copying Function Bodies</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Part of Rebol's design is that every execution frame has an order to its arguments, represented by an integer.</p>
<p>So for instance with IF:</p>
<ul>
<li>the CONDITION would be in frame <code>cell[1]</code>
</li>
<li>the BRANCH would be in frame <code>cell[2]</code>
</li>
</ul>
<h2>
<a name="native-functions-use-the-ordering-1" class="anchor" href="https://forum.rebol.info#native-functions-use-the-ordering-1"></a>NATIVE Functions Use The Ordering</h2>
<p>Natives don't have to walk the keys of a frame, e.g. to find a slot that matches the symbol "BRANCH".  They are hardcoded to look directly at the index it should be in.</p>
<p><em>(R3-Alpha hardcoded these indices, and you can see that as <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/n-control.c#L620">D_ARG(1) and D_ARG(2)</a> in the implementation of IF.  Ren-C puts the native specs in comments, processed during the build to make macros.  These macros like <a href="https://github.com/metaeducation/ren-c/blob/0d2d7c39733f21e1088240ab2e5c7614072c8059/src/core/n-control.c#L151">ARG(condition) and ARG(branch)</a> resolve to the integers at compile-time.)</em></p>
<h2>
<a name="interpreted-func-functions-use-indices-too-but-differently-2" class="anchor" href="https://forum.rebol.info#interpreted-func-functions-use-indices-too-but-differently-2"></a>Interpreted FUNC Functions Use Indices Too, But Differently</h2>
<p>When a FUNC is being generated, a table is made that maps from symbols for the arguments and locals to the integer for that symbol in the frame.</p>
<p>Then the BLOCK! passed as a body is walked through to find ANY-WORD!s that reference the arguments and locals, and the index is poked into the cells for those words.  Then, the binding pointer in the cell is set to point at the function definition.</p>
<p>That isn't enough information to look up the word--because the binding is relative to a function that is not running.  Hence it's called <strong>relative binding</strong>.  You need to somehow connect this to an instance of the function that is running and has its argument and local cells filled.</p>
<p>Historical Rebol &amp; Red make this connection dynamically by climbing the stack and looking for an instance of the function seen in the binding.  Clearly that doesn't permit indefinite extent (closure) over a local or argument, because when the function isn't running its variables can no longer be found.  But it also can be wrong if recursions interact with material bound to various instances of the same function.</p>
<p>Ren-C uses a better mechanism of <a href="https://forum.rebol.info/t/relative-binding-and-frame-internals/1344">Specific Binding</a>, where information is slipped through instances of BLOCK!s and other arrays, saying which frame that view of the array is executing on behalf of.</p>
<h2>
<a name="conflict-with-new-policy-of-leave-binding-alone-3" class="anchor" href="https://forum.rebol.info#conflict-with-new-policy-of-leave-binding-alone-3"></a>Conflict With New Policy of "Leave Binding Alone"</h2>
<p>The bias in the new proposal of binding is that any WORD!s already bound will be left-as is, with only some minor surgery on environments of blocks done by things like FUNC or FOR-EACH, to inject some new variables at the head of the lookup.</p>
<p>That may make it sound like these words which are bound relatively inside function bodies are more trouble than they are worth.  In the model, they're actually supposed to be thought of as unbound--so if they carry a secret binding, that could only be used as an optimization when they are actually intentionally combined with the right frame.</p>
<h2>
<a name="the-optimization-is-actually-not-minor-4" class="anchor" href="https://forum.rebol.info#the-optimization-is-actually-not-minor-4"></a>The Optimization is Actually Not Minor</h2>
<p>Ren-C function frames are unusual, in that a single frame can be reused for several "phases" of a function composition.  e.g. you can ADAPT and SPECIALIZE and AUGMENT a function--even a native--and all these just reuse the same frame.  But during the phase, it only sees the fields of the frame it is supposed to be able to see.</p>
<p>For instance, if you specialize out the VALUE from append (e.g. by making it always append 5) and then try to ADAPT the resulting function, you won't even know that APPEND has a VALUE at all.  It will seem like it only has a series.</p>
<p>Being strict about enforcing this information hiding permits you to do unusual things, like once the VALUE is shielded in the inner function... you can AUGMENT the resulting functions with another argument named VALUE.  This process can proceed indefinitely.  There could be dozens of fields named VALUE in the frame, but only one visible at a time.</p>
<p>So checking whether a value is visible in the frame is more involved than just walking a list of symbols and comparing them (which isn't necessarily fast in and of itself).  The parameter definitions must be consulted also, to see if they line up with the running phase of the function in order to determine the visibility.</p>
<p>The time of function creation is a good moment to do this work, instead of going through it on every lookup.  And squeaking performance out of pure virtual binding is going to be hard... we need all the tricks we can get.</p>
<h2>
<a name="copying-the-function-body-5" class="anchor" href="https://forum.rebol.info#copying-the-function-body-5"></a>Copying The Function Body</h2>
<p>R3-Alpha actually had a trick during bootstrap, where while loading the library functions it would use a special version of FUNC that did not copy the function body blocks.  It assumed none of them were composed out of parts that were exposed or would change, and if there were exceptions the library authors were supposed to be sophisticated enough to COPY or COPY/DEEP where needed.</p>
<p>(IIRC, it would contaminate these blocks by putting the relative binding information in them, so they would be seen as corrupt if anyone managed to get access to them.)</p>
<p>But after bootstrap was over, it would replace FUNC with a version that deeply copied the bodies.</p>
<p>It invites a lot of accidents in the historical world to have a FUNC that doesn't copy its body deeply.  But there are some new possibilities that might be able to avoid accidents while still covering a lot of cases.  For instance, it could deeply protect the blocks and make them immutable.  This way, if a user ever found themselves bit by it they could add in their own COPY or COPY/DEEP at the relevant places.  But 95% or more of the time, you'd not need to and the system could speed up.</p>
<p>In any case, it's interesting that the relative binding information wouldn't be corrupting the binding information any longer, because it's just an optimization for unbound values... and any PICKs or FOR-EACHs would see relatively bound words as unbound.</p>
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/semantics-and-optimization-of-copying-function-bodies/2119">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/semantics-and-optimization-of-copying-function-bodies/2119</link>
          <pubDate>Tue, 16 Jan 2024 09:00:09 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2119</guid>
          <source url="https://forum.rebol.info/t/semantics-and-optimization-of-copying-function-bodies/2119.rss">Semantics and Optimization of Copying Function Bodies</source>
        </item>
        <item>
          <title>Performance Implications of Isotopic-FRAME!-is-Action</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Making the isotopic state of frames be <a href="https://forum.rebol.info/t/taking-action-on-function-vs-action/596/6"><em>"interpret this frame as an action when run through a word reference"</em></a> is a deep change.  The implications haven't been fully absorbed, and there are some things I noticed that create troubles for optimization.</p>
<p>Looking again at a simple example:</p>
<pre><code>&gt;&gt; f: make frame! :append
== make frame! [
    series: ~
    value: ~
    part: ~
    dup: ~
    line: ~
]

&gt;&gt; f.value: 5

&gt;&gt; append5: isotopic f
== ~make frame! [
    series: ~
    value: 5
    part: ~
    dup: ~
    line: ~
]~  ; isotope

&gt;&gt; append5 [a b c]
== [a b c 5]
</code></pre>
<p>That's cool, for sure.  And it removes the number of things we have to name in the system (I was particularly annoyed by having to differentiate "action" and "activation" when there was a separate ACTION! datatype).</p>
<h2>
<a name="but-what-if-the-frame-is-modified-1" class="anchor" href="https://forum.rebol.info#but-what-if-the-frame-is-modified-1"></a>But What If The Frame Is Modified...?</h2>
<p>Say the user writes this:</p>
<pre><code>&gt;&gt; append5 (f.series: [d e f], [a b c])
== ???
</code></pre>
<p>When running the specialization <strong>append5</strong> we'd already decided that the series parameter wasn't specialized and needed to be fulfilled.  But the user is <em>specializing the series parameter during the fulfillment</em>.</p>
<p>At best, this is semantically messy (and can manifest as hard-to-comprehend behavior when the example is less obvious than this one).  At worst, the internal bookkeeping of the evaluator gets confused and it crashes due to having the slot it's filling changed out from under it.</p>
<h2>
<a name="we-could-snapshot-the-frame-but-snapshots-arent-free-2" class="anchor" href="https://forum.rebol.info#we-could-snapshot-the-frame-but-snapshots-arent-free-2"></a>We Could Snapshot The Frame... But Snapshots Aren't Free</h2>
<p>If we are forced to make a snapshot of the frame state at the start of execution, then that means making a copy, which takes time and space.</p>
<p>The space isn't actually the problem...because we can just put the snapshot in the frame space we're already making for the function call.</p>
<p>Where we pay is that it effectively adds an extra traversal of the arguments.  We're traversing the space to make the copy (with some slots unspecialized and needing to be fulfilled).  Then we're traversing the space again to do the fulfillment of the unspecialized arguments.</p>
<p>With no snapshot, we could leave the memory for the frame cells completely garbage at the outset... and fill them as we go with either a fulfillment or a specialization.  Empirically, avoiding the snapshot could save as much as 5% of the runtime.</p>
<h2>
<a name="another-issue-no-moment-to-cache-optimizations-3" class="anchor" href="https://forum.rebol.info#another-issue-no-moment-to-cache-optimizations-3"></a>Another Issue: No Moment To Cache Optimizations</h2>
<p>When there was a separate "make an action! out of this frame" step, the action was a way of saying "freeze!" on the arguments, so they could no longer be changed.  So it avoids the problem of changes during fulfillment.</p>
<p>But it did something else: it meant properties of the arguments could be studied...to remember things like <em>"what's the first unspecialized argument slot"</em>.</p>
<p>(It might seem that it would be easy to find the first unspecialized argument slot.  But this isn't just a search from the beginning of the frame, because it's possible to reorder arguments or have partial specializations.  So it's tricky.)</p>
<h2>
<a name="how-about-just-freezing-the-frames-if-theyre-executed-4" class="anchor" href="https://forum.rebol.info#how-about-just-freezing-the-frames-if-theyre-executed-4"></a>How About Just Freezing The Frames If They're Executed?</h2>
<p>The simplest idea here is just to say that once you invoke a frame, it's frozen...you can't change its fields anymore.  Then that freezing process can do the caching of properties that accelerate action execution, and all is well.</p>
<p>If you want to keep a frame mutable, then COPY it before making it isotopic.  The issue is just that changes to the frame won't be seen by the isotope you made.</p>
<h2>
<a name="do-technicalities-like-this-make-me-question-frame-as-action-5" class="anchor" href="https://forum.rebol.info#do-technicalities-like-this-make-me-question-frame-as-action-5"></a>Do Technicalities Like This Make Me Question FRAME!-as-Action?</h2>
<p>Well, it's a delicate balance of choices, and I'm still feeling it out.</p>
<p>The worry is that the number of concerns being exposed to users is such that the properties of "the kind frame you can run is different enough that you basically have to think of it as a different type".</p>
<p>To avoid that in this case, it's better to do the freezing implicitly vs. saying that you need a special routine like RUNS to bless a frame as runnable.  This way, RUNS can be understood as simply taking in a plain frame! and giving back an isotopic frame!... something you could easily write yourself.</p>
<p>Crafting a uniformity of experience with FRAME! so it doesn't feel like it's making up for a missing ACTION! type is certainly something to continue to be mindful of.  But performance needs to be minded too at some point, so I'm doing what I can about that.</p>
            <p><small>2 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/performance-implications-of-isotopic-frame-is-action/2083">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/performance-implications-of-isotopic-frame-is-action/2083</link>
          <pubDate>Wed, 20 Dec 2023 02:09:21 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2083</guid>
          <source url="https://forum.rebol.info/t/performance-implications-of-isotopic-frame-is-action/2083.rss">Performance Implications of Isotopic-FRAME!-is-Action</source>
        </item>
        <item>
          <title>Executable Size circa 2023...and tweaking INLINE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>A modern Ren-C non-debug executable on Linux--with https and the libuv filesystem and networking code (which supports asynchronous file I/O etc.) is about 1.7 MB when it is built at an O2 level of optimization (optimize for speed).</p>
<p>When built at Os optimization it's about 1.2 MB, sacrificing 40% of the speed to get the compression.  (In the modern era, most people would say that the extra size isn't a big deal to get that much of a speed improvement.)</p>
<p>By comparison, an R3-Alpha Linux executable is about 0.56 MB at O2.  And a Red CLI-only binary on Linux is about 1.0 MB.</p>
<h2>
<a name="why-has-size-gone-up-1" class="anchor" href="https://forum.rebol.info#why-has-size-gone-up-1"></a>Why Has Size Gone Up?</h2>
<p>I've looked under the hood at the differences with R3-Alpha to see what accounts for the disparity with modern Ren-C.  libuv accounts for a couple 100k, and is worth it--it would be especially so if taking advantage of things like the async file I/O.</p>
<p>But the rest just generally comes down to the fact that it's about twice as much code.  If you enjoy using ADAPT or ENCLOSE or SPECIALIZE, well, there's code that implements it.  And it's a deeper, safer, far more advanced codebase that just does more.</p>
<h2>
<a name="i-actually-pared-out-about-600k-by-tweaking-inlining-2" class="anchor" href="https://forum.rebol.info#i-actually-pared-out-about-600k-by-tweaking-inlining-2"></a>I Actually Pared Out About 600K By Tweaking Inlining</h2>
<p>When I started looking at size, the O2 binary was like 2.4 MB.  That was more than I expected, so I decided to look under the hood into why.</p>
<p>I used Google's tool <a href="https://github.com/google/bloaty">Bloaty McBloatface</a> to get some insight, and to my surprise...some rather small functions had a disproportionate amount of code attributed to them.</p>
<p>It turned out that this was due to putting functions in header files and inlining them with <code>static inline</code>.  When I moved 5 of these functions into the .c files instead of the .h files, that saved 400k in one blow... and the executable only got 0.4% slower (four tenths of a percent) as a result.</p>
<p>Then I managed to make it so the C++ build was about 140K lighter by changing the <code>static inline</code> on the remaining functions to a macro of INLINE that's either <code>inline</code> in the C++ build, or <code>static inline</code> in the C build.</p>
<p>I guess the takeaway here is that even if you notice that something is getting bigger due to good reasons of having more code, it always pays to look under the hood a bit when you can.  A few hours of work can get some low-hanging fruit.</p>
<p>(Another takeaway is that being able to build a C codebase as C++--if you want to--continuously pays dividends...)</p>
<p>Here's some notes on the INLINE macro:</p>
<pre><code class="lang-auto">
//=//// INLINE MACRO FOR LEVERAGING C++ OPTIMIZATIONS /////////////////////=//
//
// "inline" has a long history in C/C++ of being different on different
// compilers, and took a long time to get into the standard.  Once it was in
// the standard it essentially didn't mean anything in particular about
// inlining--just "this function is legal to appear in a header file and be
// included in multiple source files without generating conflicts."  The
// compiler makes no particular promises about actually inlining the code.
//
// R3-Alpha had few inline functions, but mostly used macros--in unsafe ways
// (repeating arguments, risking double evaluations, lacking typechecking.)
// Ren-C reworked the code to use inline functions fairly liberally, even
// putting fairly large functions in header files to give the compiler the
// opportunity to not need to push or pop registers to make a call.
//
// However, GCC in C99 mode requires you to say `static inline` or else you'll
// get errors at link time.  This means that every translation unit has its
// own copy of the code.  A study of the pathology of putting larger functions
// in headers as inline with `static inline` on them found that about five
// functions were getting inlined often enough to add 400K to the executable.
// Moving them out of .h files and into .c files dropped that size, and was
// only about *0.4%* slower (!) making it an obvious win to un-inline them.
//
// This led to experimentation with C++ builds just using `inline`, which
// saved a not-insignificant 8% of space in an -O2 build, as well as being ever
// so slightly faster.  Even if link-time-optimization was used, it still
// saved 3% on space.
//
// The long story short here is that plain `inline` is better if you can use
// it, but you can't use it in gcc in C99 mode (and probably not other places
// like TinyC compiler or variants). So this clunky INLINE macro actually
// isn't some pre-standards anachronism...it has concrete benefits.
//
#if CPLUSPLUS_11
    #define INLINE inline
#else
    #define INLINE static inline
#endif
</code></pre>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061</link>
          <pubDate>Tue, 21 Nov 2023 05:02:05 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2061</guid>
          <source url="https://forum.rebol.info/t/executable-size-circa-2023-and-tweaking-inline/2061.rss">Executable Size circa 2023...and tweaking INLINE</source>
        </item>
        <item>
          <title>Intrinsics: Functions without Frames</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Redbol's historical type system really had only one design point: <em>be fast</em>.  There were 64 fundamental datatypes, and parameters of a function could either accept each datatype or not.  So a simple bitset of 64 bits was stored alongside each parameter, and checked when the function was called.  That was it.</p>
<p>Ren-C's richer design explodes the number of "types" in the system.  Not only are there more fundamental types, but antiform isotopes like <strong><code>~null~</code></strong> are variations on WORD!, but you don't want every function that takes a WORD! to take nulls...and you don't want to have the type checking be so broad as to take <strong><code>[antiform!]</code></strong> just because you want to be able to take nulls (because that would include splices, packs, etc.)</p>
<p>It's not just this reason that Redbol's type checking was too simple, but it forced my hand in coming up with some sort of answer.  <em>I couldn't think of any better idea than Lisp, which does type checking via functions ("predicates").</em>  So I rigged it up where if you want to say a function can take an integer or null, you can write <strong><code>[null? integer!]</code></strong>  You can freely mix LOGIC-returning functions with fundamental types, and we're no longer stuck with the 64 fundamental type limit.</p>
<h2>
<a name="isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1" class="anchor" href="https://forum.rebol.info#isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1"></a>Isn't It Slow To Call A List of Functions For Typechecking?</h2>
<p>It can be.  And in particular, it can be if you have to go through calling those functions twice.</p>
<p>Why twice?  Because of "coercion".  For example, if you pass a pack to a function that expects packs, you'll get the meta-pack:</p>
<pre><code>&gt;&gt; foo: func [^x [pack?]] [probe x]

&gt;&gt; foo pack [1 "hi"]
~['1 '"hi"]~
</code></pre>
<p>But if your function didn't want packs, but wanted the type the pack decays to, it has to work for that as well:</p>
<pre><code>&gt;&gt; bar: func [^x [integer?]] [probe x]

&gt;&gt; bar pack [1 "hi"]
'1 
</code></pre>
<p><em>Did the function want the meta form or the meta-decayed form?</em>  There's no way of knowing for sure in advance.  The method chosen is to offer the meta form first, and if that doesn't match then the decayed form is offered.</p>
<p>It didn't know before walking through the block of functions to typecheck that a pack wouldn't have been accepted.  So it had to go through offering the pack, and then offering the integer.</p>
<h2>
<a name="but-i-noticed-something-about-these-functions-2" class="anchor" href="https://forum.rebol.info#but-i-noticed-something-about-these-functions-2"></a>But I Noticed Something About These Functions...</h2>
<p>Typically these functions are very simple:</p>
<ul>
<li>
<p>They take one argument.</p>
</li>
<li>
<p>They can't fail.</p>
</li>
<li>
<p>They don't require recursive invocations of the evaluator.</p>
</li>
</ul>
<p>This led me to wonder how hard it would be to define a class of actions whose implementations were a simple C function with an input value and output value.  If you weren't in a scenario where you needed a full FRAME!, you could reach into the ACTION's definition and grab the simple C function out of it.  All these functions would use the same dispatcher--that would be a simple matter of proxying the first argument of a built frame to pass it to this C function.</p>
<p>I decided to call these <strong>"intrinsics"</strong>, which is named after a <a href="https://en.wikipedia.org/wiki/Intrinsic_function">trick compilers use</a> when they see certain function calls that they implement those functions via direct code inlining.  It's not a perfect analogy, but it's similar in spirit.</p>
<h2>
<a name="it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3" class="anchor" href="https://forum.rebol.info#it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3"></a>It Wasn't All That Hard To Implement (relatively speaking <img src="https://forum.rebol.info/images/emoji/twitter/roll_eyes.png?v=12" title=":roll_eyes:" class="emoji" alt=":roll_eyes:" loading="lazy" width="20" height="20"> )</h2>
<p>All of the native function implementations were assumed to have the same type signature, taking a frame as an argument.  I took away that assumption and added an /INTRINSIC refinement to the NATIVE function generator.  If it was an intrinsic, then the C function in the native table would take a single value argument and an output slot to write to.</p>
<p>So it's still one C function per native.  But if it's an intrinsic, then the function is not a dispatcher... the Intrinsic_Dispatcher() is used, and the C function is poked into the properties of the function.</p>
<p>Callsites that want to optimize for intrinsics just look to see if an action has the Intrinsic_Dispatcher(), and if so they have to take responsibility for procuring an argument and type checking it.  But if they do, they can just call the C function directly with no frame overhead.</p>
<p><strong>This helps make the switchover to functions in type spec blocks much more palatable.</strong>  It's never going to be as fast as the bitset checking, but it's fast enough to allow things to make progress.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/intrinsics-functions-without-frames/2050">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/intrinsics-functions-without-frames/2050</link>
          <pubDate>Sun, 15 Oct 2023 17:25:32 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2050</guid>
          <source url="https://forum.rebol.info/t/intrinsics-functions-without-frames/2050.rss">Intrinsics: Functions without Frames</source>
        </item>
        <item>
          <title>Is The Script Compression Feature Necessary?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>R3-Alpha introduced the option that when you SAVE a script, you can ask that it be compressed.</p>
<p>It doesn't compress the header...just the body of the script.  There were two options for how this body could be compressed after the header: either as a Base64 BINARY literal ("script compression"), or directly as gzip'd bits ("raw compression").</p>
<p>As an example:</p>
<pre><code>&gt;&gt; data: save/compress blank [1 &lt;two&gt; "three"] 'script
== #{
    5245424F4C205B0A202020204F7074696F6E733A205B636F6D70726573735D0A
    5D0A3634237B483473494141414141414141436A4E5573436B707A3764545543
    724A4B45704E56654943414E425746325951414141417D
}

&gt;&gt; print as text! data
REBOL [
     Options: [compress]
]
64#{H4sIAAAAAAAACjNUsCkpz7dTUCrJKEpNVeICANBWF2YQAAAA}

&gt;&gt; [body header]: load data
== [1 &lt;two&gt; "three"
]

&gt;&gt; body
== [1 &lt;two&gt; "three"
]

&gt;&gt; header
== make object! [
    Title: "Untitled"
    File: ~null~
    Name: ~null~
    Type: 'script
    Version: ~null~
    Date: ~null~
    Author: ~null~
    Options: [compress]
    Description: ~null~
]
</code></pre>
<h2>
<a name="rebol2-didnt-have-it-red-doesnt-have-it-1" class="anchor" href="https://forum.rebol.info#rebol2-didnt-have-it-red-doesnt-have-it-1"></a>Rebol2 Didn't Have It, Red Doesn't Have It...</h2>
<p>Arguments that it helps with transmitting over networks don't hold up much these days, because the HTTP protocol itself does compression.</p>
<p>Plus, keeping scripts in compressed form is an annoying form of opaqueness on a language that's supposed to be about simplicity.</p>
<p>I've kept it around just because there were tests for it, and it exercised compression code (including showcasing a really bad design method of trying to decompress garbage to see if it was the raw compressed form, causing a crazy memory allocation).  But I'm not sure what the compelling use case for this feature is.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044</link>
          <pubDate>Thu, 27 Jul 2023 23:20:14 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-2044</guid>
          <source url="https://forum.rebol.info/t/is-the-script-compression-feature-necessary/2044.rss">Is The Script Compression Feature Necessary?</source>
        </item>
        <item>
          <title>Python Speedup Proposals</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>The original "CPython" implementation is in some ways similar to Rebol/Red... though these days Ren-C is more aligned with the stackless Python implementation...which is also written in C, but distinct from CPython.</p>
<p><em>(But Ren-C's design overall is a lot less comparable to anything, due to the number of very "alien" ideas in it, but that I think are what gives it more notable properties.)</em></p>
<p>In any case, despite running an interpreter loop and doing kind-of-what-Rebol-does, they've committed themselves to doing some speedup proposals and apparently it's paying off:</p>
<p><a href="https://devblogs.microsoft.com/python/python-311-faster-cpython-team/" class="inline-onebox">A Team at Microsoft is Helping Make Python Faster - Python</a></p>
<p>Some of their proposals involve JIT-compiling things (which they know won't work on restrictive platforms like iOS).  But they apparently have done a lot of tweaks besides that which have turned out beneficial.  Because it's a C interpreter there might be something applicable to be learned by looking at their "Stage 1" and "Stage 2" changes.</p>
<blockquote>
<h3>Stage 1 -- Python 3.10</h3>
<p>The key improvement for 3.10 will be an adaptive, specializing interpreter. The interpreter will adapt to types and values during execution, exploiting type stability in the program, without needing runtime code generation.</p>
<h3>Stage 2 -- Python 3.11</h3>
<p>This stage will make many improvements to the runtime and key objects. Stage two will be characterized by lots of "tweaks", rather than any "headline" improvement. The planned improvements include:</p>
<ul>
<li>Improved performance for integers of less than one machine word.</li>
<li>Improved peformance for binary operators.</li>
<li>Faster calls and returns, through better handling of frames.</li>
<li>Better object memory layout and reduced memory management overhead.</li>
<li>Zero overhead exception handling.</li>
<li>Further enhancements to the interpreter</li>
<li>Other small enhancements.</li>
</ul>
<h3>Stage 3 -- Python 3.12 (requires runtime code generation)</h3>
<p>Simple "JIT" compiler for small regions. Compile small regions of specialized code, using a relatively simple, fast compiler.</p>
<h3>Stage 4 -- Python 3.13 (requires runtime code generation)</h3>
<p>Extend regions for compilation. Enhance compiler to generate superior machine code.</p>
</blockquote>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/python-speedup-proposals/1992">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/python-speedup-proposals/1992</link>
          <pubDate>Sun, 27 Nov 2022 10:05:19 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1992</guid>
          <source url="https://forum.rebol.info/t/python-speedup-proposals/1992.rss">Python Speedup Proposals</source>
        </item>
        <item>
          <title>Boot Footprint: Giant String Literal vs. Encap?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>One thing you can do with C is embed literal data.  This is how R3-Alpha ships with its mezzanine functions "built in", the prep process stores everything in a big compressed array of bytes called (misleadingly) <strong><code>Native_Specs</code></strong>:</p>
<p><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/b-init.c#L166">https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/b-init.c#L166</a></p>
<p>The name being <code>Native_Specs</code> might suggest it was the contents of <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/natives.r">%natives.r</a>.  But it's actually a lot more, with glued-together source code... including all of the contents of the <a href="https://github.com/rebol/rebol/tree/master/src/mezz">%base-xxx.r, %sys-xxx.r, and %mezz-xxx.r</a> files.  So I renamed it to <code>Boot_Block_Compressed</code>.</p>
<p>But it doesn't embed the files as-is... it LOADs them and SAVEs them using an already-built version of R3.  This round-tripping removes the comments and normalizes the spacing.  It also actually scrambled it with CLOAK for whatever reason--a waste of time because you could read all the code with SOURCE if you felt like it.  :-/</p>
<p><em>(Ren-C doesn't use an old-R3's LOAD+SAVE to strip out comments, because it would lock down the format.  Your hands would be tied on adding or changing lexical forms in the sys/base/mezzanine.  So it has its own STRIPLOAD function that does a light stripping out of comments and spaces for this glue-files-together purpose)</em></p>
<h2>Is Embedding Big Fat C Constants Supported By The Standard?</h2>
<p>C compilers are only <em>required</em> to allow you to build in string literals that are <a href="https://stackoverflow.com/a/11488687">509 characters in C89, and 4095 characters in C99</a>.  They can allow more, but don't have to.</p>
<p>So I recall R3-Alpha having problems when you turn up <code>--pedantic</code> warning levels by using a syntax like:</p>
<pre><code>const char Native_Specs[] = "\x01\x02\x03...";
</code></pre>
<p>That warning went away when I changed it to:</p>
<pre><code>const unsigned char Boot_Block_Compressed[] = { 0x01, 0x02, 0x03 ...};
</code></pre>
<p>Regarding the problem of hitting length limits, Ren-C actually breaks things up a bit more...because each extension has its own constant declaration like this for its Rebol portion.</p>
<p>Because this code is decompressed and scanned once--and then tossed--there's probably a number of experiments that could be done.  What if the blob were loaded as mutable data, and then used as some kind of buffer for another purpose?  Is there some way to help hint to the OS that you really are only going to use the information only once so it will throw out the page from memory?  Or will the right thing happen to scan it and use it just once?</p>
<p>Long story short--it hasn't been a problem, even with the TCC build.  So it has been taken for granted that it works acceptably.</p>
<h2>But Would Encapping Be Better?</h2>
<p>One vision of how the boot would work is that it would only load enough to get de-encapping working.  Then the de-encapping would be how all the blobs for the "built-in" extensions were extracted.</p>
<p><em>This seems like an interesting vision,</em> because if someone gave you a big fat Ren-C and you wanted any skinnier version, you could basically ask it to cut everything out you don't want and give you a new EXE.  You could roll it up with any customizations you like.</p>
<p>But if you're using any "real" form of encapping (e.g. manipulating the resource portions of a Linux ELF file or a Windows PE file) this gets complicated.  And Ren-C's encap facilities are <a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb">written in usermode</a>...so that expects things like file I/O and PARSE of a BINARY!, etc.  I also assume that unzip facilities would be part of encapping.  So you need a reasonably runnable system just to get to that point.</p>
<p><strong>I've punted on worrying too much about this, because of the focus on the web build.</strong></p>
<p>It would be a bad investment of limited resources to handwrite and maintain encapping code in C, just so that encapping can be the means by which more of the bootstrap can be done with encap.</p>
<h2>Script Code Is Easy to Encap, EXE/DLL Code Is Not</h2>
<p>So the "easy" part would be changing the build to go in two steps.</p>
<p>The first step would make an r3-one.exe that is capable of augmenting itself with encapped data.  The second step would ask that r3 to fold in various scripts and resources to make an r3-two.exe that had more things in it...such as a console.</p>
<p>This isn't that far out to accomplish.  <strong>The hard part is when what you're encapping isn't script data, but compiled and executable C code...like bits from a DLL.</strong>  e.g. encapping "extensions".</p>
<p>What some people do in this situation is to actually glue the DLL file into the executable, but extract it to the filesystem and load the extracted version.  If you Google around for "using a DLL as an embedded resource" you'll find people who've done such things...but the answers you find will be from over a decade ago, because no one cares about how they ship such things anymore.</p>
<h2>Making Encap A Dependency Is Probably Unwise...</h2>
<p>It isn't going to be a terribly big win for bootstrap if it can't be used to pull out or put in extensions.</p>
<p>I don't think it's wise to pursue handcrafted C de-encapping.  In fact there's no way I'd be writing any kind of encap code right now if it weren't already made.  Kind of the only reason we have the usermode encapping around is because Atronix was using it, but I was trying to keep the feature but cut it out of the C.  It hasn't been tossed entirely because it functions as test code.</p>
<p>We <em>could</em> make a token two-step build (the phase one executable, that uses the phase one to build a phase two with encapped data in it).</p>
<p>But it seems what we might want more is an easy option to not build in encapping whatsoever, and have more control over options at build time than the current list of extensions.</p>
<p>For the limited audience looking at desktop builds--I imagine the answer will be that if you want a differently-sized r3.exe, you do it with a C compiler and ticking different boxes.  Or you build everything as a DLL and accept it's not all one file.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977</link>
          <pubDate>Sat, 24 Sep 2022 01:57:39 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1977</guid>
          <source url="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977.rss">Boot Footprint: Giant String Literal vs. Encap?</source>
        </item>
        <item>
          <title>Influences On Startup Time And Memory Use</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Right now it's not ideal to be focusing on things like startup time and memory use.  There are a lot of fundamental features being rethought--and recall that rules of optimizing code at the cost of clarity and flexibility of design are:</p>
<ul>
<li>
<p>Rule <span class="hashtag">#1:</span> Don't Do It</p>
</li>
<li>
<p>Rule <span class="hashtag">#2</span> (Experts Only) Don't Do It... Yet.</p>
</li>
</ul>
<p>...BUT, the issues can't be ignored forever.  And it's reasonable for one to ask why there's been a dramatic increase in boot time and memory use between the build being used for bootstrap and a current commit.</p>
<p>So it's worth having a thread here to track some of what's involved.</p>
<h2>ENCAP Detection</h2>
<p>By default we still run encap detection on all desktop builds, scanning the executable.  On Windows I think Shixin's version loads the whole binary into memory, and on Linux it still does quite a lot.</p>
<p>You can skip the detection by using <code>--no-encap</code>.</p>
<p><a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb">https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb</a></p>
<p>But the encap and de-encapping tools will still be bundled in the executable.  They're not an extension, so if you don't want to pay for that...you need to entirely remove <a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977">early-boot modules like encap and unzip</a> which are <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/src/main/prep-main.reb#L45">built in another way</a></p>
<p>Obviously platform-specific C code would be faster and lighter than PARSE.  And there was some before, but it entangled things in the core with FILE I/O...and it was dedicated finicky C for a purpose we're not really focusing on, especially in the web build.</p>
<p>The decision to move encapping to userspace tools was mine, and not something I regret.  But since we're not using it, all it's really doing is acting as a test.  I've made a separate thread to talk about the fate of Encap, and whether we should depend on it more or distance from it further:</p>
<p><a href="https://forum.rebol.info/t/boot-footprint-giant-string-literal-vs-encap/1977" class="inline-onebox">Boot Footprint: Giant String Literal vs. Encap?</a></p>
<h2>A Big Cost Is Going To Come From UPARSE</h2>
<p>UPARSE right now is an elaborate exercise of the ability to build complex feature-filled dialects in userspace.  And it does so at great cost to the evaluator.</p>
<p>Of course the plan is to cut that down, because COMBINATORs are just functions.  They could be written as natives.  And even more importantly, the process of <em>combinating itself</em> needs to be native.</p>
<p>I have done some experiments with this:</p>
<p><a href="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636" class="inline-onebox">Progress on Nativizing Parser Combinators</a></p>
<p>But those experiments are currently inactive, because the design needed more work.  And it's easier to churn through that work with userspace code.</p>
<p>What can we do about it? Well until UPARSE goes through an optimization phase, we can just use PARSE3 in boot...or at least for whatever subsetted codebase is in this metric.  The main thing is just to get it measured so we know how much of this is known UPARSE-ism vs. other unknowns.  I'm going to bet it's a lot...even though it's not used all that terribly much in boot, it's going to be big.</p>
<p>Cutting it out for the moment would at least help focus on the next bigger things.</p>
<h2>Another Pain Point Is Going to be GET+SET Atop PICK+POKE</h2>
<p>I spent quite a while working through what a GET and SET and PICK and POKE actually were.  Ultimately I concluded:</p>
<ul>
<li>
<p>GETs are just sequences of individual PICK steps (where a GET of a WORD! starts the chain with the binding of the word, and PICKs the word out of that object)</p>
</li>
<li>
<p>SETs are a sequence of PICK steps which are kept track of...followed by POKE.  That POKE can return nothing (in which case you're done) or it can return an adjusted value.  If the value needed to be adjusted that means it then gets POKE'd back into the cell back in the chain, and this ripples back so long as the bitpattern in cells need to be adjusted.</p>
</li>
</ul>
<p>I haven't gone back to this prototype and optimized it.  That means it quite literally is building evaluation chains of PICK and POKE every time it does tuple processing (what would be "path picking", e.g. variables out of objects).  I wasn't sure if this was the answer or not, so it seemed best to keep it general to be able to play with it.</p>
<p>It's tough to know how much "hardening" should be done on this.  It's nice to be able to hijack and hook and bend things.  I think I still want to consider it to be calls to PICK and POKE, but we can do those calls via frames built just for those functions...and not generic evaluation.  I'll have to look at it.</p>
<h2>Each Extension Adds Memory Use, But Also Has Startup Code</h2>
<p>By default the desktop includes every extension, even for making animated GIFs...as well as currently</p>
<p>If one wants to make a non-kitchen-sink test build of Ren-C...obviously use <code>debug: none</code>, and <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/configs/default-config.r#L23">chopping extensions out with <strong><code>-</code></strong> instead of <strong><code>+</code></strong></a>, for starters.  Note that extensions can be <a href="https://github.com/metaeducation/ren-c/actions/runs/3056527403/jobs/4930783888#step:22:3">built as separate DLL/.so with <strong><code>*</code></strong></a></p>
<h2>Other Factors Need Managing On a Case-by-Case Basis</h2>
<p>Those would be among the only things that can be done without <em>some</em> attention to the C, which hasn't been vetted for this metric in years.  But it isn't a priority right at this exact moment--there are much more important things.</p>
<p><em>(If you want some of my general philosophy about why Ren-C will be competitive with R3-Alpha despite "increased complexity", then seeing some <a href="https://forum.rebol.info/t/the-now-even-more-special-specialize/588">old stats on SPECIALIZE might be illuminating</a>)</em></p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972</link>
          <pubDate>Sun, 18 Sep 2022 22:11:11 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1972</guid>
          <source url="https://forum.rebol.info/t/influences-on-startup-time-and-memory-use/1972.rss">Influences On Startup Time And Memory Use</source>
        </item>
        <item>
          <title>Rethinking The Stale Bit: Invisibility In The Isotopic Age</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Every evaluation step is asked to target an output cell.  Before the evaluation, a single bit is set on that cell to say it is "stale".  So if the evaluation doesn't write to it--and confirms that wasn't a mistake--then the old result is left around to recover.</p>
<p>The ability to recover the previous result with the flip of a bit is used for invisibility.  It's used not just in the evaluator when it goes step by step, but also in things like ANY and ALL.</p>
<pre><code>&gt;&gt; 1 + 2 comment "in the evaluator"
== 3

&gt;&gt; all [1 + 2 comment "here too"]
== 3
</code></pre>
<p><em>Doing it this way has sacrificed some features.</em>  For instance, you can't make an "invisible" enfix function:</p>
<pre><code>&gt;&gt; foo: enfix func [left] [
       print ["Left was" left]
       return void
   ]

&gt;&gt; 304 1020 foo
Left was 1020
== 1020
</code></pre>
<p>The 1020 from the previous evaluation was used as an argument.  But after "consuming all its arguments" the product of FOO could not leave the 304 there.  If each evaluation step was to a temporary cell, that temporary cell could be used to fill the enfix slot of FOO... and 304 could be left.</p>
<p><em>Doing it this way has also required acrobatics to accomplish non-negotiable features.</em>  The related problem of making it possible for an enfix function to perceive voidness on the left hand side requires stale bit mechanics that aren't for the faint of heart... e.g. to differentiate these two cases:</p>
<pre><code>&gt;&gt; (else [print "Won't work"])
** Error

&gt;&gt; () else [print "Will work"]
Will work
</code></pre>
<h2>Is All The Bit-Fiddling Worth It Vs. Copying?</h2>
<p>Considering the small size of cells (4 platform pointers), the logic to test and clear the "stale" bit may seem to add overhead and complexity that isn't saving that much.  Instead, every evaluation could be done into a temporary slot...and then if not invisible, the 4 pointers could be moved.</p>
<p>This is actually a bit misleading--because copying cells is actually a bit more expensive in the general case.  Cell format flags have to be checked, bindings may need to be managed, and if a reference count mechanic is implemented this could make it all worse.</p>
<p>Less copying is desirable, and it seems neat to have achieved invisibility thus far without needing an extra eval per-eval-step.</p>
<blockquote>
<p><em>"So if the evaluation doesn't write to it--and confirms that wasn't a mistake..."</em></p>
</blockquote>
<p>This is one of the main reasons I've stuck with the current method.  It's useful for debug purposes to know if a native just forgot to write an output cell anyway.  So I figured: <em>"so long as the output cell is going to have a flag on it saying it hasn't been written to yet, why not make that flag able to coexist with the previous value...and hence avoid a mechanic of needing to copy every time?"</em></p>
<h2>But Isotopes Mean It's Time For Change</h2>
<p>Early on I observed that there was no way to get this to work:</p>
<pre><code>&gt;&gt; 1000 + 20 if true [comment "hi"]
== 1020  ; not possible
</code></pre>
<p>The IF had to produce something as a proxy for VOID that wasn't void... in order to signal a taken branch (we want THEN to run).</p>
<p>But even if that proxy was able to <em>decay</em> to a void state, it was too late.  It had overwritten the output.  Today that proxy is a parameter pack with a meta-void in it: <strong><code>~[~]~</code></strong>.</p>
<p>There's more stuff with parameter packs that <em>should</em> work, like this:</p>
<pre><code> &gt;&gt; 1000 + 20 [x @y]: pack [304 void]
 == 1020
</code></pre>
<p><em>And isotopic objects that represent lazy evaluations should be able to produce void, too.</em>  They're a proxy for behavior, and if you pick and choose behaviors that could be accomplished with a normal result that a REIFY method on a lazy object can't, you're saying they're not as powerful.</p>
<p><strong>These features tip the scales.</strong>  And really, the circuitous nature of void enfix handling was already tipping them.</p>
<p>The concerns over copying are mostly addressed by something I'm calling "cell movement"; this means we can really get closer to the 4 platform pointer copies, because you're destroying the old cell in the process.  So if techniques like reference counting came along, you're not adding and removing them--you're just letting the new cell take over the resources of the old.</p>
<p>Plus, detecting whether a cell has been written to or not is a generic debug feature now that has easy coverage.</p>
<p><em>The stale bit is thus on the chopping block.</em>  So expect more robust void-related behavior coming soonish.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963</link>
          <pubDate>Thu, 08 Sep 2022 21:34:36 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1963</guid>
          <source url="https://forum.rebol.info/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963.rss">Rethinking The Stale Bit: Invisibility In The Isotopic Age</source>
        </item>
        <item>
          <title>Incomplete TRANSCODEs: Actually an Optimization Problem</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C has a multi-return interface for TRANSCODE.  Without /NEXT, you get the whole thing:</p>
<pre><code>&gt;&gt; transcode "abc def"
== [abc def]
</code></pre>
<p>With the /NEXT refinement, it will go one item at a time.  But the return convention is that you receive back a the remainder as the primary return result, and the transcoded value is the second:</p>
<pre><code>&gt;&gt; [pos value]: transcode/next "abc def"
== " def"

&gt;&gt; pos
== " def"

&gt;&gt; value
== abc
</code></pre>
<p>Of course, with multi-return you can ask for the overall return result to be the synthesized value:</p>
<pre><code>&gt;&gt; [pos @value]: transcode/next "abc def"
== abc
</code></pre>
<p>You don't even have to name things if you don't want them!</p>
<pre><code>&gt;&gt; [_ @]: transcode/next "abc def"
== abc
</code></pre>
<p>And you can just use a regular SET-WORD! to get just the primary result.</p>
<pre><code>&gt;&gt; pos: trancode/next "abc def"
== " def"
</code></pre>
<p><strong>You know that you're at the end of the input when it returns NULL.</strong>  This means there was no value synthesized, and you're done.</p>
<pre><code>&gt;&gt; [pos /value]: transcode/next ""
== ~null~  ; anti

&gt;&gt; pos
== ~null~  ; anti

&gt;&gt; value
== ~null~  ; anti
</code></pre>
<p>Writing foolproof loops to process items are a breeze:</p>
<pre><code>while [[utf8 /item]: transcode utf8]
    print mold item
]
</code></pre>
<p>The leading slash on <code>/item</code> is necessary when you want to accommodate the case where transcode didn't produce any item.  Because then it doesn't return a 2-parameter pack, it just returns a pure null.  This is required for clean interoperability with THEN and ELSE...because nulls in packs are considered to be "something" vs. "nothing".  Multi-return unpacking requires you to demonstrate consciousness when you are trying to unpack more items than you're getting, hence the slash is needed when trying to unpack a singular null into two slots.</p>
<p>On the plus side, if you are expecting that there must be a transcoded item, then you get a free check by eliminating the slash...it will then cause an error if the item isn't produced!</p>
<p><img src="https://forum.rebol.info/images/emoji/twitter/+1.png?v=12" title=":+1:" class="emoji only-emoji" alt=":+1:" loading="lazy" width="20" height="20"></p>
<h2>
<a name="this-runs-circles-around-red-and-r3-alpha-1" class="anchor" href="https://forum.rebol.info#this-runs-circles-around-red-and-r3-alpha-1"></a>This Runs Circles Around Red and R3-Alpha</h2>
<p>For starters: neither support strings as input--because the scanner is built for reading UTF-8 files...and both R3-Alpha and Red unpack strings into fixed-width encodings.  So if you have string input, you have to pay for a copy encoded as UTF-8 via TO BINARY!.  (<a href="https://forum.rebol.info/t/realistically-migrating-rebol-to-utf8-everywhere/374">Ren-C's UTF-8 Everywhere</a> wins again!)</p>
<p>R3-Alpha unconditionally returns a block with the last element as a remainder, whether you ask for one item via /NEXT or not:</p>
<pre><code>r3-alpha&gt;&gt; transcode to binary! "abc def"
== [abc def #{}]

r3-alpha&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

r3-alpha&gt;&gt; transcode/next to binary! ""
== [#{}]
</code></pre>
<p>So if you were transcoding an entire input, you have to TAKE/LAST an always-empty binary off of the result.</p>
<p>But you are using /NEXT you have to PICK out the element from the start of the array and the remainder from the end.  But you need to notice the exception of no-value-produced where the block is length 1 instead of 2.</p>
<p>That's awkward, but as usual... <em>Red somehow manages to make an incompatible interface that is as much worse as it is better:</em></p>
<p>The better part is that if you don't ask for /NEXT you just get the block back, like in Ren-C:</p>
<pre><code>red&gt;&gt; transcode to binary! "abc def"
== [abc def]
</code></pre>
<p>But the /NEXT interface is outright broken:</p>
<pre><code>red&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

red&gt;&gt; transcode/next to binary! ""
== [[] #{}]
</code></pre>
<p>It might look better because you don't have to guess about which position to find the remainder in--it's always in the second slot.  But it has a fatal flaw: you can't distinguish the result state of scanning <code>"[]"</code> and any string with nothing but comments and whitespace.</p>
<p>Consider this very basic loop to scan one item at a time and print it:</p>
<pre><code>red&gt;&gt; utf8: to binary! "abc def"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print mold item
]
abc
def
</code></pre>
<p>You get two items.  But what if you had something that was--say--a comment:</p>
<pre><code>red&gt;&gt; utf8: to binary! "; I'm just a comment"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print ["Item is:" mold item]
]
Item is: []
</code></pre>
<p>You get one spurious item.  (They chose BLOCK! for the item, but it wouldn't matter what it was--a NONE! would be just as bad, you're just losing the distinction between empty strings and <code>"#[none]"</code> then.)</p>
<p><strong>If I were prescribing a solution for Red I'd suggest approximating Ren-C's solution as closely as possible.</strong></p>
<p>When /NEXT is used have it take a variable to write the transcoded value into.  Then return the position.  If the scan turns out to have no product, return NONE.  For consistency with Ren-C you might set the transcoded value to NONE as well <em>(it doesn't matter, because the return of none signals whatever it is isn't meaningful...so use UNSET! if you want.)</em></p>
<pre><code>while [utf8: transcode/next utf8 'item] [
    print mold item
]
assert [none? item]  ; or unset, or whatever
</code></pre>
<p>Not as nice as the multi-returns, and you can't duck out of passing the variable if you aren't interested.  But... <img src="https://forum.rebol.info/images/emoji/twitter/man_shrugging.png?v=12" title=":man_shrugging:" class="emoji" alt=":man_shrugging:" loading="lazy" width="20" height="20"></p>
<h2>
<a name="ren-c-also-thrashes-r3-alpha-and-red-in-error-handling-2" class="anchor" href="https://forum.rebol.info#ren-c-also-thrashes-r3-alpha-and-red-in-error-handling-2"></a>Ren-C Also <em>Thrashes</em> R3-Alpha and Red In Error Handling</h2>
<p>Ren-C TRANSCODE has these potential behaviors:</p>
<ul>
<li>
<p>RETURN a BLOCK! (if plain TRANSCODE)</p>
</li>
<li>
<p>RETURN a PACK of the <strong>~[remainder value]~</strong> if TRANSCODE/NEXT) -or- RETURN NULL if no value was transcoded from the input (empty string, comments, just spaces, etc.)</p>
<ul>
<li>
<p>Having remainder as the primary return means you can check the default result in a loop for truthiness and loop easily using WHILE or whatever.</p>
</li>
<li>
<p>Returning pure NULL when no value is transcoded means you can react to there being nothing to transcode with THEN and ELSE, etc.</p>
</li>
</ul>
</li>
<li>
<p>It can do a "hard FAIL"</p>
<ul>
<li>
<p>This would happen if you asked something fundamentally incoherent...like asking to TRANSCODE an input that was non-UTF-8...like a GOB!, or something like that</p>
</li>
<li>
<p>Such errors are only interceptible by a special SYS.UTIL.RESCUE method--they are not supposed to be easy to gloss over and unlikely to have meaningful mitigation.  So only special sandboxing situations (like writing consoles that print out the error) are supposed to trap them.</p>
</li>
</ul>
</li>
<li>
<p>It can RETURN an <em>antiform ERROR!</em> ("raised error") if something went wrong in the transcoding process itself</p>
<ul>
<li>
<p>This would be something like a syntax error, like  if you asked <strong>transcode "a bc 1&amp;x def"</strong></p>
</li>
<li>
<p>These will be promoted to a hard FAIL if the immediate caller doesn't do something to specially process them.</p>
</li>
<li>
<p>You can casually ignore or intercept these, because you can be confident that it was a formal return result of the thing you just called--not some deeper problem like a random typo or other issue.</p>
</li>
</ul>
</li>
</ul>
<p>I won't rehash the entire <a href="https://forum.rebol.info/t/fail-vs-return-raise-the-new-age-of-definitional-failures/1852">"why definitional errors are foundational"</a> post, but TRANSCODE was one of the first functions that had to be retrofitted to use them.</p>
<pre><code>&gt;&gt; transcode "a bc 1&amp;x def" except e -&gt; [print ["Error:" e.id]]
Error: scan-invalid
</code></pre>
<p><strong>The definitionality is extremely important!</strong>  I spent a long time today because in the bootstrap shim I had a variation of transcode...parallel to this in R3-Alpha:</p>
<pre><code>r3-alpha&gt;&gt; transcode: func [input] [
               prnit "My Transcode Wrapper"  ; oops, typo
               return transcode input
           ]

r3-alpha&gt;&gt; if not attempt [transcode to binary! "abc def"] [print "Bad input"]
Bad input
</code></pre>
<p><em><strong>But the input isn't bad!!!</strong></em>  This leads to a nightmare of trying to figure out what was going wrong.  Today's particular nightmare was when tinkering with the shim implementation of TRANSCODE.  A bug in the shim was leading to silently skipping work that should have been done, because the caller wanted to be tolerant of bad transcode input.</p>
<p><strong>There's simply no practical way of working on code of any complexity without something like definitional failures, and experience has proven this day after day.</strong></p>
<h2>
<a name="getting-incomplete-results-via-r3-alphas-error-3" class="anchor" href="https://forum.rebol.info#getting-incomplete-results-via-r3-alphas-error-3"></a>Getting Incomplete Results Via R3-Alpha's /ERROR</h2>
<p>R3-Alpha offered this feature:</p>
<pre><code>/error -- Do not cause errors - return error object as value in place
</code></pre>
<p>The intended use is that you might want the partial input of what had been successfully scanned so far.  If the code went and raised an error, you could trap that error.  But you wouldn't have any of the scanned items.</p>
<p>It would put it any ERROR! as the next-to-last item in the block, with the remainder after that:</p>
<pre><code>&gt;&gt; transcode/error to binary! "a bc 1&amp;x def"
== [abc make error! [
    code: 200
    type: 'Syntax
    id: 'invalid
    arg1: "pair"
    arg2: "1&amp;x"
    arg3: none
    near: "(line 1) a bc 1&amp;x def"
    where: [transcode]
] #{20646566}]

&gt;&gt; to string! #{20646566}
== " def"  ; wait...why isn't 1&amp;x part of the "remainder"
</code></pre>
<p>It's clumsy to write the calling code (or to read it...testing to see if the next-to-last-item is an ERROR! and reacting to that.</p>
<p><em>(Also: What if there was some way to represent ERROR! values literally in source?  This would conflate with such a block that was valid...but just incidentally had an ERROR! and then a BINARY! in the last positions.)</em></p>
<p>But the thing that had me most confused about it was the remainder.  Notice above you don't get  <code>1&amp;x</code> as the start of the stuff it couldn't understand.</p>
<p>Was it trying to implement some kind of recoverable scan?  What would that even mean?  <img src="https://forum.rebol.info/images/emoji/twitter/thinking.png?v=12" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"></p>
<p><strong>Ultimately I think this was just a leaking of an implementation detail as opposed to any reasonable attempt at recoverable scanner.</strong>  It only didn't tell you where the exact tail of the successfully scanned material was because it did not know.</p>
<p>The scanning position is based on token consumptions, and so if you started something like a block scan and it saw a <strong>[</strong> then it forgets where it was before that.  Then if something inside the block goes bad, it will just give you a remainder position somewhere inside that--<em>completely forgetting about how many nesting levels it was in</em>.</p>
<p>So what you were getting was a crappier implementation of scanning one by one, and remembering where you were before the last bad scan:</p>
<pre><code>pos: input
error: null
block: collect [
   while [true] [
       keep [pos @]: transcode pos else [
           break
       ] except e -&gt; [
           error: e
           break
       ]
   ]
]
</code></pre>
<p>That gives you a proper version, setting error if something happened and giving you the block intact.</p>
<h2>
<a name="so-finally-we-see-its-an-optimization-problem-4" class="anchor" href="https://forum.rebol.info#so-finally-we-see-its-an-optimization-problem-4"></a>So Finally... We See It's An Optimization Problem</h2>
<p>Question is if there's some way of folding this into TRANSCODE, so it's doing the looping and collecting efficiently for you.  What would the interface be like that gave you back the error, and how would you know to remember to check it?</p>
<p>The problem is that when you return a raised definitional error from a function, that's the only thing you return.  How would you return partial results (and maybe a resumption position) as well?</p>
<p>A /TRAP refinement could cause another variation in how the return results are given:</p>
<pre><code>&gt;&gt; [error block]: transcode/trap "a bc"
; null

&gt;&gt; block
== [a bc]

&gt;&gt; [error block]: transcode/trap "a bc 1&amp;x def"
== make error! [...]

&gt;&gt; block
== [a bc]
</code></pre>
<p>Having the error be first seems good, lining up with TRAP.  Then the block as the second result.</p>
<p>That's not bad, but it would require some implementation reworking that I don't have time for.  Problem is that how the scanner is written now it clears all the stack out when an error gets raised, and there'd have to be some flag to tell it to persist the data stack accruals despite unwinding the level stack.  It's not rocket science it's just not important right now.</p>
<h2>
<a name="answer-for-now-kill-off-error-5" class="anchor" href="https://forum.rebol.info#answer-for-now-kill-off-error-5"></a>Answer For Now: Kill Off /ERROR</h2>
<ul>
<li>
<p>The answer /ERROR has been giving back in error cases for the remainder is sketchy and conflates potential literal error scanning with a scanning error.</p>
</li>
<li>
<p>You can get the behavior 100% reliably just by intercepting errors going one transcode item at a time.</p>
<ul>
<li>Bear in mind that one-at-a-time is only going one <em>top-level</em> item at a time.  If you scan a block with 1000 items in it, that's one transcode step.  So we're not really talking about that many steps most of the time with regards to the scale of a file.</li>
</ul>
</li>
<li>
<p>This is a good opportunity to write tests of item-by-item scanning with error handling</p>
</li>
<li>
<p>Red added a bunch of refinements on transcode [/next /one /prescan /scan /part /into /trace], and they didn't pick up /error themselves</p>
</li>
</ul>
<p>Speaking of adding lots of refinements: I also want to get away in general from investments in weird C scanner code and hooks (<em>especially</em> if it's just an optimization).</p>
<p>What we should be investing in is more fluid mixture of PARSE of strings/binary with the scanner.  e.g. we should have ways of knowing what line number you're at during the parse for any combinator, and just generally pushing on that.  Adding TRANSCODE parameters up the wazoo isn't a winning strategy.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940</link>
          <pubDate>Mon, 22 Aug 2022 15:09:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1940</guid>
          <source url="https://forum.rebol.info/t/incomplete-transcodes-actually-an-optimization-problem/1940.rss">Incomplete TRANSCODEs: Actually an Optimization Problem</source>
        </item>
        <item>
          <title>Optimizing TRANSCODE Usage in String/Binary PARSE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>As written, the DATATYPE! combinator in UPARSE may do wasteful value loading when operating on string input.</p>
<p>Consider this case.</p>
<pre><code>&gt;&gt; parse "[some big block ...] 10" [collect some [keep integer! | block!]]
== [10]
</code></pre>
<p><em>Pretty impressive that it works.</em>  (Red will only do this on BINARY! input, but Ren-C's UTF-8 everywhere allows it to do it on strings too!)</p>
<p>But at the combinator level, it's wasteful.  What happens is:</p>
<ul>
<li>
<p>Hitting the INTEGER! combinator, causing it to scan the next element, loading <strong><code>[some big block ...]</code></strong> as a series into memory.</p>
<ul>
<li>It then checks the type, notices it's not an integer, and the INTEGER! combinator gives back a rejection...so the BLOCK! combinator goes to the next alternate.</li>
</ul>
</li>
<li>
<p>It hits the BLOCK! combinator and scans the block again.</p>
<ul>
<li>
<p>This time it matches, so the parser returns success and the synthesized block</p>
</li>
<li>
<p><em>But the block isn't actually desired</em>, so it is thrown away</p>
</li>
</ul>
</li>
<li>
<p>The next iteration scans the INTEGER! and keeps it.</p>
</li>
</ul>
<h2>Why Does It Work This Way?</h2>
<p>It's based on TRANSCODE, and does basically exactly what I said:</p>
<pre><code>[item remainder]: transcode input except e -&gt; [return raise e]

if datatype != type of item [
    return raise ["Could not TRANSCODE" datatype "from input"]
]
return item
</code></pre>
<p>If we could pass in a datatype to TRANSCODE when using the /NEXT option (e.g. requesting a remainder, as we are above) then it could short-circuit and we wouldn't need that test.</p>
<h2>Red Has Looked At This Kind of Problem</h2>
<p>There are a bunch of new arguments to Red's TRANSCODE function:</p>
<pre><code>USAGE:
     TRANSCODE src

DESCRIPTION: 
     Translates UTF-8 binary source to values.
     Returns one or several values in a block. 

ARGUMENTS:
     src          [binary! string!]
     {UTF-8 input buffer; string argument will be UTF-8 encoded.}

REFINEMENTS:
     /next        =&gt; Translate next complete value (blocks as single value).
     /one         =&gt; Translate next complete value, returns the value only.
     /prescan     =&gt; Prescans only, do not load values. Returns guessed type.
     /scan        =&gt; Scans only, do not load values. Returns recognized type.
     /part        =&gt; Translates only part of the input buffer.
         length       [integer! binary!] "Length in bytes or tail position."
     /into        =&gt; Optionally provides an output block.
        dst          [block!] 
     /trace       =&gt; 
        callback     [function! [
                        event [word!]
                        input [binary! string!]
                        type [word! datatype!]
                        line [integer!]
                        token
                        return: [logic!]
                      ]] 

RETURNS:
    [block!]
</code></pre>
<p>I'm not sure exactly how useful the /PRESCAN option is (what good is a "guess" of the type?)  But the /SCAN option would offer some bit of efficiency.</p>
<p>It would mean instead of one call to TRANSCODE followed by a datatype test, there'd be two calls</p>
<ul>
<li>
<p>The first as TRANSCODE/SCAN to get the datatype (but not synthesize a value from it)</p>
</li>
<li>
<p>A second call to scan again and get the value</p>
</li>
</ul>
<p>We assume the idle mode of scanning without producing anything can be fast.</p>
<p>I would suggest the scan feature be <strong>transcode/types</strong> so it worked more generally, not just with /NEXT.</p>
<pre><code>&gt;&gt; transcode/types [1 a [b]]
== [#[datatype! integer!] #[datatype! word!] #[datatype! block!]]
</code></pre>
<p><sub><em>(When I figure out the story of datatypes, there are going to be a lot of forum posts fixing up the above ugly notation.)</em></sub></p>
<h2>But What About The Synthesis Of Unused Values?</h2>
<p>This is a bit of a pickle.  <em>We don't know if you're going to use the product or not.</em></p>
<p>UPARSE's design has values bubbling out the top, and no line of communication to be aware of whether what it produces will be used:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! group!] 
== (b)
</code></pre>
<p>You might think that when the block! rule is going to be run, UPARSE could notice it wasn't at the end and send some kind of signal to the BLOCK! combinator that it doesn't have to synthesize an output.  But there's no a-priori psychic power saying that GROUP! hasn't been configured to evaluate to void.  Until the combinator gets looked up and run, it's potentially the same situation as this:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! void] 
== [a]
</code></pre>
<h2>It Seems We Have Two Choices</h2>
<ol>
<li>
<p>We can assume that a plain DATATYPE! intends to synthesize a value, and use a different combinator to say you only want to match the type:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [scan block!]
== #[datatype! block!]  ; cheap (but useful) return value, no series synthesis

&gt;&gt; uparse "[a b c]" [block!]
== [a b c]
</code></pre>
</li>
<li>
<p>We can reverse it and say that by default it does the cheap thing, and you have to explicitly ask to get the expensive thing:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
== #[datatype! block!]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Looked at in isolation, it might seem like (2) would be the obvious winner.</p>
<p>The thorn is that this would be a pretty notable divergence from how array parsing works, which I would basically call non-negotiable:</p>
<pre><code>&gt;&gt; uparse [[a b c]] [x: block!]

&gt;&gt; x
== [a b c]
</code></pre>
<p>So is there actually an option 3?</p>
<ol start="3">
<li>
<p>Make lone datatype! an error, and have two distinct operations for transcoding:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
** Error: On string input, use either TRANSCODE BLOCK! or SCAN BLOCK!

&gt;&gt; uparse "[a b c]" [transcode block!]
== [a b c]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Urg.  That kind of sucks.</p>
<p><strong>I think the answer is to accept option (1) being suboptimal performance, allowing those who are performance-minded to tune it.</strong>  There's no overt harm by scanning things you throw away, it's just wasteful.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939</link>
          <pubDate>Sun, 21 Aug 2022 19:10:10 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1939</guid>
          <source url="https://forum.rebol.info/t/optimizing-transcode-usage-in-string-binary-parse/1939.rss">Optimizing TRANSCODE Usage in String/Binary PARSE</source>
        </item>
        <item>
          <title>Simple Objects vs. What The People Want</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C has a more streamlined version of how R3-Alpha implemented simple OBJECT!s, but it's really mostly the same <em>(though MODULE! has changed significantly)</em></p>
<p>An OBJECT! is just two parallel lists, which I have called the <strong>"keylist"</strong> and the <strong>"varlist"</strong>.</p>
<p>So if you say something like:</p>
<pre><code>obj: make object! [
    x: 1 + 2
    y: 10 + 20
]
</code></pre>
<p>You will get:</p>
<pre><code>keylist: {symbol(x) symbol(y)}
varlist: [*V0* 3 30]
</code></pre>
<p>The first slot in a varlist is used for some tracking information.  So:</p>
<ul>
<li>
<code>keylist[0]</code> is the key for <code>varlist[1]</code>
</li>
<li>
<code>keylist[1]</code> is the key for <code>varlist[2]</code>
</li>
</ul>
<h2>You Get A New Keylist With Every MAKE OBJECT!</h2>
<p>Nothing in the system goes around looking for common patterns in your object creation to notice that you've made several objects with the same keys.</p>
<pre><code>collect [
    count-up i 1000 [
        keep make object! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>You just made 1000 objects, and all of them have their own copy of the keylist <code>{symbol(X) symbol(Y)}</code>.  Ren-C made this overhead cost less than 1/4 as much as R3-Alpha, but it's still kind of lame.</p>
<p><strong>The only way you avoid making a new keylist is if you do object inheritance.</strong></p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>This time, there's 1000 objects all sharing a single keylist.</p>
<p><strong>If you expand keys at all, that will result in a new keylist...</strong></p>
<p>You spoil the optimization if you put anything additional in your derived object:</p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20, z: i * 30]
    ]
]
</code></pre>
<p>There's no inheritance mechanism that makes use of the common sublist.  So this puts you at <em>1001</em> keylists, because your keylist for the original point! never gets used.</p>
<p><strong>Object Expansion via APPEND disconnects shared keylists</strong></p>
<p>R3-Alpha allowed you to add fields to an object.  If you did so, you would lose any sharing that it had taken advantage of before.</p>
<pre><code>p: make point! [x: 10 y: 20]  ; reuses point!'s keylist
append p [z: 30]  ; oop, not anymore...gets its own keylist.
</code></pre>
<p><strong>Comparisons Are Difficult</strong></p>
<p>Because there's no global mechanism of canonization of keylists, you get entirely different-looking objects by creating the fields in different orders.</p>
<pre><code>obj1: make object! [x: 10 y: 20]
obj2: make object! [y: 20 x: 10]
</code></pre>
<p>These objects have been considered to be not equal historically.  Because comparisons are done by walking the fields in order.  So obj1 &lt;&gt; obj2 in this case.</p>
<p>However, if you create an object via inheritance so it shares a keylist, that will standardize the order of the fields:</p>
<pre><code>point1: make point! [x: 10 y: 20]
point2: make point! [y: 20 x: 10]
</code></pre>
<p>Here we will have point1 = point2, since their shared keylist forces the order of x and y to whatever it was in POINT!.</p>
<h2>There Are Fancier Ways Of Dealing With This</h2>
<p><strong>If you're willing to say that the order of keys in objects shouldn't matter...</strong> then you can rethink the data structures to exploit commonalities in the patterns of keys that are created.</p>
<p>The V8 JavaScript engine approaches this with <strong><a href="https://richardartoul.github.io/jekyll/update/2015/04/26/hidden-classes.html">Hidden Classes</a></strong>.</p>
<p>But there's really always some other way of approaching the problem.  The way modules work in "Sea of Words" is an example of a structure that seems to work reasonably well for modules--but wouldn't work as well for lots of little objects.</p>
<h2>Today's FRAME! Depends On This Non-Fancy Way</h2>
<p>Right now, when a native runs it does so with a concept of the order of the arguments and refinements that gets baked into the C code directly.  IF knows that the condition is argument 1 and that the branch is argument 2, and it looks directly in slots 1 and 2 of the varlist of the frame to find those variables.</p>
<p>This is pretty foundational to the idea of the language, and is part of what gives it an appealing "simple-ness".</p>
<p>Ren-C has come along and permitted higher level mechanisms like specialization and adaptation, but everything is always getting resolved in a way that each step in a function's composition works on putting information into the exact numbered slot that the lower levels expect it to be in.</p>
<h2>Binding Has Depended On This Non-Fancy Way</h2>
<p>A premise in Rebol has been that you can make a connection between a variable and an object that has a key with the name of that variable, and once that connection is made it will last.  This rule is why there's been dodginess about deleting keys in objects or rearranging them...and why R3-Alpha permits adding new variables but not removing any.</p>
<pre><code> obj: make object! [x: 10 y: 20]
 code: [x + y]
 bind code obj
</code></pre>
<p>If you write something like the above, you are annotating the X inside of CODE with (obj field <span class="hashtag">#1</span>), and the Y inside of CODE with (obj field <span class="hashtag">#2</span>).  So nothing can happen with obj that can break that.</p>
<p><strong>This isn't strictly necessary.</strong>  It could have annotated X and Y with just (obj) and then gone searching each time it wanted to find it.  This would permit arbitrary rearrangement of OBJ, inserting and removing keys.  It could even remove X or Y and then tell you it couldn't find them anymore.</p>
<p>There are compromises as well.  The binding could be treated as a potentially fallible cache...it could look in that slot position (if it's less than the total keylist size) and see if the key matched.  If not, it could fall back on searching and then update with the slot where it saw the field.</p>
<p>(Of course this means you have to look at the keylist instead of just jumping to where you want to be in the varlist, and locality is such that they may not be close together; so having to look at the keylist <em>at all</em> will bring you a slowdown.)</p>
<h2>But What Is The Goal, Here?</h2>
<p>I've mentioned how the FRAME! design pretty much seems to go along well with the naive ordering of object fields.</p>
<p>I guess this is where your intuition comes in as to what represents "sticking to the rules of the game".  <em>And I think that hardcoding of positions into the executable of where to find the argument cells for natives is one of the rules.</em></p>
<p>This suggests that all functions hardcode the positions of their arguments--even usermode functions.  I'm okay with this.</p>
<p>So then we get to considering the question about OBJECT!.</p>
<ul>
<li>
<p>A lot of languages force you to predefine the structure of an object before creating instances.  And defining that structure is a good place to define its interfaces.  If Rebol wants to go in a more formal direction (resembling a Rust/Haskell/C++) then you might suggest you <em>always</em> make a base structure...and you can only have the fields named in it.</p>
</li>
<li>
<p>Other languages (like JavaScript) are more freeform, and as mentioned can look for the relationships after-the-fact.  Order of fields does not matter.</p>
</li>
</ul>
<p>It's clear that Rebol's userbase so far are people who would favor better implementation of the JavaScript model over going to more strictness.  I think there'd be a pretty good reception of a model where you could create objects with <strong>{...}</strong> and where fields could be added or removed as people saw fit.  If behind-the-scenes the system was optimizing access to those objects, that would presumably be preferable to this idea that you had to be responsible for declaring prototypes to get efficiencies (that would instantly disappear if you added another field).</p>
<p>But the mechanics definitely get more complicated.  :-/</p>
            <p><small>8 posts - 3 participants</small></p>
            <p><a href="https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745</link>
          <pubDate>Mon, 18 Oct 2021 06:45:29 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1745</guid>
          <source url="https://forum.rebol.info/t/simple-objects-vs-what-the-people-want/1745.rss">Simple Objects vs. What The People Want</source>
        </item>
        <item>
          <title>Changing Strategies on Avoiding Stdio Inclusion</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Among the battles that Rebol picked, one was to not become dependent on the IO and formatting constructs of libc.  So you could build an interpreter without the logic that is behind <strong>printf("Hello %s, your Score is %d\n");</strong></p>
<p>Ren-C embraced this and tried to enforce it by causing compile-time errors when stdio.h was included in release builds.  Over time this has turned out to be a non-viable strategy for accomplishing the intent.</p>
<p>Modern C compilers have more or less assumed that if you include one header you want to include them all.  So if you <strong><code>#include &lt;string.h&gt;</code></strong> you're likely to get all of <strong><code>&lt;stdio.h&gt;</code></strong> too.</p>
<p>And as it happens, while we don't need printf(), we now do need some definitions out of stdio in some files.</p>
<p><strong>We still should keep an eye on included functions, but that oversight needs to shift from the compiler level to the linkage level</strong>.</p>
<p>Here was some of the trickery used to try and keep stdio.h out of release builds:</p>
<pre><code>//
// DISABLE STDIO.H IN RELEASE BUILD
//
// The core build of Rebol published in R3-Alpha sought to not be dependent
// on &lt;stdio.h&gt;.  Since Rebol has richer tools like WORD!s and BLOCK! for
// dialecting, including a brittle historic string-based C "mini-language" of
// printf into the executable was a wasteful dependency.  Also, many
// implementations are clunky:
//
// http://blog.hostilefork.com/where-printf-rubber-meets-road/
//
// To formalize this rule, these definitions will help catch uses of &lt;stdio.h&gt;
// in the release build, and give a hopefully informative error.
//
#if defined(NDEBUG) &amp;&amp; !defined(DEBUG_STDIO_OK)
    //
    // `stdin` is required to be macro https://en.cppreference.com/w/c/io
    //
    #if defined(__clang__)
        //
        // !!! At least as of XCode 12.0 and Clang 9.0.1, including basic
        // system headers will force the inclusion of &lt;stdio.h&gt;.  If someone
        // wants to dig into why that is, they may...but tolerate it for now.
        // Checking if `printf` and such makes it into the link would require
        // dumping the library symbols, in general anyway...
        //
    #elif defined(stdin) and !defined(REBOL_ALLOW_STDIO_IN_RELEASE_BUILD)
        #error "&lt;stdio.h&gt; included prior to %sys-core.h in release build"
    #endif

    #define printf dont_include_stdio_h
    #define fprintf dont_include_stdio_h
#else
    // Desire to not bake in &lt;stdio.h&gt; notwithstanding, in debug builds it
    // can be convenient (or even essential) to have access to stdio.  This
    // is especially true when trying to debug the core I/O routines and
    // unicode/UTF8 conversions that Rebol seeks to replace stdio with.
    //
    // Hence debug builds are allowed to use stdio.h conveniently.  The
    // release build should catch if any of these aren't #if !defined(NDEBUG)
    //
    #include &lt;stdio.h&gt;

    // NOTE: F/PRINTF DOES NOT ALWAYS FFLUSH() BUFFERS AFTER NEWLINES; it is
    // an "implementation defined" behavior, and never applies to redirects:
    //
    // https://stackoverflow.com/a/5229135/211160
    //
    // So when writing information you intend to be flushed before a potential
    // crash, be sure to fflush(), regardless of using `\n` or not.
#endif
</code></pre>
<p>Here are some comments on how the C++ <strong><code>&lt;string&gt;</code></strong> header on MSVC pulled in formatting that pulled in string.h (aka. <strong><code>&lt;cstring&gt;</code></strong> in C++ terms).</p>
<pre><code>/*
 * If using C++, variadic calls can be type-checked to make sure only
 * legal arguments are passed.  It also means one can pass literals
 * and have them coerced (e.g. integer =&gt; INTEGER! or bool =&gt; LOGIC!).
 *
 * Note: In MSVC, `#include &lt;string&gt;` will pull in `&lt;xstring&gt;` that
 * then sucks in `&lt;iosfwd&gt;` which brings in `&lt;cstdio&gt;`.  This means
 * that including the release version of %sys-core.h will see an
 * inclusion of stdio that it doesn't want.  Bypass the assertion
 * for this case, and hope the C build maintains dependency purity.
 */
#ifdef TO_WINDOWS
    #define REBOL_ALLOW_STDIO_IN_RELEASE_BUILD  // ^^-- see above
#endif
</code></pre>
<h2>Again, not giving up, just changing tactics...</h2>
<p>In fact, it's really better to be looking at the binary for bloat anyway.  You can write innocuous lines of source and find that brought in all kinds of things in the compiler.  So going by the source isn't the best idea.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685</link>
          <pubDate>Thu, 26 Aug 2021 13:24:52 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1685</guid>
          <source url="https://forum.rebol.info/t/changing-strategies-on-avoiding-stdio-inclusion/1685.rss">Changing Strategies on Avoiding Stdio Inclusion</source>
        </item>
        <item>
          <title>Sea of Words is now in Beta (or something?)... Some Numbers</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I've gotten Sea of Words through the test suite and Bootstrap...and running the scenarios that have GitHub Actions (rebol-httpd, rebol-odbc, rebol-whitespacers.)  And I got it working in the web console too, of course!</p>
<p><em>(Note: If the web console seems sluggish these days don't blame Sea of Words...I started using UPARSE in it, and right now UPARSE is in full-on experimental mode.  It's a beast, so using it at all--even on trivial samples--will be resource intensive.)</em></p>
<h1>Some Easy-To-Get Numbers For The Moment</h1>
<p><strong>These numbers should be taken with a grain of salt...</strong> they don't measure everything, and some things shift around in ways that are hard to quantify.  But they're better than nothing.</p>
<p>(Note: I actually had to fix a bug in the evaluation count that was giving wild answers.  R3-Alpha lacked a double-check on its optimized method of incrementing the total evaluation count without needing to so every time in the loop...)</p>
<h2>Prior to Sea of Words</h2>
<p>Here is a report from a freshly booted desktop build on Windows, which avoids trying to read the executable into memory:</p>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 123702
    series-made: 53630
    series-freed: 27122
    series-expanded: 728
    series-bytes: 3460161
    series-recycled: 25447
    made-blocks: 33403
    made-objects: 191
    recycles: 3
]
</code></pre>
<h2>After Sea of Words</h2>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 138386
    series-made: 72860
    series-freed: 39697
    series-expanded: 706
    series-bytes: 3270669
    series-recycled: 20856
    made-blocks: 52054
    made-objects: 221
    recycles: 3
]
</code></pre>
<p>On the bright side, <strong>Sea of Words is not only a watershed moment in binding/modules, it's also saving 189K or so of memory</strong>, even just here in its first debut.</p>
<p>You may be wondering why there are so many more blocks.  The answer is that they're very <em>tiny</em> optimized stub blocks, used to hold individual variables that are floating in the "sea".  This is expected and purposeful.  As shown, the total memory use went down...</p>
<h2>This is Really Only A Beginning</h2>
<p>While the abilities that just came into play are a tremendous step for making a "real" and usable module system, there is significantly more left.  I'll be posting more on those issues after some <img src="https://forum.rebol.info/images/emoji/twitter/zzz.png?v=9" title=":zzz:" class="emoji" alt=":zzz:"></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678</link>
          <pubDate>Sun, 22 Aug 2021 11:45:46 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1678</guid>
          <source url="https://forum.rebol.info/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678.rss">Sea of Words is now in Beta (or something?)... Some Numbers</source>
        </item>
        <item>
          <title>Beating REPEND: A New Parameter Convention?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>When you do an <strong>append a reduce b</strong>, the REDUCE generates a new series... let's call it <strong>rb</strong>.  Then <strong>rb</strong> is spliced into <strong>a</strong>.  And then <strong>rb</strong> needs to be GC'd.</p>
<p>The idea behind <strong>repend a b</strong> is that you never make <strong>rb</strong>.  Instead, expressions are evaluated one by one and put onto <strong>a</strong> as you go.  The savings are twofold...reduced memory overhead and reduced tax on the GC by not making extra series nodes.</p>
<p>That might sound like a great savings, but here is a heated debate in Red about the questionable benefit of REPEND (as well as /INTO):</p>
<p><a href="https://github.com/red/red/issues/3340">https://github.com/red/red/issues/3340</a></p>
<p>I guess I'm halfway on DocKimbel's side there...in that if REPEND isn't showing a benefit it's probably more to do with a bug in REPEND vs. that the idea doesn't represent a savings.</p>
<p>But I <em>hate</em> the word REPEND.  Things like REMOLD are double monstrous, and REFORM?  Give me a break.  These make a terrible impression.</p>
<p>More generally, I don't like the idea that every function would have to come in two flavors and create anxiety on the part of the caller as to if they're using the optimized one or not.  I'd like any optimization to be more "under the hood" so the caller doesn't have to fret about it.</p>
<p>This got me to thinking...</p>
<h2>A New Voodoo for GET-params!</h2>
<p>Let's imagine that we have a new rule for params that look like GET-WORD!:</p>
<ul>
<li>
<p>If the argument is a GET-XXX!, it is passed literally</p>
</li>
<li>
<p>If the argument is anything else, it is evaluated normally and the product is passed in with one quoting level added.</p>
</li>
</ul>
<p>Here's an example definition</p>
<pre><code>appender: func [
    block [block!]
    :value [any-value!]
][
   print ["Block is" mold block]
   print ["Value is" mold value]
   if get-block? value [
       append block reduce as block! value
   ] else [
       append block unquote value
   ]
]
</code></pre>
<p>Let's look at some concrete examples:</p>
<pre><code>&gt;&gt; appender [1 2 3] 2 + 2
Block is [1 2 3]
Value is '4
== [1 2 3 4]

&gt;&gt; data: [[a b c] [d e f]]
&gt;&gt; appender [1 2 3] second data
Block is [1 2 3]
Value is '[d e f]
== [1 2 3 d e f]

&gt;&gt; appender [1 2 3] :[10 + 20 100 + 200]
Block is [1 2 3]
Value is :[10 + 20 100 + 200]  ; not quoted!
== [1 2 3 30 300]
</code></pre>
<p>At the source level, the user doesn't really have to worry about the parameter convention.  They get the same outcome as if the REDUCE had been done by the evaluator, but the APPENDER becomes complicit.</p>
<p>And look what happens if the GET-BLOCK! is in a variable...</p>
<pre><code>&gt;&gt; data: the :[10 + 20 100 + 200]
&gt;&gt; appender [1 2 3] data
Block is [1 2 3]
Value is ':[10 + 20 100 + 20]
** Error: Cannot append evaluative items...
</code></pre>
<p><strong>A ha!</strong> We could tell that this was an evaluative get-block product, and not meant to participate in our little trick.  <em>(Erroring is actually the right answer here, you would need to use <strong>only data</strong> or <strong>^data</strong> or <strong>quote data</strong> etc. to APPEND an evaluative GET-BLOCK! under the new rules.)</em></p>
<p>This is neat, because it means users can express intention to reduce at the callsite...and it is something that you can optimize on an as-needed basis.</p>
<h2>As One Would Expect, There Are Some Glitches...</h2>
<p>There are some seeming semantic glitches when a function takes these and they're not the last parameter, where you might see variations along the lines of:</p>
<pre><code> &gt;&gt; takes-first-args-normally :[elide print "A", 1 + 2] (print "B", &lt;x&gt;)
 A
 B
 3
 &lt;x&gt; 

&gt;&gt; takes-first-arg-specially: :[elide print "A", 1 + 2] (print "B", &lt;x&gt;)
A
B
&lt;x&gt;
3
</code></pre>
<p>Basically: <strong>If you somehow relied on side effects happening in left-to-right parameter order at the callsite, then moving the REDUCE of any parameters other than the last one into the body of the operation will change that order.</strong></p>
<p>This is nothing new for this line of thinking in optimization: imagine if APPEND and REPEND took their arguments in the reverse order, so that the block wasn't the last item.  You couldn't just blindly substitute APPEND REDUCE for REPEND in that case, if you were dependent on argument-ordering effects...if there was an evaluation in the first parameter's reduction that was needed for the second parameter.</p>
<p>But the difference is that the person editing APPEND REDUCE =&gt; REPEND made  a change at the callsite.  If you change the parameter convention and don't touch the callsites--with the intent that they stay working and you're just adding an optimization--it starts to matter.</p>
<p>We have some control here, though!  We can define how GET-BLOCK!s act as arguments to function calls, and we can say that they don't actually perform their REDUCE until the function executes.  That leaves breathing room for people who wish to add the optimization later...knowing they won't break the expectations.</p>
<p><em>Whew, that solves that problem!  Good thing it's the only one!  Oh, no, wait...</em>  <img src="https://forum.rebol.info/images/emoji/twitter/face_with_head_bandage.png?v=9" title=":face_with_head_bandage:" class="emoji" alt=":face_with_head_bandage:"></p>
<h2>Not All REPEND Operations Take Literal Blocks</h2>
<p>You see <strong>repend data [...]</strong> a lot of the time, but there's also <strong>repend block1 block2</strong>.</p>
<p>So <strong>append data :[...]</strong> can be finessed as an optimization for the first case, but doesn't solve the second.</p>
<p>To shore it up, we'd have to say that <strong><code>:(...)</code></strong> means "reduce the result of what's in the expression".</p>
<pre><code>&gt;&gt; :(reverse [1 + 2 10 + 20])  ; -&gt; :[20 + 10 2 + 1]
== [30 3]
</code></pre>
<p>This way, we could actually pass the APPEND an expression to reduce the product of.  We'd need to do the evaluation at the moment we passed the parameter (I think), and then alias it as a GET-BLOCK!, so:</p>
<pre><code>&gt;&gt; appender [1 2 3] :(reverse [1 + 2 10 + 20])
Block is [1 2 3]
Value is :[20 + 10 2 + 1]
== [1 2 3 3 30]
</code></pre>
<h2>Where Are GET-WORD!, GET-PATH!, GET-TUPLE! in all of this?</h2>
<p>We don't have GET-WORD! mean "reduce the product of fetching the word":</p>
<pre><code>&gt;&gt; block: [1 + 2]

&gt;&gt; :block
== [1 + 2]  ; not [3]
</code></pre>
<p>But it seems it would be inconsistent to not put these other GET-XXX! types into the family of parameters that are captured as-is.  So the above code would get this behavior:</p>
<pre><code>&gt;&gt; appender [1 2 3] :foo
Block is [1 2 3]
Value is :foo
** Error: Cannot append evaluative items...
</code></pre>
<p>Instead of a REDUCE it would need a GET.  But this makes a good argument for why REDUCE of a GET-WORD! should work as a word fetch, for generality... it makes routines like this easier to write correctly.</p>
<p>I don't think it's worth shuffling the symbols around so that <strong>:foo</strong> does a reduce and we pick something else for GET.  It seems to me that <strong>:(foo)</strong> is fine enough.</p>
<p>But even though GET-WORD! won't run arbitrary code, you can be impacted by ordering problems, where someone might pass a <strong>:foo</strong> argument and then in the next parameter change the value of foo.  Hence for consistency, we'd be saying that normal parameters would likely have to delay their get of foo until all the parameters were given...this way you could change the parameter convention without affecting callsites.</p>
<p>But likely the best way to go about that would be to protect the word from modification:</p>
<pre><code>&gt;&gt; some-func :foo (foo: 20, &lt;arg&gt;)
** Error: FOO captured by GET-WORD! in parameter slot, can't modify
      while gathering arguments
</code></pre>
<h2>I'm Probably Over-Worrying About It</h2>
<p>...these protection mechanisms I mention in order to make it painless to change a parameter convention are not likely suited to being the kind of concern that applies.</p>
<p>But it's good to articulate what the limits of a design are...</p>
            <p><small>4 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673</link>
          <pubDate>Thu, 19 Aug 2021 22:29:50 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1673</guid>
          <source url="https://forum.rebol.info/t/beating-repend-a-new-parameter-convention/1673.rss">Beating REPEND: A New Parameter Convention?</source>
        </item>
        <item>
          <title>Paring Down the Boot Block Symbol Table</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Rebol has historically had a file called %words.r, that points out words that the C code would like to be able to recognize quickly by ID numbers.</p>
<p>So if you want to write something like C code for PARSE that recognizes keywords, you might write something like:</p>
<pre><code>switch (VAL_WORD_ID(word)) {
  case SYM_SOME:
      // code for implementing a some rule..
     break;

 case SYM_WHILE:
     // code for implementing a while rule...
    break;
}
</code></pre>
<p>etc.  In C you can only switch() on integers, not pointers.  So these SYM_XXX values have to be agreed upon by the C code and the symbol-loading subsystem.</p>
<p>Some tricks depend on actual ordering of these symbols, or ranges of them.  But most of the time, it doesn't really matter.</p>
<p>If something doesn't have a symbol ID, then you have to do slower creations and comparisons by string... or create your own instance of a symbol and then compare to that symbol by pointer.</p>
<h2>Another Idea: Nix the Table And Trust Determinism</h2>
<p>Right now the way the loading process goes, you have your list of words in %words.r and they count up.  Let's imagine:</p>
<pre><code>apple
banana
orange
...
</code></pre>
<p>So apple becomes SYM_APPLE = 1, banana becomes SYM_BANANA = 2, orange becomes SYM_ORANGE = 3, etc.</p>
<p>The beginning of the boot block has these words in a block, and then stuff using them</p>
<pre><code>[
    [apple banana orange ...]
    [foo: func [] [eat 'apple] peel orange/banana ...]
    ...
]
</code></pre>
<p>Each of those word cells takes up 4 platform pointers, so 32 bytes apiece.  Which is a fair amount to pay to convey the contract between the C code and the interpreter that APPLE needs to have an associated shorthand of 1, BANANA needs a shorthand of 2, etc.</p>
<p><strong>But if you don't care about the values, why not use whatever the value was organically?</strong></p>
<p>Imagine that list at the beginning wasn't there:</p>
<pre><code>[
    [foo: func [] [eat 'apple] peel orange/banana ...]
    ...
]
</code></pre>
<p>The scanner can still give a number to basically every unique word that's in the boot block if it wants to.  It would just come out in a different order... FOO would be 1, FUNC would be 2, EAT would be 3, APPLE would be 4 etc.</p>
<p>You don't necessarily want a giant C file of SYM_XXX for absolutely every word used in the mezzanine.  But what could be done here would be that %words.r would be an indication of <em>registering interest</em> in what value a loaded word ultimately got.</p>
<p><strong>But how do you know what order the scanner is going to visit words in?</strong>  Well, you don't...and you get a chicken and an egg problem.  You can't build the executable to scan without the SYM_XXX numbers.</p>
<p><strong>...unless the scanner was a separate library that could be linked and run standalone...</strong>  If the scanner was factored you could have one compile step that built it, and then linked it into a small executable just for the purposes of generating an enum of SYM_XXX values for a particular boot block.</p>
<p>Not something likely to happen this year (or this lifetime), but... I thought it was interesting to think that if the code were a little more self-aware, the array of words in the boot block could be cut way back to only words that required having sequential integer numbers for some optimization.  <em>(Or <em>specific</em> numbers for some optimization...the only case of that is that the spelling of datatype words line up with the enum value of the datatype in the system.)</em></p>
<h2>Another Related Idea: An Internet Registry for WORD &lt;=&gt; ID</h2>
<blockquote>
<p>Note: This concept is actually contentious with the above...</p>
</blockquote>
<p>If we really wanted to (and weren't concerned about size), we could get a dictionary off the Internet of the 65535 most common words, and number them all in advance.  Then we could tell people who write C extensions that they can use those numbers in their code, so their extensions would be faster if they happened to want to deal with that spelling of that word.</p>
<p>Except then the r3.exe would have a big fat dictionary inside it with a list of strings that extensions may never use.</p>
<p>But putting the dictionary in isn't actually necessary.  If you trust extension authors to be true to the string table, then just publish the table on the Internet in an agreed upon place.  All an extension has to when it gets loaded is to supply the list of strings and numbers out of that table it wants to use.  You only pay for the entries in the table you need...and it doesn't cost any more than having those strings would anyway.</p>
<p>The system could then reconcile and notice if one extension said "banana" is 1020 and another said "banana" is 304.  It could just say one of those extensions is wrong and refuse to load them.  It can do so without r3.exe needing to store the string "banana" or information about it being 304 intrinsically.</p>
<p><em>The reason I say it's contentious is because changes in the boot block would shuffle the symbol IDs around.  This means the deterministic (but changing) approach would create symbol values that would force extensions to be recompiled, while a committed database of numbers would not.</em></p>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671</link>
          <pubDate>Thu, 19 Aug 2021 00:59:26 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1671</guid>
          <source url="https://forum.rebol.info/t/paring-down-the-boot-block-symbol-table/1671.rss">Paring Down the Boot Block Symbol Table</source>
        </item>
        <item>
          <title>About the Optimization category</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>This is a category for discussing performance and optimization ideas.</p>
<p>Though remember the very important <strong>Rules For Optimizations (at least, the Ones Make Code Less Clear)</strong>:</p>
<h1>Rule <span class="hashtag">#1:</span> Don't do it.</h1>
<h1>Rule <span class="hashtag">#2:</span> (Experts only!) Don't do it...yet.</h1>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/about-the-optimization-category/1670">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/about-the-optimization-category/1670</link>
          <pubDate>Thu, 19 Aug 2021 00:15:43 +0000</pubDate>
          <discourse:topicPinned>Yes</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1670</guid>
          <source url="https://forum.rebol.info/t/about-the-optimization-category/1670.rss">About the Optimization category</source>
        </item>
        <item>
          <title>Progress on Nativizing Parser Combinators</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>So... let's start with a virtual machine I have...where r3-alpha gets this time for a rather simple parse operation:</p>
<pre><code>r3-alpha&gt;&gt; delta-time [
     parse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some [opt "c" opt "b" opt "a"]
     ]
 ]
 == 0:00:00.000020  ; averages around here on 
</code></pre>
<p>UPARSE was written with design consideration only; it wasn't even optimized usermode code.  Performance was no object.  And of course, that shows:</p>
<pre><code>uparse-on-day-zero&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]  ; here UPARSE needs FURTHER
     ]
 ]
 == 0:00:00.014000  ; averages around here
</code></pre>
<p>The performance varies a lot because there's so much stuff happening the GC gets triggered, so you have to eyeball it.</p>
<p>But it shows we're talking a ballpark of around 700x slower.  This didn't surprise me at all...running usermode code for all parts of the operation...specializing functions and making frames on so many steps... in a completely general architecture.  <strong>I'm actually surprised it wasn't even slower!</strong></p>
<p>I began chipping away at the infrastructure for combinators to make more of it native.  Generally not the combinators themselves yet (actually only OPT has been changed here...)</p>
<pre><code>basics-plus-opt&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]
     ]
]
== 0:00:00.006000  ; averages around here
</code></pre>
<p>So that cuts it from 700x down to around 300x slower.  This is actually not bad for a beginning!</p>
<p><em>In fact</em>, right here in real-time I'm going to use the techniques I've established to make the combinator that matches TEXT! native and see how much that moves the needle.</p>
<pre><code>with-text&gt;&gt; delta-time [
     uparse "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" [
         some further [opt "c" opt "b" opt "a"]
     ]
]
== 0:00:00.005000  ; averages around here
</code></pre>
<p>Just then I'd estimate an hour of work just took it from 300x slower to 250x slower.</p>
<p><a href="https://github.com/metaeducation/ren-c/blob/4a17795c153ff45d8477387987fe8e15e7a2480b/src/mezz/uparse.reb#L846">Here is the usermode form of the original TEXT! combinator</a></p>
<p><a href="https://github.com/metaeducation/ren-c/blob/4a17795c153ff45d8477387987fe8e15e7a2480b/src/core/functionals/c-combinator.c#L367">Here is the native form that I compiled, tested, and measured in about an hour on a slow-compiling machine...</a></p>
<p>Since I just succeeded so quickly, <a href="https://github.com/metaeducation/ren-c/commit/608286c941889a36c70fbdec984f9d5ccd0d00a7">I just hacked up SOME and FURTHER as natives...they're easier than TEXT!</a></p>
<p>And with that it dips down to around 0:00:00.004000.  <em>A total of an hour and a half of work for three combinators and we went from 300x slower down to a mere 200x slower!</em>  <img src="https://forum.rebol.info/images/emoji/twitter/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p>It might seem for this example we've run out of combinators to make native.  But there's one combinator you're missing that might not be obvious...that's the BLOCK! combinator which orchestrates the sequencing of the OPT clauses.  It's quite a high value case to optimize!</p>
<p>Doing that optimization would mean that with any luck, we'd get to 150x slower than R3-Alpha PARSE for this (or any other) apples-to-apples comparison task.  But I have some things I need to do first, and it would take longer than I want before I finish this post.  But I think you got the idea.</p>
<h2>
<a name="even-with-the-high-multiplier-im-optimistic-about-the-endgame-1" class="anchor" href="https://forum.rebol.info#even-with-the-high-multiplier-im-optimistic-about-the-endgame-1"></a>Even With The High Multiplier, I'm Optimistic About The Endgame</h2>
<p>There's still a bunch of infrastructure besides the combinators to attack where much of the cost exists.  Most of the combinators themselves are pretty simple, but the logic that does the "combinating" itself is not!</p>
<p>But let's imagine that at the end of optimizing <em>all</em> the usermode pieces into native pieces it hits a wall of being 20x or so slower than traditional PARSE.  What then?</p>
<p>Let me give you several reasons why I'm optimistic:</p>
<ul>
<li>
<p><strong>The UPARSE concept is built around fundamental mechanics that are used everywhere by the system.</strong>  For example: there's no special "parse stack", it's using function calls on the same stack as everything else.  It uses clever mechanisms to hook which levels are parsing so you can get UPARSE trace output which doesn't mix up regular function calls with the combinators, but those mechanisms are generic too.</p>
<p>This has plenty of good implications.  Improvements to the function call stack become improvements to the "UPARSE stack" automaticlly (e.g. stacklessness).  Any work we do on making parse recursions faster are likely to make all function calls faster.  Think about all the other aspects this applies to as well.</p>
<p><em>(Of course I'll just restate that all of this is in service of one of the big goals...of letting users pick their own mixes of combinators and write their own.  It wasn't just about reusing work, it was about designing the protocol in a way that the native code wouldn't be locked into a monolithic blob just to save on optimizing some switch() statement.  Everything can be teased out and reconfigured as a mash-up of natives and usermode code.)</em></p>
</li>
<li>
<p><strong>UPARSE is vastly more powerful, so chasing performance parity with any given laborious piece of historical parse code may not be the point.</strong>  Let's say you can express something briefly and eloquently as an idiomatic UPARSE expression and that code runs in 1 second.  Then does it matter that when you write it the convoluted way in historical PARSE it takes 3 seconds, when that convoluted code would run in UPARSE in 5?</p>
</li>
<li>
<p><strong>People with performance-sensitive scenarios who hit a bottleneck can attack that with a combinator specific to their purpose.</strong>  If you write <strong><code>opt some ["a" | "b"]</code></strong> so often that it's bothering you to pay for all the generalized protocols where OPT talks to SOME talks to BLOCK! talks to TEXT!... you could natively write the OPT-SOME-A-OR-B combinator and plug it in.</p>
<p>In addition: there could be ways to make a semi-optimized version of an OPT-SOME-A-OR-B by just asking to pre-combinate those things together.  This would cost you some flexibility... in the past I've talked about <a href="https://forum.rebol.info/t/when-should-parse-notice-changes/1528/3">when PARSE notices rule changes"</a> and that's the kind of phenomenon that might come into play.</p>
<p><em>(Note: Building a CHAIN of functions like <strong>negated-multiply: chain [:multiply | :negate]</strong> have a similar aspect.  They are faster but if what's assigned to the word MULTIPLY or NEGATE change they won't see it...as they commit to the definitions from the time of the CHAIN.)</em></p>
</li>
<li>
<p><strong>The code is organized so much better with responsibility isolated so clearly that I think clever optimizations will be much easier to try.</strong>  There was little you could do with R3-Alpha PARSE without worrying about breaking it.</p>
</li>
</ul>
<h2>
<a name="i-hope-im-right-2" class="anchor" href="https://forum.rebol.info#i-hope-im-right-2"></a>I Hope I'm Right</h2>
<p>It's a challenge but an interesting one to make UPARSE perform.  Let's see where this goes...</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636</link>
          <pubDate>Mon, 19 Jul 2021 05:07:33 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1636</guid>
          <source url="https://forum.rebol.info/t/progress-on-nativizing-parser-combinators/1636.rss">Progress on Nativizing Parser Combinators</source>
        </item>
        <item>
          <title>Fundamental Changes Needed for GC (Reference Counting)</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>So I'd gone ahead with the implementation of virtual binding and LET, because I don't see any real future for the language without it...at least not for the kinds of distinguishing features that I think would make it notable.</p>
<p>But it means we're creating a lot of garbage.  I've brought up pathological cases, like:</p>
<pre><code>count-up x 1000000 [
   let y: x + 1
   print ["Y is" y]
]
</code></pre>
<p>Creating a million tiny tracking entities for each time through the loop is a lot of junk for the GC to have to crunch through.</p>
<p>But the problem runs much deeper than this, because even without LETs you get issues with nested loops and their virtual binding information.  It's one example of many.</p>
<p>It isn't allocating and freeing memory that kills us.  We have memory pools and the layouts of everything are tuned fairly well.  It's having to sweep through all of memory to clear out things that aren't used.</p>
<h2>
<a name="reference-counting-cant-replace-gc-but-would-help-1" class="anchor" href="https://forum.rebol.info#reference-counting-cant-replace-gc-but-would-help-1"></a>Reference Counting Can't Replace GC, But Would Help</h2>
<p>If we had room in each series node for a reference counter, we could notice when that counter reached zero...and free the series without allowing it to accumulate and tax the GC.</p>
<p>That won't get everything, because blocks and objects can have cyclical references.  But a lot of the time, it would let us rapidly reclaim memory to reuse...leading to far less accumulation.</p>
<p>So in the example of the tight COUNT-UP loop above, a FRAME! would be allocated that would have a "specifier chain".  That chain would get the entry for the LET, and so that would count as a reference.  When PRINT runs, the BLOCK! <strong>["Y is" y]</strong> fills into its argument slot...and that instance of the block cell is coupled with the specifier chain...adding another reference.  But when PRINT finished, it would release its hold on the frame where that block cell lived...in this case nothing is holding that frame (it's a native, no debugger, etc.)  That means no one is seeing the cells, so they could all be blanked out...releasing their references.  This would drop the reference <strong>["Y is" y]</strong>'s derelativization has on the specifier chain, bringing it down to 1 reference.  And then, when the frame finished that iteration of the body, it would drop the reference on the specifier chain...reducing its references to 0.  That would free the LET.</p>
<p><em>Or at least the theory is something like that.</em></p>
<h2>
<a name="how-hard-would-it-be-2" class="anchor" href="https://forum.rebol.info#how-hard-would-it-be-2"></a>How Hard Would It Be?</h2>
<p>Offhand, I'd say very hard.</p>
<p>With a C++ build to draw on, it becomes easier to check.  Though I'd definitely say this kind of change would be one of those moments where I'd start to seriously question the sanity of trying to keep on building a sophisticated system in C89.</p>
<p>Doing anything with low-level mechanics is harder the more low-level "core" code you have.  Anything written to higher-levels of abstraction like libRebol wouldn't have to change, but everything that assumes lower access gets a lot hairier.</p>
<p>It's better at the moment to write the code how it's supposed to look...and tackle big challenges, tolerating the slowness.  But I just wanted to bring this up because I don't think the slowness can be beaten unless we do better bookkeeping to know how to reclaim memory.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527</link>
          <pubDate>Fri, 26 Feb 2021 08:53:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1527</guid>
          <source url="https://forum.rebol.info/t/fundamental-changes-needed-for-gc-reference-counting/1527.rss">Fundamental Changes Needed for GC (Reference Counting)</source>
        </item>
        <item>
          <title>Web Build Performance Stats</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I resurrected the "stats" function to get some metrics.  It's actually a good example of how nicely Ren-C can improve things:</p>
<ul>
<li>
<p><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/n-system.c#L129">Here's the code for stats in R3-Alpha</a> (which references an object prototype <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/sysobj.r#L255">defined elsewhere in sysobj.h</a>, and you can also see that all you see in this file of the function spec is REBNATIVE(stats))</p>
</li>
<li>
<p><a href="https://github.com/metaeducation/ren-c/blob/de62515f95ce335c07622ef51218d5da9e938a12/src/core/d-stats.c#L56">Here's that in Ren-C</a>, and the maintainability advantages should be obvious.  The distinction of counting natives didn't exist in the same fashion as before, so it was deleted, but we could do that kind of thing another way.</p>
</li>
</ul>
<p>In any case, running the statistics between R3-Alpha and Ren-C are going to show <em>a lot</em> more series and memory use in Ren-C.  The main reasons are:</p>
<ul>
<li>
<p>There's a Windows encapping issue that it reads the whole executable into memory to probe it for resource sections.  This is especially crazy for debug builds.  I'd raised this as an issue for Shixin to look at but forgot about it.</p>
</li>
<li>
<p><strong>Function frames do not use the data stack, and instead the arguments of functions are stored in individual arrays.</strong>  While there are some optimizations to mean this doesn't require an allocation on quite every function call, it means a good portion of function calls do allocate series.  This stresses the GC, but, I've mentioned how it was important for many reasons (including that the data stack memory isn't stable, and that meant the previous approach had bugs passing pointers to arguments around.  It's a given that this is how things are done now--especially with stackless--so it just needs to be designed around and tuned.</p>
</li>
<li>
<p><strong>WORD!s are special cases of string series.</strong>  Things like the word table and binding didn't count in series memory before, and wasn't tabulated in R3-Alpha in the series count.  There are some other examples of this.</p>
</li>
<li>
<p><strong>ACTION!s create more series and contexts.</strong>  The HELP information for most actions that have help information has two objects linked to it...one mapping parameter names to datatypes, and one mapping parameter names to descriptions.  I'm hoping that the one mapping parameter names to datatypes can be covered by the parameter information that the interpreter also sees...but for today, there's a difference because one contains TYPESET!s and the other contains human-readable BLOCK!s.</p>
</li>
<li>
<p><strong>So Much More Is Done In Usermode.</strong>  Ranging from console code to command-line argument processing, there's more source code (which counts as series itself) and more code running.</p>
</li>
</ul>
<p>I see it as good--not bad--that a ton of things run in the boot process.  Although I think you should be able to build an run a minimal system...even one that doesn't waste memory on HELP strings (it's now easier to make such things, since the spec isn't preserved).</p>
<p>But for today, the closest we have to a "minimal build" is the web build.  It's a bit more comparable to R3-Alpha in terms of how much startup code it runs.</p>
<h2>The Current State</h2>
<p>Starting up R3-Alpha on Linux, I get the following for <strong>stats/profile</strong>:</p>
<pre><code>r3-alpha&gt;&gt; stats/profile
== make object! [
    timer: 0:00:02.639939
    evals: 20375
    eval-natives: 3340
    eval-functions: 369
    series-made: 8393
    series-freed: 2597
    series-expanded: 70
    series-bytes: 2211900
    series-recycled: 2526
    made-blocks: 5761
    made-objects: 64
    recycles: 1
]
</code></pre>
<p>Ren-C on the web is considerably heavier, at least when it comes to evals + series made + GC churn <em>(a little less overall series bytes...probably mostly owed to optimizations that fit small series into the place where tracking information would be stored if it were a larger one)</em>:</p>
<pre><code>ren-c/web&gt;&gt; stats/profile
== make object! [
    evals: 65422
    series-made: 28569
    series-freed: 11160
    series-expanded: 419
    series-bytes: 1731611
    series-recycled: 8669
    made-blocks: 16447
    made-objects: 109
    recycles: 229  ; !!! see update, this is now 1
]
</code></pre>
<p>The increased number of evals just goes with the "a lot more is done in usermode" bit.  There's lots of ways to attack that if it's bothersome.</p>
<p>The series-made number is much bigger.  8393 v. 28569.  I mentioned how a lot of this is going to come from the fact that many evals need to make series, but we don't really have a breakdown of that number here to be sure that's accounting for them.  Anyway, this number isn't all that bothersome to me given that knowledge...but it should be sanity-checked.</p>
<p>What does bother me is the 229 recycles.  That's a lot.  Despite making 3-4x as many series, I don't see how exactly that's translating into 200x the recycling.</p>
<p><strong>UPDATE: This was the result of accidentally committed debug code.  It's back to 1.</strong></p>
<h2>Writing Down The Current State is Better Than Nothing</h2>
<p>Ideally we'd have some kind of performance regression chart that plotted some of these numbers after each build.  Though really it's not too worth doing that unless the numbers carried more information that was more actionable.</p>
<p>But...lacking an automated method, writing it down now and having a forum thread to keep track of findings and improvements is better than nothing.</p>
<p>There's likely a lot that could be done to help the desktop build (such as obviously tending to that encap-reading issue).  But I'd like to focus principally on improvements to the internals that offer benefit to the web build, where I think the main relevance is.  And:</p>
<ul>
<li>
<p><strong>Having a system built from rigorously understood invariants is the best plan for optimization over the long-term.</strong>  If you don't have a lot of assertions and confidence about what is and isn't true around your codebase, you can't know if a rearrangement will break it or not.  So I spend a lot of time focusing on defining these invariants and making sure they are true.</p>
</li>
<li>
<p><strong>Avoid optimizing things before you're sure if they're right.</strong>  I'm guilty as anyone of fiddling with things for optimization reasons just because it's cool or I get curious of whether something can work or not.  Programmers are tinkerers and that's just how it is.  But it's definitely not time to go over things with a fine-toothed comb when so many design issues are not worked out.</p>
</li>
</ul>
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/web-build-performance-stats/1468">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/web-build-performance-stats/1468</link>
          <pubDate>Mon, 18 Jan 2021 06:51:16 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1468</guid>
          <source url="https://forum.rebol.info/t/web-build-performance-stats/1468.rss">Web Build Performance Stats</source>
        </item>
        <item>
          <title>Moving Away From &quot;NULL termination&quot; (END!) of BLOCK!s</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C preserved an idea from R3-Alpha...which was that there would be a cell type byte reserved to signal the end of an array.  This is a bit like how null terminators are used with C strings.  However, arrays also tracked their length.  So it was redundant information.</p>
<p>In R3-Alpha, the special cells were given the END! datatype.  Sometimes you would see bugs that would leak the existence of this internal type to the user.  Ren-C hid it more effectively, by not making it an actual "type".</p>
<p>On the plus side, this provides a clean-looking way to walk through the values in an array:</p>
<pre><code>Cell* item = Array_Head(array);  // first cell pointer in the array
for (; Not_End(item); ++item) {
    ...
}
</code></pre>
<p>However, there are several downsides:</p>
<ul>
<li>
<p><strong>You have to pay for a dereference on each step.</strong>  item is a pointer, and you have to follow that pointer to its memory location to read a byte there to see if you've reached the end.  This probably isn't <em>that</em> bad, because odds are you are going to be working with that memory inside the loop anyway.  But maybe you aren't...and you certainly aren't going to be for the last cell.</p>
</li>
<li>
<p><strong>You typically wind up paying a cell's worth of cost for this convenience.</strong>  If your array is empty, it still needs space for at least one cell.  If your array has one cell, it needs space for two.  If it has two it needs space for three, etc.  This isn't just an extra byte (as in C '\0' termination)...it's 4 platform pointers.  So 32 bytes of oft-wasted space on 64-bit platforms for a mostly empty cell.</p>
</li>
<li>
<p><strong>But rounding up by 1 is even worse than wasting one cell...</strong> because it propagates to rounding up in the memory pool block size, and memory pools are sized in multiples of 2 (2, 4, 8, 16, etc).  So if what you really want is a two-cell array--e.g. enough for <strong>a/b</strong>, you move up to the next size and take a chunk from the 4-cell pool.  A 4-cell array needs to come from the 8-cell pool.  Etc.</p>
</li>
</ul>
<h2>
<a name="should-we-scrap-this-idea-1" class="anchor" href="https://forum.rebol.info#should-we-scrap-this-idea-1"></a>Should We Scrap This Idea?</h2>
<p>It's bothered me for a while, but since it might make enumeration faster in some cases I've let it hang around.  Having a terminator has helped catch out of bounds cases more easily.</p>
<p><strong>But I think the time has come to demote termination to a debug-build-only practice.</strong>  It's gotten in the way of too many interesting optimizations.</p>
<p>Data point: Red doesn't do it.  They just store the pointer to the tail of the data (in the slot where R3-Alpha stored the length).  It works either way since you can calculate the length by subtracting the head from the tail...or calculate the tail by adding the length to the head.  I'd imagine the tail is needed more often.</p>
<p>The code isn't usually that much worse:</p>
<pre><code>Cell* item = Array_Head(array);
Cell* tail = Array_Tail(array);
for (; item != tail; ++item) {
    ...
}
</code></pre>
<p>But sometimes there were cases that a function would be passed a Cell* resident in an array, without passing the array also.  And then it would enumerate that value forward until it reached an end.  Such routines aren't all that common, but a few do exist.  They'd need to be revisited.</p>
<p>It's not that huge a deal, and kind of trivial in the scheme of things.  But it would touch a lot of code.  <img src="https://forum.rebol.info/images/emoji/twitter/frowning.png?v=12" title=":frowning:" class="emoji" alt=":frowning:" loading="lazy" width="20" height="20">  But, as usual in Ren-C...the asserts can keep it running.</p>
<h2>
<a name="end-signals-would-still-exist-2" class="anchor" href="https://forum.rebol.info#end-signals-would-still-exist-2"></a>END signals would still exist</h2>
<p>The END cell type is important for other reasons.  It's used in rebEND as a terminator for C va_list arguments, and that's not going away.  There are other applications which are beyond the scope of this post to explain.</p>
<p>And as I say, termination of some kind would probably continue in debug builds.  So they might over-allocate to have enough room at the tail to put an end cell, just to get errors to trigger if you went past the limit.</p>
<p>So let's not malign the END marker too much.  It has been a valuable contributor.  <img src="https://forum.rebol.info/images/emoji/twitter/medal_sports.png?v=12" title=":medal_sports:" class="emoji" alt=":medal_sports:" loading="lazy" width="20" height="20"></p>
            <p><small>3 posts - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445</link>
          <pubDate>Wed, 30 Dec 2020 10:29:18 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1445</guid>
          <source url="https://forum.rebol.info/t/moving-away-from-null-termination-end-of-block-s/1445.rss">Moving Away From &quot;NULL termination&quot; (END!) of BLOCK!s</source>
        </item>
        <item>
          <title>Idea: Agreed Upon Symbol Number for Extensions</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>A concept in the R3-Alpha codebase is that there are a certain number of built-in words...which come from a file called %words.r</p>
<p><a href="https://github.com/rebol/rebol/blob/master/src/boot/words.r">https://github.com/rebol/rebol/blob/master/src/boot/words.r</a></p>
<p>This is done so you can switch on a numeric code for these words, and not bother with needing to do a string comparison in C.  Some words (like PARSE keywords) are strategically chosen to be in a sequential range, to make testing for them faster.</p>
<p>If you write an extension in C that operates at the internal level API and want the performance of a native, you might want to talk about a word that's not in that list.  You can get a bit close to the performance for a single test by caching a pointer to the canonized version of that word, and comparing to that canon pointer.  But it won't be quite as fast, and since that won't be a constant...C can't use it in switch statements.</p>
<p>To be more concrete, imagine you have some words not in %words.r like OVERLOAD, MULTIPLE, INHERITANCE.  You couldn't write:</p>
<pre><code> switch (VAL_WORD_SYM(some_word)) {  ; small 16-bit # can be cached in word
     case SYM_OVERLOAD: ...  ; ...but these weren't in %words.r!
     case SYM_MULTIPLE: ...
     case SYM_INHERITANCE: ...
     default: ...
}
</code></pre>
<p>Can't do that for those new terms.  You'd have to do case-insensitive string comparisons, or something like this pseudocode:</p>
<pre><code> REBSTR *canon_overload;
 REBSTR *canon_multiple;
 REBSTR *canon_inheritance;

 void On_Module_Load() {
      canon_overload = Register_Word("overload");
      canon_multiple = Register_Word("multiple");
      canon_inheritance = Register_Word("inheritance");
 }

 void On_Module_Shutdown() {
     Unregister_Word(canon_overload);
     Unregister_Word(canon_multiple);
     Unregister_Word(canon_inheritance);
 }
</code></pre>
<p>So imagine this gives you word series pointers that are guarded from GC for as long as your module is loaded.  Then you could say:</p>
<pre><code> REBSTR *canon = VAL_WORD_CANON(some_word);
 if (canon == canon_overload) { ... }
 else if (canon == canon_multiple) { ... }
 else if (canon == canon_inheritance) { ... }
 else { ... }
</code></pre>
<p>It's less elegant than the switch(), and since the numbers are runtime pointers and not fixed at compile-time, there's no way to optimize as in a switch() by repeatedly bisecting the range of values...if you have N words, you will do N comparisons.</p>
<h2>Weird idea: Agree on a list of words and numbers, commit on Internet</h2>
<p>It would be pretty heinous to make a much bigger %words.r and ship it in every executable...inflating the size of Rebol to include a dictionary.</p>
<p>But there's a possibility that doesn't go that far yet still gets the benefit.  Make the word list and commit it somewhere on the internet that developers can look.  Give every common word a number.  Then, the extension ships with just the spellings and numbers it needs.  All extensions agree to use the same numbers:</p>
<pre><code> #define SYM_OVERLOAD 15092
 #define SYM_MULTIPLE 32091
 #define SYM_INHERITANCE 63029

 void On_Module_Load() {
      Register_Word("overload", SYM_OVERLOAD);
      Register_Word("multiple", SYM_MULTIPLE);
      Register_Word("inheritance", SYM_INHERITANCE);
 }

 void On_Module_Shutdown() {
     Unregister_Word(SYM_OVERLOAD);
     Unregister_Word(SYM_MULTIPLE);
     Unregister_Word(SYM_INHERITANCE);
 }
</code></pre>
<p>Your switch() statements can work just fine, and you're only out of luck if you use a sequence of characters that wasn't committed to in the database.  But the database can grow, so long as it grows centrally and not inconsistently.  (In fact, it's probably better to do it that way, where extension authors ask for the words they want and get them approved before shipping the extension.)</p>
<p>The worst that can happen is you load two extensions that disagree, and it refuses to load them.  It could print out the disagreeing numbers and you could consult the internet to decide who was the culprit using the wrong number.</p>
<p>It's a weird idea but kind of interesting--not in particular because of the performance aspect, but because of enabling the C switch()es.  Since there's only 16 bits of space in the word available for the symbol trick, it's an exhaustible resource.  But maybe still worth doing.  This really isn't difficult, outside of the administrative headache of deciding the policy on giving out <span class="hashtag">#s</span></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188">Read full topic</a></p>
          ]]></description>
          <link>https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188</link>
          <pubDate>Thu, 25 Jul 2019 18:44:16 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">forum.rebol.info-topic-1188</guid>
          <source url="https://forum.rebol.info/t/idea-agreed-upon-symbol-number-for-extensions/1188.rss">Idea: Agreed Upon Symbol Number for Extensions</source>
        </item>
  </channel>
</rss>
