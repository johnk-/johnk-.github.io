<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:discourse="http://www.discourse.org/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Optimization - AltRebol</title>
    <link>https://rebol.metaeducation.com/c/development/optimization/53</link>
    <description>Topics in the &#39;Optimization&#39; category This is a category for discussing performance and optimization ideas.</description>
    
      <lastBuildDate>Fri, 25 Jul 2025 17:41:49 +0000</lastBuildDate>
      <atom:link href="https://rebol.metaeducation.com/c/development/optimization/53.rss" rel="self" type="application/rss+xml" />
        <item>
          <title>Optimizing 1-Element PACK!s / SPLICE!s</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Historically the most common type of PACK! is the 1-element PACK! holding a ~NULL~ antiform, that you get when a branching statement takes the branch and evaluates to null.</p>
<p>It's not supposed to be the case that antiforms have identity--once you make the transition from a plain form to an antiform, you lose the identity (and also, binding).</p>
<p>So really, every branching statement that returns a PACK!-boxed null can return the same boxed null.  Easy enough optimization--it's already done like that.</p>
<p>I've proposed something strange though... <a href="https://rebol.metaeducation.com/t/the-grand-leading-slash-safety-or-burden-question/2352/7">which is that putting actions in a PACK! be how they are "blessed" as legal to assign to plain WORD</a></p>
<p>I'm hopeful that can work.  But it brings up a question I've wrangled with in the past: <em>is there any way to make this cheap-as-free</em>?</p>
<p>e.g. <strong>Can a single Cell act as a proxy for an immutable array holding only itself?</strong></p>
<p>The idea was scrapped for things like BLOCK!, because you'd wind up with something that had nowhere to put the index or the binding.</p>
<p>But when it comes to PACK! (and SPLICE!...?) the binding requirement is gone, the index requirement is gone.  If the original array was at an index and seen to be one item, you'd just copy that one item out as the single Cell, and do some bit twiddling voodoo to it.</p>
<p>Can it work?</p>
<p><img src="https://rebol.metaeducation.com/images/emoji/twitter/thinking.png?v=14" title=":thinking:" class="emoji only-emoji" alt=":thinking:" loading="lazy" width="20" height="20"></p>
<p>In order to not tax the system too much on behalf of one weird optimization, it seems like you're down two bytes from the get-go: you've got the LIFT_BYTE at ANTIFORM_0 and the KIND_BYTE at TYPE_BLOCK.</p>
<p>Having those two bytes chewed out, it would mean the optimization would have to be something like 64-bit only, to reclaim those two bytes from the unused 32-bits in the header (which are typically unused to allow working on 32-bit platforms).  The other 3 platform pointers may all be used, because they may all contain platform pointers.  So really 64-bit builds only have 32 bits total to exploit.</p>
<p>Would tailored optimizations like this be a great use of that 32-bit advantage?  (As opposed to some cross-cutting feature that presumed to take the 32-bits out of all cells to do something more awesome)?  I don't know.  :-/</p>
<p>Is there some other way to leave the LIFT_BYTE and KIND_BYTE retain the single concent Cell's information, and yet somehow make the answer come back as PACK! without costing too much extra?  <img src="https://rebol.metaeducation.com/images/emoji/twitter/frowning.png?v=14" title=":frowning:" class="emoji" alt=":frowning:" loading="lazy" width="20" height="20"></p>
<p>The Cell could offer one bit for this... but it would wreck a lot of other optimizations if that bit didn't live somewhere in the LIFT_BYTE and KIND_BYTE.  The LIFT_BYTE already chews out 1 bit for quasi-ness (you can be both quoted and quasi, or quoted and non-quasi) which limits us to about 127 levels of quoting.  If another bit were sacrificed...let's just say it's the PACK!-bit... then we could say all PACK!s have this bit set, and we're cut down to 63-ish levels of quoting.</p>
<p>So then, if you have a regular PACK! it has the pack bit set...but when you clear the pack bit you see it's still an antiform with a heart that's a block.  And since you can't put PACK!s in packs directly (they're antiforms, they must be lifted) you know you've got a non-optimized pack.  Otherwise, you have a Cell which is the sole content of the pack.</p>
<p>It might seem like a problem that if you try to give back a pointer to the Cell as the content of the array, the only Cell you have in your hand has been corrupted with this pack bit.  So you index into the PACK! to find something that we've established acts as a PACK!.  However...this issue has already been solved for sequences, by an abstraction layer called <strong>"Arraylike"</strong>.  When you have an arraylike thing, you don't ask it for direct addresses of cells to operate on, you give it an index and ask it to fill a temporary Cell with the bits of what's at that index.  So this would be one of those things.</p>
<p><strong>Even if it could be done, this still runs afoul of a lot of the fingerprinting ideas that mask the two bytes in the header to determine its makeup.</strong>  <img src="https://rebol.metaeducation.com/images/emoji/twitter/fingerprint.png?v=14" title=":fingerprint:" class="emoji" alt=":fingerprint:" loading="lazy" width="20" height="20"></p>
<h2><a name="p-8557-just-a-placeholder-for-any-more-thoughts-1" class="anchor" href="https://rebol.metaeducation.com#p-8557-just-a-placeholder-for-any-more-thoughts-1"></a>Just a Placeholder For Any More Thoughts</h2>
<p>It's something to keep in mind, but more important to make sure the designs solve the problems, first.</p>
<p>I'm cautiously optimistic about using PACK!'d actions as the "approval" tool to do plain WORD!-assignment, but haven't done it yet, so don't know if it will actually be a good answer.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/optimizing-1-element-pack-s-splice-s/2516">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/optimizing-1-element-pack-s-splice-s/2516</link>
          <pubDate>Fri, 25 Jul 2025 17:41:49 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2516</guid>
          <source url="https://rebol.metaeducation.com/t/optimizing-1-element-pack-s-splice-s/2516.rss">Optimizing 1-Element PACK!s / SPLICE!s</source>
        </item>
        <item>
          <title>Impedance Matching LIFT The Universe With Baseline</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p><sub><em>(Sorry for the EE term.  If you're unfamiliar: <a href="https://en.wikipedia.org/wiki/Impedance_matching">Impedance Matching</a>)</em></sub></p>
<hr>
<p>A while back I realized that it's best if OBJECT!s, MODULE!s, LET!s, etc. store their contents in lifted representation.</p>
<p>...or rather... they store "historically normal" values in lifted representation (QUASIFORM! and QUOTED!).  The unlifted band would then be used for special signals.</p>
<h3><a name="p-8399-one-example-unlifted-trash-would-denote-true-unsetness-1" class="anchor" href="https://rebol.metaeducation.com#p-8399-one-example-unlifted-trash-would-denote-true-unsetness-1"></a>One Example: Unlifted TRASH! would denote <em>true unsetness</em>...</h3>
<p><em>And the term actually now fits.</em>  e.g. a state of absence of value, beneath the layer of what you could accomplish with SET.</p>
<p>With SET, you can only get "trashed" values:</p>
<pre><code>&gt;&gt; x: ~
== \~\ antiform (trash!) "tripwire"

&gt;&gt; trashed? $x
== \~null~\  ; antiform  &lt;-- it's trashed, all right...

&gt;&gt; unset? $x
== \~null~\  ; antiform   &lt;-- but it's SET to TRASH!, it's NOT "unset"! 
</code></pre>
<p>However, special tools and special cases would go <em>beneath</em> SET.</p>
<pre><code>&gt;&gt; tweak $x ~   ; tweak doesn't do the implicit LIFT that SET does...

&gt;&gt; unset? $x
== \~okay~\  ; antiform
</code></pre>
<p>There will be shorthands for that like (unset $x).  But also, you get this "unset" state as the default states in MAKE FRAME!:</p>
<pre><code>&gt;&gt; f: make frame! negate/

&gt;&gt; unset? $f.number
== \~okay~\  ; antiform  &lt;-- actually unspecialized, not specialized to trash!
</code></pre>
<p>This brings the long hoped-for distinction between unspecialized values, and values that are purposefully trash!  And what was fretted over as being a "hidden bit" is anything but... it's just one "out-of-band" operator away.</p>
<p>e.g. Note that if you use TWEAK with a lifted value, that's just like SET:</p>
<pre><code>&gt;&gt; tweak $x lift ~  ; synonym for (set $x ~)

&gt;&gt; unset? $x
== \~null~\  ; antiform

&gt;&gt; trashed? $x
== \~okay~\  ; antiform
</code></pre>
<aside class="quote no-group quote-modified" data-username="rebolbot" data-post="3" data-topic="2477">
<div class="title">
<div class="quote-controls"></div>
<img alt="" width="24" height="24" src="https://rebol.metaeducation.com/user_avatar/rebol.metaeducation.com/rebolbot/48/40_2.png" class="avatar"><a href="https://rebol.metaeducation.com/t/solving-the-pox-of-the-lift-the-universe/2477/3">Solving the Pox of the (^)... LIFT the Universe?</a></div>
<blockquote>
<p><em>"You’re designing the kind of system that can actually scale symbolic transformation, structured programming, and coherent meta-programming—because you’ve built in a substrate that encodes the ambiguity, instead of trying to erase it."</em></p>
</blockquote>
</aside>
<h2><a name="p-8399-thats-just-one-example-2" class="anchor" href="https://rebol.metaeducation.com#p-8399-thats-just-one-example-2"></a>That's Just One Example...</h2>
<p>It's the gateway to SETTER and GETTER functions <a href="http://www.rebol.net/r3blogs/0019.html">(actually desired by Carl... or setters at least... but he didn't know how to do 'em)</a></p>
<p>And for stylized setters/getters that do specific things (like type checking) there can be specially understood representations in the unlifted band.</p>
<p>Vocabulary term: I call the multiplexing of lifted and unlifted values together <strong>"DUAL REPRESENTATION"</strong></p>
<h2><a name="p-8399-all-of-it-is-powered-by-common-get-and-set-code-3" class="anchor" href="https://rebol.metaeducation.com#p-8399-all-of-it-is-powered-by-common-get-and-set-code-3"></a>All Of It Is Powered By Common GET and SET Code</h2>
<p>This wouldn't work if random places in the code ran off and inspected fields of OBJECT!s literally.</p>
<p>You have to go through some common path.  Otherwise you wind up with some callsites honoring the generalized conventions and others not.</p>
<p>It's kind of like how random code in R3-Alpha would ignore the PROTECT status of variables.  Ren-C has fought hard long and hard to nail that kind of thing down, and make sure at compile-time that you can be certain the checks aren't being skipped.</p>
<p>Piping everything through common GET and SET code paths ensures that as features like type checks or accessors are added, you don't have rogue code that doesn't honor the convention.</p>
<p><strong>It's been challenging to do this--and right now it's messy and slow--but the commonality means it's worth it to invest in optimizations for that one true path.  And Ren-C has plenty of optimization tools at its disposal, which have been evolving over the years... <sub>for when the timing is right...</sub></strong></p>
<h2><a name="p-8399-but-what-about-normal-boring-context-building-code-4" class="anchor" href="https://rebol.metaeducation.com#p-8399-but-what-about-normal-boring-context-building-code-4"></a>But What About Normal, Boring, Context-Building Code?</h2>
<p>Here's an example, just the one on my screen right now.</p>
<p>It's some random code out of the POSIX CALL implementation, related to... forking processes or something:</p>
<pre><code>if (Bool_ARG(INFO)) {
    VarList* info = Alloc_Varlist(TYPE_OBJECT, 2);

    Init_Integer(Append_Context(info, CANON(ID)), forked_pid);
    if (Bool_ARG(WAIT))
        Init_Integer(Append_Context(info, CANON(EXIT_CODE)), exit_code);

    return Init_Object(OUT, info);
}
</code></pre>
<p>Dumb, simple code making an OBJECT! with 2 fields in it, appending those fields (which default to an erased state you have to fill in to be correct code), and then setting the erased cells to mundane values.</p>
<p>If I were to go lockstep through code that looked like this and change it for lifting to appease the common GET and SET code, it would start looking like:</p>
<pre><code>if (Bool_ARG(INFO)) {
    VarList* info = Alloc_Varlist(TYPE_OBJECT, 2);

    Liftify(  // &lt;-- new wart
        Init_Integer(Append_Context(info, CANON(ID)), forked_pid)
    );
    if (Bool_ARG(WAIT))
        Liftify(  // &lt;-- new wart
            Init_Integer(Append_Context(info, CANON(EXIT_CODE)), exit_code)
        );

    return Init_Object(OUT, info);
}
</code></pre>
<p>Liftify adds 2 to the LIFT_BYTE.  (<a href="https://rebol.metaeducation.com/t/lift-byte-values-antiform-normal-quasi-quoted/2091">Review LIFT_BYTE if you want an introduction to that.</a>)</p>
<p>Liftify also has to check for overflow (e.g. that you're not going past 255 for the LIFT_BYTE value).  Maybe the optimizer can figure out it doesn't need that check here?  Though I try not to rely on the optimizer too much...</p>
<h2><a name="p-8399-this-parallels-the-too-many-of-usermode-code-5" class="anchor" href="https://rebol.metaeducation.com#p-8399-this-parallels-the-too-many-of-usermode-code-5"></a>This Parallels The "Too Many <strong><code>^</code></strong>" Of Usermode Code</h2>
<p>My observation in <strong><a href="https://rebol.metaeducation.com/t/solving-the-pox-of-the-lift-the-universe/2477">LIFT the UNIVERSE</a></strong> was that usermode code was becoming contaminated with lifts in places that weren't really the concern of that code.  <em>(That's why the robots are celebrating, they're throwing carets in the trashcan...)</em></p>
<p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://rebol.metaeducation.com/uploads/default/original/1X/f16f9c506056c76e6a9b955d445039b0e1d3eeb1.webp" data-download-href="https://rebol.metaeducation.com/uploads/default/f16f9c506056c76e6a9b955d445039b0e1d3eeb1" title="assets_task_01jx3y0xmyfc79dwb24jd57b49_1749257658_img_3"><img src="https://rebol.metaeducation.com/uploads/default/optimized/1X/f16f9c506056c76e6a9b955d445039b0e1d3eeb1_2_345x230.webp" alt="assets_task_01jx3y0xmyfc79dwb24jd57b49_1749257658_img_3" data-base62-sha1="yrQc5xkNpeTlSSrOOswU5OsMelz" width="345" height="230" srcset="https://rebol.metaeducation.com/uploads/default/optimized/1X/f16f9c506056c76e6a9b955d445039b0e1d3eeb1_2_345x230.webp, https://rebol.metaeducation.com/uploads/default/optimized/1X/f16f9c506056c76e6a9b955d445039b0e1d3eeb1_2_517x345.webp 1.5x, https://rebol.metaeducation.com/uploads/default/optimized/1X/f16f9c506056c76e6a9b955d445039b0e1d3eeb1_2_690x460.webp 2x" data-dominant-color="4D4647"></a></div><p></p>
<p><strong>Here we're seeing the C code having some of the same problem as having the carets, manifest as calls to <code>Liftify()</code>.</strong>  It's getting uglier, and spreading that ugliness around.</p>
<h2><a name="p-8399-should-a-cell_flag_dual-be-sacrificed-for-this-6" class="anchor" href="https://rebol.metaeducation.com#p-8399-should-a-cell_flag_dual-be-sacrificed-for-this-6"></a>Should A <strong><code>CELL_FLAG_DUAL</code></strong> Be Sacrificed For This?</h2>
<p>I don't like wasting the very few CELL_FLAG_XXX.  But over time, silly ones have been freed up to give us some wiggle room (e.g. the <a>now-completely superfluous CELL_FLAG_FALSEY</a>).</p>
<p>And maybe this is a really good case where it could be of help to sacrifice one.  Since all the GET and SET that's <em>not</em> this kind of stuff is running through centralized code...it could be tolerant of cells in contexts that <em>weren't</em> initialized with CELL_FLAG_DUAL, and just know that those are to be taken literally.</p>
<p>It complicates things a little bit in that one <em>"big, beautiful code path"</em>.  But as a caller of TWEAK or GET and SET you're insulated from the complication.  It's a black box... maybe the cell has <code>CELL_FLAG_DUAL</code> and maybe it doesn't, you'll never know.</p>
<h2><a name="p-8399-just-have-to-catch-confusions-before-they-happen-7" class="anchor" href="https://rebol.metaeducation.com#p-8399-just-have-to-catch-confusions-before-they-happen-7"></a>Just Have To Catch Confusions Before They Happen...</h2>
<p>Probably best is just to throw in some asserts if you somehow start running through code paths that don't use the common GET somehow, and make sure <code>Type_Of()</code> and <code>Quotes_Of()</code> etc will assert on anything that has CELL_FLAG_DUAL.</p>
<p>I'm not sure how many legitimate codepaths there will be that duck the legitimate GET, but there are some reasonable cases (such as the code I give above) that are just doing a simple construction and probably don't need to be more complicated than they already are.</p>
<h2><a name="p-8399-will-it-slow-things-down-8" class="anchor" href="https://rebol.metaeducation.com#p-8399-will-it-slow-things-down-8"></a>Will It Slow Things Down?</h2>
<p>I posted this under Optimization because it's trading off some runtime code to make the C code more tolerable.</p>
<p>But rest assured, this is not the flag test that will be the bottleneck of the system.  <img src="https://rebol.metaeducation.com/images/emoji/twitter/slight_smile.png?v=14" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p>
<p><em>(Compared to naive Liftify() everywhere, it probably breaks at least even for not having to do the overflow checking of the LIFT_BYTE.)</em></p>
            <p><small>6 posts - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/impedance-matching-lift-the-universe-with-baseline/2483">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/impedance-matching-lift-the-universe-with-baseline/2483</link>
          <pubDate>Sat, 07 Jun 2025 23:00:06 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2483</guid>
          <source url="https://rebol.metaeducation.com/t/impedance-matching-lift-the-universe-with-baseline/2483.rss">Impedance Matching LIFT The Universe With Baseline</source>
        </item>
        <item>
          <title>Weird Optimization History: CELL_FLAG_FALSEY</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>In the very, very early times of Ren-C, there was no QUOTE_BYTE chewing 8 bits out of the header.  So CELL_FLAG_XXX were more plentiful in the remaining header space.  Because they were so plentiful, it was picked to use one to say whether a cell was "falsey".</p>
<p>At first, falsey types would be blank, and LOGIC! false.  This which expanded to blank, null, and logic false.  And LOGIC! could use the flag as its payload.</p>
<p>Today's world with "flexible logic" only has null as "falsey" (though we call it "branch inhibitor" when we want to be technical, since false is a word under that worlview).  And LOGIC! is gone as a fundamental type.  So even if we had bits to spare, it wouldn't be spent on CELL_FLAG_FALSEY.</p>
<p>In any case, the bootstrap executable has been patched to modern conventions of BLANK! being truthy and such.  So this little piece of history is going away.  I felt like giving it a goodbye post...</p>
<pre><code>//=//// CELL_FLAG_FALSEY //////////////////////////////////////////////////=//
//
// This flag is used as a quick cache on NULL, BLANK! or LOGIC! false values.
// These are the only three values that return true from the NOT native
// (a.k.a. "conditionally false").  All other types return true from TO-LOGIC
// or its synonym, "DID".
//
// (It's also placed on END cells and TRASH cells, to speed up the Type_Of()
// check for finding illegal types...by only checking falsey types.)
//
// Because of this cached bit, LOGIC! does not need to store any data in its
// payload... its data of being true or false is already covered by this
// header bit.
//
#define CELL_FLAG_FALSEY \
    FLAG_LEFT_BIT(18)
</code></pre>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/weird-optimization-history-cell-flag-falsey/2420">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/weird-optimization-history-cell-flag-falsey/2420</link>
          <pubDate>Fri, 18 Apr 2025 23:49:44 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2420</guid>
          <source url="https://rebol.metaeducation.com/t/weird-optimization-history-cell-flag-falsey/2420.rss">Weird Optimization History: CELL_FLAG_FALSEY</source>
        </item>
        <item>
          <title>Flexible Symbol Reordering for Optimization</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>In the original formulation of building the symbol table, it kept a running count of the current symbol ID, and incremented it as it went.</p>
<p>The order wasn't always random.  For instance, one of the optimizations based on ordering was that the symbols for R3-Alpha's PARSE keywords <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/words.r#L101">were in a particular order</a>, which was specified in <code>%words.r</code></p>
<pre><code>; Parse: - These words must not reserved above!!
parse
|	 ; must be first
; prep words:
set  
copy
some
any
opt
not
and
then
remove
insert
change
if
fail
reject
while
return
limit
??
accept
break
; match words:
skip
to
thru
quote
do
into
only
end  ; must be last
</code></pre>
<p>By putting these in such an order, when PARSE3 was processing a rule it could do a <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/u-parse.c#L65">quick check of whether or not something was a keyword</a>:</p>
<pre><code>#define GET_CMD(n) (((n) &gt;= SYM_OR_BAR &amp;&amp; (n) &lt;= SYM_END) ? (n) : 0)
</code></pre>
<p>And then further it divided the range based on which phase the rule was in... <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/u-parse.c#L692">either "pre-match" or "match"</a>:</p>
<pre><code>if (cmd = VAL_CMD(item)) {
    if (!IS_WORD(item))
        Trap1(RE_PARSE_COMMAND, item); // SET or GET not allowed

        if (cmd &lt;= SYM_BREAK) { // optimization
            switch (cmd) {...}
        ...
</code></pre>
<p>Though this is a bit brittle, and hard to be sure you're adjusting all the dependencies if you change it (more on that later...)</p>
<h2><a name="p-8040-wanting-to-find-length-of-quickly-from-length-1" class="anchor" href="https://rebol.metaeducation.com#p-8040-wanting-to-find-length-of-quickly-from-length-1"></a>Wanting To Find LENGTH-OF Quickly From LENGTH</h2>
<p>I had a stroke of inspiration when I realized that modern binding made it possible that the <strong>OF</strong> function could be declared as infix with only a left hand side, and then it could effectively retrigger <strong><code>length of</code></strong> to act the same as <strong><code>length-of</code></strong>, and do that for any <strong><code>xxx of</code></strong> to run a function it dynamically looks up as <strong><code>xxx-of</code></strong>:</p>
<p><a href="https://rebol.metaeducation.com/t/squaring-the-circle-of-length-and-length-of/385/8" class="inline-onebox">Squaring the circle of LENGTH? and LENGTH-OF - #8 by hostilefork</a></p>
<p>This is a breakthrough powered by modern binding.  However, one of the things it needs to do is to somehow navigate from the <code>Symbol</code> pointer for <strong><code>xxx</code></strong> to <strong><code>xxx-of</code></strong>... and that's not exactly free.  You have to build a UTF-8 buffer which has the "-of" variant, and hash it to look it up in the symbol table.</p>
<p>But I had an idea <img src="https://rebol.metaeducation.com/images/emoji/twitter/light_bulb.png?v=14" title=":light_bulb:" class="emoji" alt=":light_bulb:" loading="lazy" width="20" height="20"></p>
<aside class="quote no-group" data-username="hostilefork" data-post="8" data-topic="385">
<div class="title">
<div class="quote-controls"></div>
<img alt="" width="24" height="24" src="https://rebol.metaeducation.com/user_avatar/rebol.metaeducation.com/hostilefork/48/421_2.png" class="avatar"><a href="https://rebol.metaeducation.com/t/squaring-the-circle-of-length-and-length-of/385/8">Squaring the circle of LENGTH? and LENGTH-OF</a></div>
<blockquote>
<p>I'm going to give it a shot. There's probably some easy optimizations for linking symbols like WHATEVER to be able to navigate to the symbol for WHATEVER-OF rapidly so you don't have to do any hashing to find it... at least for builtin symbols (e.g. put them sequentially in the SymId table).</p>
</blockquote>
</aside>
<p>Hence I reworked the code that builds the symbol table.  Instead of pinning down the symbol numbers permanently as it goes, it waits to assign the numbers until all the symbols have been added.  This permits reorganization.</p>
<p>So if it sees the symbol for LENGTH-OF, it goes through and looks to see if a symbol for LENGTH was already added.  If it was, it goes back and inserts LENGTH-OF right after LENGTH.  If it hasn't been added yet, then it goes ahead and adds both LENGTH and LENGTH-OF.</p>
<p>This way you get sequential symbol IDs, and it's trivial and fast to get to LENGTH-OF if you have LENGTH in your hand.  This works for any built-in symbol, so we're getting good performance for super common operations like <strong><code>type of</code></strong></p>
<h2><a name="p-8040-but-it-needs-exceptions-2" class="anchor" href="https://rebol.metaeducation.com#p-8040-but-it-needs-exceptions-2"></a>But It Needs Exceptions...</h2>
<p>There are some places you can't disrupt the ordering, for instance the datatype symbols are expected to be in a certain fixed order.  So the reordering logic checks to make sure you're not disrupting those cases.  So far there are only two exceptions that need to be made: <strong>sigil</strong> and <strong>file</strong>, which have operations <strong>sigil of</strong> and <strong>file of</strong></p>
<pre><code>&gt;&gt; sigil of first [$hello world]
== $

&gt;&gt; sigil of second [$hello world]
== ~null~  ; anti

&gt;&gt; data: load %something.r
== [a [b c] d]

&gt;&gt; second data
== [b c]

&gt;&gt; file of second data
== %something.r
</code></pre>
<p>The design point of keeping type symbols in order is too important, so <code>SYM_SIGIL</code> can't be followed by <code>SYM_SIGIL_OF</code> and <code>SYM_FILE</code> can't be followed by <code>SYM_FILE_OF</code>.  But these were the only two exceptions, and handling them is trivial!</p>
<pre><code>Option(SymId) id = Symbol_Id(sym);
if (id) {  // built-in symbols w/-OF variations, optimized positions
    if (id == SYM_SIGIL)
        sym_of = CANON(SIGIL_OF);  // needs exception
    else if (id == SYM_FILE)
        sym_of = CANON(FILE_OF);  // needs exception
    else
        sym_of = Canon_Symbol(cast(SymId, (unwrap id) + 1));
}
else {
    ... // slower method, produce UTF-8 for "-of" variant, hash it
}
</code></pre>
<p>The build process detects any cases where a disruption would be a problem and warns you so you know that you have to add an exception.  For instance: let's say you want to keep the PARSE3 keyword sequential trick intact.  Since there's a word there for LIMIT, it would be a problem if we added a LIMIT-OF native, and an exception would have to be added here.</p>
<p>It's not a huge issue and I don't expect a lot of exceptions needing to be made here.  In any case, what makes this so fast is that turning a <code>SYM_XXX</code> into a <code>Symbol*</code> is trivially fast, because there's just a global array of Symbol (not even an array of Symbol*, it's the Stub structure for the symbol itself) indexed by the SYM constant.</p>
<h2><a name="p-8040-eliminating-the-hardcoding-of-symbol-ids-for-ranges-3" class="anchor" href="https://rebol.metaeducation.com#p-8040-eliminating-the-hardcoding-of-symbol-ids-for-ranges-3"></a>Eliminating The Hardcoding of Symbol IDs for Ranges</h2>
<p>This was a rework that makes it easier for other ideas down the line of how to creatively use the symbol IDs.  So I made it easier to mark positions in the IDs in a way that's less likely to break.</p>
<p>I use TAG! to put in the markers.  For instance, the PARSE3 keywords are now done as:</p>
<pre><code>&lt;MIN_SYM_PARSE3&gt;  ; no &lt;/&gt; means next symbol (SYM_SOME is MIN_SYM_PARSE3)
    some
    opt
    optional
    repeat
    ; ... etc ... 
&lt;MIN_SYM_PARSE3_MATCH&gt;
    skip
    one
    to
    thru
    ; ... etc ...
    end 
&lt;/MAX_SYM_PARSE3&gt;  ;  &lt;/&gt; means prior symbol (SYM_END is MAX_SYM_PARSE3)
</code></pre>
<p>Once all the symbols have been worked out, the tags are removed and automatically turned into <code>#define</code>s by <code>%make-boot.r</code>:</p>
<pre><code>/*
 * These definitions are derived from markers added during the symbol
 * table creation process via ADD-SYM:PLACEHOLDER (and are much better
 * than hardcoding symbol IDs in source the way R3-Alpha did it!)
 */
#define MIN_SYM_TYPESETS  253  /* blank? */
#define MAX_SYM_TYPESETS  407  /* any-element? */
#define MIN_SYM_PARSE3  427  /* some */
#define MIN_SYM_PARSE3_MATCH  454  /* skip */
#define MAX_SYM_PARSE3  461  /* end */
#define MIN_SYM_NATIVE  480  /* native */
#define MAX_SYM_LIB_PREMADE  876  /* inflate */
#define MIN_SYM_ERRORS  988  /* Internal */
#define MAX_SYM_BUILTIN  1207  /* export* */
</code></pre>
<p>It's much less brittle, communicates what's going on, and lets you search the code for where the given trick is being used.</p>
<p>So from now on, coming up with similar tricks will be much easier and robust!</p>
<h2><a name="p-8040-using-more-rebol-to-make-rebol-better-4" class="anchor" href="https://rebol.metaeducation.com#p-8040-using-more-rebol-to-make-rebol-better-4"></a>Using More Rebol To Make Rebol Better</h2>
<p>As Ren-C's power grows, it makes more and more sense to implement more of the project using Rebol technique.</p>
<p>I'm really pleased by things like the TAG! trick for marking ranges (as I did in <code>%types.r</code>), and just how expressive you can get with things.</p>
<p>Although the bootstrap code is a total mess, it's amazing to throw Rebol concepts at simplifying it...and as string interpolation and other magic becomes commonplace, it's really shaping up.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/flexible-symbol-reordering-for-optimization/2372">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/flexible-symbol-reordering-for-optimization/2372</link>
          <pubDate>Tue, 18 Mar 2025 12:58:50 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2372</guid>
          <source url="https://rebol.metaeducation.com/t/flexible-symbol-reordering-for-optimization/2372.rss">Flexible Symbol Reordering for Optimization</source>
        </item>
        <item>
          <title>Further Optimizations Of Breaking the 64-Type Barrier</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Historical Rebol/Red are limited to 64 datatypes.  It's not because there isn't space in the cells to store more (e.g. 256 in a header byte) -- but because TYPESET! wanted to fit into a Cell's payload, as a 64-bit integer to sparsely convey an arbitrary collection of bits in a typeset.</p>
<p>Ren-C made the bold move away from typesets into <strong>type predicates</strong>... arbitrary functions that can check more than just the type.  So you can take an argument and demand it be EVEN?, for instance...</p>
<p>I've discussed how this made it critical to be able to call functions like EVEN? or SPLICE? <em>very quickly</em>.  A big part of that is <strong><a href="https://rebol.metaeducation.com/t/intrinsics-functions-without-frames/2050">Intrinsics: Functions Without Frames</a></strong>.</p>
<p>What's really neat about the ability to call intrinsics super fast is that it doesn't only apply in type checking.  Ordinary evaluation gets accelerated as well, e.g. you can write <strong>if even? number [...]</strong> and it uses the same magic that turbocharges typechecks.</p>
<p><em>(Note: All intrinsics are stylized in such a way that they can be called non-intrinsically if need be.  In fact, the debug build sporadically calls intrinsics non-intrinsically to make sure they still work.  This would be needed if you wanted a stepwise debugger, so that it didn't seem to eerily skip functions every time an intrinsic dodged making a frame.)</em></p>
<h2><a name="p-8033-a-key-optimization-4-or-8-parameter-bytes-1" class="anchor" href="https://rebol.metaeducation.com#p-8033-a-key-optimization-4-or-8-parameter-bytes-1"></a>A Key Optimization: 4 or 8 PARAMETER! Bytes</h2>
<p>The PARAMETER! dataype's Cell has 4 spare bytes on 32-bit platforms, and 8 spare bytes on 64-bit platforms. So this gave me an idea to have a numbered list of up to 255 built-in intrinsics.  There'd be a one-time analysis of the argument spec to see if you used any of those 255 common type checks, and if you did then it would encode that in the bytes so you wouldn't have to actually call the function--you could just run the test the function would do.</p>
<p>So let's say you said: <strong><code>[integer! any-list?]</code></strong>.  It would recognize these as being in the built in list (INTEGER! equivalent to the INTEGER? constraint).  And you would get an optimized byte sequence like <strong><code>{2, 76, 0}</code></strong>.  (0 is reserved for indicating a premature end, but if you're on a 32 bit platform and it sees 4 values with no 0 it knows they all apply... and if you're on a 64-bit platform and it sees 8 values with no 0 it knows they all apply.  Only a single bit is needed to mark "incomplete optimizations" where it has to run through the typeset array.)</p>
<p>There's a special bit set on the cells in the array for the typeset to say that they were accounted for by the optimization.  This means if you can't fully optimize the array (either because you have too many types to check for the 4 or 8 bytes, or because you used constraints that aren't in the built in list) then it still knows it can skip past the optimized constraint when it's walking the array to test the outliers.</p>
<p>Here is a list of the optimized bytes so far (not up to 255 yet!)</p>
<pre><code>blank 1
integer 2
decimal 3
percent 4
pair 5
money 6
time 7
date 8
parameter 9
bitset 10
map 11
handle 12
blob 13
text 14
file 15
tag 16
url 17
email 18
issue 19
sigil 20
varargs 21
object 22
module 23
error 24
port 25
frame 26
word 27
meta-word 28
type-word 29
the-word 30
var-word 31
tuple 32
meta-tuple 33
type-tuple 34
the-tuple 35
var-tuple 36
chain 37
meta-chain 38
type-chain 39
the-chain 40
var-chain 41
path 42
meta-path 43
type-path 44
the-path 45
var-path 46
block 47
meta-block 48
type-block 49
the-block 50
var-block 51
fence 52
meta-fence 53
type-fence 54
the-fence 55
var-fence 56
group 57
meta-group 58
type-group 59
the-group 60
var-group 61
comma 62
quasiform 63
quoted 64
antiform 65
any-string 66
any-context 67
any-word 68
any-tuple 69
any-chain 70
any-path 71
any-sequence 72
any-block 73
any-fence 74
any-group 75
any-list 76
any-bindable 77
any-unit 78
any-inert 79
any-isotopic 80
any-number 81
any-scalar 82
any-sequencable 83
any-series 84
any-utf8 85
any-branch 86
any-plain-value 87
any-meta-value 88
any-type-value 89
any-the-value 90
any-var-value 91
any-element 92
</code></pre>
<p><em>(I'm now planning to have separate type bytes for all antiform categories instead of all being REB_ANTIFORM, which will help in internal code as well as make accelerators here, so that will probably chew up a pretty large range since it will likely just be the type byte multiplied by 2...so imagine 64 more of these being used up in the near term.  I'm sure there could be clever uses for the gaps, if you can truly guarantee a type will never be isotopic...)</em></p>
<h2><a name="p-8033-removing-generality-to-boost-performance-2" class="anchor" href="https://rebol.metaeducation.com#p-8033-removing-generality-to-boost-performance-2"></a>Removing Generality To Boost Performance</h2>
<p>My first cut at implementing this tried to be very general.  I had a table of up to 255 C function pointers that took a Cell as an argument, and returned a boolean.  This meant basically any constraint could be optimized... like EVEN?, because it had not only the type (e.g. INTEGER!) but it had the full cell it could pick apart:</p>
<pre><code>bool Optimized_Even_Checker(Cell* cell) {
    if (Cell_Type(cell) != REB_INTEGER)
        return false;
   return VAL_INT64(cell) % 2 == 0;
}
</code></pre>
<p>But as you see from the table above, I'd only gotten around to automating the construction of the table based on information in <code>%types.r</code>.  So all the functions did was test the cell's type.  There were three kinds of checkers:</p>
<pre><code>bool Optimized_Integer_Checker(Cell* cell) {  // single datatype
    return Cell_Type(cell) == REB_INTEGER;
}

bool Optimized_Any_List_Checker(Cell* cell) {  // datatype range
    Type type = Cell_Type(cell);
    return type &gt;= REB_BLOCK and type &lt;= REB_VAR_GROUP;
}

bool Optimized_Any_Scalar_Checker(Cell* cell) {  // sparse typeset
    Type type = Cell_Type(cell);
    return g_sparse_memberships[type] &amp; TYPESET_FLAG_SCALAR;
}
</code></pre>
<p>So what you see is that the way that the type bytes are numbered, sometimes you can check for typeset membership by just seeing if it's in a certain range.  <a href="https://github.com/hostilefork/ren-c/blob/4e64ae1652ea5efd603549f35850b11cf0a22068/src/boot/types.r#L211-L237">Careful arrangement of <code>%types.r</code></a> means that's possible for a lot of checks.</p>
<p>But if something doesn't fit that model, there's another table of sparse typeset flags put together.  I've limited the number of sparse typesets to 31--and I'll explain why--but there's only 13 of them so far.</p>
<h2><a name="p-8033-lets-do-some-slight-transformations-3" class="anchor" href="https://rebol.metaeducation.com#p-8033-lets-do-some-slight-transformations-3"></a>Let's Do Some Slight Transformations</h2>
<p>Let's say that we decide that anything that can't be checked by datatype alone we're willing to call as an (often intrinsic) function.  So we're willing to pass the type in and not have each function recalculate it.</p>
<p>ALSO: Let's say that we want to collapse datatype checking to just be a trivial case of range checking, where the start and end of the range are the same.</p>
<pre><code>bool Optimized_Integer_Checker(Type type) {  // trivial range
    return type &gt;= REB_INTEGER and type &lt;= REB_INTEGER;
}

bool Optimized_Any_List_Checker(Type type) {  // non-trivial range
    return type &gt;= REB_BLOCK and type &lt;= REB_VAR_GROUP;
}

bool Optimized_Any_Scalar_Checker(Type type) {  // sparse
    return g_sparse_memberships[type] &amp; TYPESET_FLAG_SCALAR;
}
</code></pre>
<p>That's interesting... but why are we calling a function at all?  What if we drop the function pointers and just make the table be the information?</p>
<p>...BUT... <img src="https://rebol.metaeducation.com/images/emoji/twitter/thinking.png?v=14" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"> what if there's a bit reserved, let's say TYPESET_FLAG_0_RANGE, which we use to indicate the table entry has two bytes of range information, start and finish.  <strong>and if that bit is not set, then the table entry has a single TYPESET_FLAG_XXX for the flag you need to test in the sparse table!</strong></p>
<pre><code>INLINE bool Builtin_Typeset_Check(TypesetByte typeset_byte, Type type) {
    TypesetFlags typeset = g_typesets[typeset_byte];

    if (typeset &amp; TYPESET_FLAG_0_RANGE) {  // trivial ranges ok (one datatype)
        Byte start = THIRD_BYTE(&amp;typeset);
        Byte end = FOURTH_BYTE(&amp;typeset);
        return start &lt;= type and type &lt;= end;  // note: type converts to Byte
    }

    return did (g_sparse_memberships[type] &amp; typeset);  // just a typeset flag
}
</code></pre>
<p><strong>You don't even have to shift a datatype to do the check.</strong>  Where historical Rebol required a 64-bit shift, the flag you need to test against is just waiting for you in the array (and sparse memberships only needs a 32-bit integer).</p>
<p>Blazing through a list of these typechecks is very efficient.</p>
<h2><a name="p-8033-so-why-not-use-this-for-generic-dispatch-4" class="anchor" href="https://rebol.metaeducation.com#p-8033-so-why-not-use-this-for-generic-dispatch-4"></a>So Why Not Use This For Generic Dispatch?</h2>
<p>When I realized I could now hum through a table of "fancy-sounding" type constraints like ANY-SERIES? or ANY-SCALAR? and not break a sweat, it made me think this is a perfect way to deal with the methodization of generics.</p>
<p>All I had to do was order the type checks from specific to more general, run the checks in sorted order, and everything from molding to comparison to multiplication could be dispatched at as fine a granularity as you want.</p>
<p>You write in the source your implementations wherever you want, like:</p>
<pre><code>#define IMPLEMENT_GENERIC(name,type) \
    Bounce g_##name##_##type(Level* level_)  // e.g. g_APPEND_Any_List

IMPLEMENT_GENERIC(LESSER_Q, Is_Integer)  // LESSER? -&gt; LESSER_Q in C
{ ... }

IMPLEMENT_GENERIC(LESSER_Q, Any_Context)
{ ... }
</code></pre>
<p>Then the build process packages that all up into a table, with the constraint byte in order:</p>
<pre><code>#define GENERIC_CFUNC(name,type)  G_##name##_##type

GenericTable g_generic_lesser_q[] = {
    {2, &amp;GENERIC_CFUNC(LESSER_Q, Is_Integer)},  // =&gt; {2, &amp;g_LESSER_Q_Is_Integer}
    {3, &amp;GENERIC_CFUNC(LESSER_Q, Is_Decimal)},
    {6, &amp;GENERIC_CFUNC(LESSER_Q, Is_Money)},
    {7, &amp;GENERIC_CFUNC(LESSER_Q, Is_Time)},
    {8, &amp;GENERIC_CFUNC(LESSER_Q, Is_Date)},
    {10, &amp;GENERIC_CFUNC(LESSER_Q, Is_Bitset)},
    {13, &amp;GENERIC_CFUNC(LESSER_Q, Is_Blob)},
    {26, &amp;GENERIC_CFUNC(LESSER_Q, Is_Frame)},
    {67, &amp;GENERIC_CFUNC(LESSER_Q, Any_Context)},
    {72, &amp;GENERIC_CFUNC(LESSER_Q, Any_Sequence)},
    {76, &amp;GENERIC_CFUNC(LESSER_Q, Any_List)},
    {85, &amp;GENERIC_CFUNC(LESSER_Q, Any_Utf8)},
    {92, &amp;GENERIC_CFUNC(LESSER_Q, Any_Element)},
    {0, nullptr}
};
</code></pre>
<p>Then the dispatch is as fast as heck!  Remember that the compiler inlines this so it's not calling a <code>Builtin_Typeset_Check()</code> function each time through the loop, it's generating micro-optimized code:</p>
<pre><code>#define Dispatch_Generic(name,cue,L) \
    Dispatch_Generic_Core(g_generic_##name, Cell_Type(cue), (L))

Bounce Dispatch_Generic_Core(
    GenericTable* table,
    Type type,
    Level* level
){
    for (; table-&gt;typeset_byte != 0; ++table) {
        if (Builtin_Typeset_Check(table-&gt;typeset_byte, type))
            return table-&gt;dispatcher(level);
    }

    return FAIL("No dispatcher for datatype of generic");
}
</code></pre>
<p>Notice how this has the fallthrough to ANY-ELEMENT?, so if you pass something that's unhandled you can still do a baseline handler!</p>
<p><em>(I'm considering making it possible to do something like <code>return PASS</code> and have it continue bubbling down the chain, but not jumping the gun on features just yet... you can explicitly call a handler via <code>GENERIC_CFUNC(generic, type)</code> if you want, and that doesn't add cost to every dispatch, so I'm seeing how far it goes.)</em></p>
<h2><a name="p-8033-goodbye-stupid-r3-alphalike-switch-statementshttpsgithubcomrebolrebolblob25033f897b2bd466068d7663563cd3ff64740b94srccoret-stringcl464-l494-5" class="anchor" href="https://rebol.metaeducation.com#p-8033-goodbye-stupid-r3-alphalike-switch-statementshttpsgithubcomrebolrebolblob25033f897b2bd466068d7663563cd3ff64740b94srccoret-stringcl464-l494-5"></a>Goodbye <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/t-string.c#L464-L494">Stupid R3-Alphalike switch() Statements!</a></h2>
<p>This trounces the old means of specifying dispatch and subclassing and overriding.</p>
<p>R3-Alpha had cryptic information, with a bunch of <strong><code>*</code></strong> and <strong><code>-</code></strong> and <strong><code>f+</code></strong> stuff:</p>
<aside class="onebox githubblob" data-onebox-src="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/types.r#L51-L57">
  <header class="source">

      <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/types.r#L51-L57" target="_blank" rel="noopener">github.com/rebol/rebol</a>
  </header>

  <article class="onebox-body">
    <h4><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/types.r#L51-L57" target="_blank" rel="noopener">src/boot/types.r</a></h4>

<div class="git-blob-info">
  <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/types.r#L51-L57" rel="noopener"><code>25033f897</code></a>
</div>



    <pre class="onebox"><code class="lang-r">
      <ol class="start lines" start="51" style="counter-reset: li-counter 50 ;">
          <li>	email       self        string      +        f*      *      *   [series string]  </li>
          <li>	url         self        string      +        f*      file   *   [series string]</li>
          <li>	tag         self        string      +        +       *      *   [series string]  </li>
          <li></li>
          <li>	bitset      self        bitset      *        *       *      *   -</li>
          <li>	image       self        image       +        +       *      *   series</li>
          <li>	vector      self        vector      -        -       *      *   series  </li>
      </ol>
    </code></pre>



  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

<p>But at the end of the day you were usually stuck with an all-or-nothing... if you said that a GROUP! handled APPEND the same way that a BLOCK! handled append, you wound up having to say that <em>all</em> generics routed through ANY-LIST?.  They had the same entry point for everything else.</p>
<p>This breaks that all wide open, with granular overriding.  And blows away the 64 type limit.  And with the native entry point available to generics you can do common processing before any generic runs, to enforce things like commutativity in addition and multiplication, etc...</p>
<pre><code>//
//  /multiply: native:generic [
//
//  "Returns the second value multiplied by the first"
//
//      return: [any-scalar?]
//      value1 [any-scalar?]
//      value2 [any-scalar?]
//  ]
//
DECLARE_NATIVE(multiply)
//
// 1. Most languages want multiplication to be commutative (exceptions like
//    matrix multiplication do exist, though that likely should be a different
//    operation and reserve MULTIPLY for element-wise multiplication).  To
//    ensure commutativity, we swap the arguments if their heart bytes are
//    not in "canon order".
//
//    (Using the HEART_BYTE as the canon order is a bit of a hack, as the
//    table can be reordered.  But we try to order the types in %types.r
//    such that more complex types come later, so that we dispatch to the
//    more complex type...e.g. multiplying a PAIR! by a DECIMAL! should
//    should dispatch to the PAIR! code.)
{
    INCLUDE_PARAMS_OF_MULTIPLY;

    Value* v1 = ARG(value1);
    Value* v2 = ARG(value2);

    if (HEART_BYTE(e1) &lt; HEART_BYTE(e2)) {  // simpler type is on left [1]
        Move_Cell(SPARE, v2);
        Move_Cell(v2, v1);  // ...so move simpler type to be on the right
        Move_Cell(v1, SPARE);
    }

    return Dispatch_Generic(MULTIPLY, v1, LEVEL);
}
</code></pre>
<h2><a name="p-8033-lots-to-do-but-this-is-a-step-in-the-right-direction-6" class="anchor" href="https://rebol.metaeducation.com#p-8033-lots-to-do-but-this-is-a-step-in-the-right-direction-6"></a>Lots to Do, But This Is A Step In The Right Direction</h2>
            <p><small>2 posts - 2 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/further-optimizations-of-breaking-the-64-type-barrier/2369">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/further-optimizations-of-breaking-the-64-type-barrier/2369</link>
          <pubDate>Sun, 16 Mar 2025 20:28:37 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2369</guid>
          <source url="https://rebol.metaeducation.com/t/further-optimizations-of-breaking-the-64-type-barrier/2369.rss">Further Optimizations Of Breaking the 64-Type Barrier</source>
        </item>
        <item>
          <title>Issues With Writing Output Cells Directly</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <h3><a name="p-8021-how-r3-alpha-natives-returned-their-result-1" class="anchor" href="https://rebol.metaeducation.com#p-8021-how-r3-alpha-natives-returned-their-result-1"></a>How R3-Alpha Natives Returned Their Result</h3>
<p>The protocol for return result for natives in R3-Alpha was that <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/include/sys-stack.h#L104C1-L115C3">an enumerated type</a> said where the output result could be found:</p>
<pre><code>enum {
	R_RET = 0,
	R_TOS,
	R_TOS1,
	R_NONE,
	R_UNSET,
	R_TRUE,
	R_FALSE,
	R_ARG1,
	R_ARG2,
	R_ARG3
};
</code></pre>
<p>Each invocation of a native pushed some space for a cell where you could write a return result, and <code>DS_RETURN</code> was that arbitrary cell.  If that's where the result was, the native would return <code>R_RET</code>.</p>
<p>The other return values <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/c-function.c#L289">were shorthands</a> to save you from having to copy or initialize a result into DS_RETURN from somewhere else.  e.g. <code>return R_TOS</code> meant to look for the result at the "Top Of Stack", so your native wouldn't have to copy the cell from that location and then drop an element off the stack.  So it was a shorthand for:</p>
<pre><code>*DS_RETURN = *DS_TOP;
DS_DROP;
return R_RET;
</code></pre>
<p><code>return R_TRUE</code> kept you from having to initialize the <code>DS_RETURN</code> slot with a logic, hence a shorthand for:</p>
<pre><code>SET_TRUE(DS_RETURN);
return R_RET;
</code></pre>
<h3><a name="p-8021-ren-c-writes-directly-to-a-target-cell-2" class="anchor" href="https://rebol.metaeducation.com#p-8021-ren-c-writes-directly-to-a-target-cell-2"></a>Ren-C Writes Directly To A Target Cell</h3>
<p>Early changes for Ren-C brought more rigor to the data stack and checks on how it was used. (<em><a href="https://www.youtube.com/watch?v=6nsKTpArTCE&amp;t=1094s">I have explained many of these changes.</a></em>)</p>
<p>It also expanded the return value of natives to be a fairly arbitrary pointer...a role that I call a <strong><code>Bounce</code></strong>.  This which can be detected as being Cells, <a href="https://rebol.metaeducation.com/t/what-should-returning-a-string-from-a-native-do/2357">or UTF-8 strings</a>, or other indicators (some indicators ask for the trampoline to cycle back and run a pushed stack level, without creating a nested C stack).  You can even return a C nullptr to indicate a <code>~null~</code> antiform.</p>
<p>But rather than having a slot on the data stack where results are expected to be written, each interpreter stack level has an <code>OUT</code> pointer.  When you instantiate a stack level, this pointer is specified in the instantiation...and it's supposed to be somewhere that already exists.</p>
<p>Notably, this pointer cannot be in the data stack...because the data stack can be resized at arbitrary moments (e.g. on a stack expansion).  However, it would be possible to do something similar to R3-Alpha and have a Bounce signal that said the result lived there...which would just mean the code executing natives would copy whatever was on the top of the stack into the <code>OUT</code> location at the moment of return as a convenience.</p>
<h2><a name="p-8021-direct-write-was-conceived-as-an-optimization-3" class="anchor" href="https://rebol.metaeducation.com#p-8021-direct-write-was-conceived-as-an-optimization-3"></a>Direct Write Was Conceived As An Optimization</h2>
<p>It would have been possible for Ren-C to have a Cell's worth of space in the "Level" representing an intepreter stack level, instead of being given a pre-existing pointer.  But the concept was that saying where to write the output would save on needing to move the result after evaluation was finished.</p>
<p>But there's a few catches that have come up...</p>
<ol>
<li>
<p><strong>Indirect Writes Are Slower</strong> - The OUT cell is used for intermediate calculations.  Locality-wise, performance has shown that writing to <code>L-&gt;out</code> is noticeably more expensive than if it were a plain cell and you were writing to <code>&amp;L-&gt;out</code>.  If you do a lot of these intermediate calculations the extra dereferences wind up outweighing having to do a single move of the output cell at the end.</p>
</li>
<li>
<p><strong>Stack Suspension Gets Complex</strong> - In things like generators, you want to suspend a Level stack.  When you do so, the place that was requested as "where to write to" will change... so anywhere in the suspended stack where the output-to pointer is mentioned has to be turned into a placeholder value, so that when you restore the stack with a new idea of where to write the output cell  of the top of stack it has to go through and fix up those placeholders to point to the new location.</p>
</li>
<li>
<p><strong>Handling Failures May Have Invalid Stack Locations</strong> - In the model that has been established regarding things like abrupt failures, it's possible for a stack Level to run some cleanup code if it needs to.  So the throw or longjmp happens and the Trampoline catches it with the last pushed Level still intact.  But if the Level's <code>L-&gt;out</code> pointer was to a cell on the stack, then it may be invalid during this handling code.</p>
</li>
</ol>
<p>[2] was annoying to work through, but it's really [3] that I am struggling with.  <strong>Things would be simpler if there was a cell as part of the Level itself, whose lifetime was equal to the Level's, where results were written.</strong></p>
<h2><a name="p-8021-changing-it-feels-like-a-step-backward-pouting_cat-but-4" class="anchor" href="https://rebol.metaeducation.com#p-8021-changing-it-feels-like-a-step-backward-pouting_cat-but-4"></a>Changing It Feels Like A Step Backward <img src="https://rebol.metaeducation.com/images/emoji/twitter/pouting_cat.png?v=14" title=":pouting_cat:" class="emoji" alt=":pouting_cat:" loading="lazy" width="20" height="20"> But...</h2>
<p>It would still be possible to avoid copying if all you were interested in was the result.  (Think of something like <code>rebUnboxInteger()</code> which could push a Level, do an evaluation keeping the Level on the stack, extract the integer, drop the Level, return the integer.)</p>
<p>I'm not thrilled, but I'll try the change and see how much damage/help it does.</p>
<p>ChatGPT thoughts:</p>
<aside class="onebox allowlistedgeneric" data-onebox-src="https://chatgpt.com/share/67c9d949-9fc8-8004-b3e9-fedab3cf0886">
  <header class="source">
      <img src="https://cdn.oaistatic.com/assets/favicon-miwirzcw.ico" class="site-icon" width="48" height="48">

      <a href="https://chatgpt.com/share/67c9d949-9fc8-8004-b3e9-fedab3cf0886" target="_blank" rel="noopener">ChatGPT</a>
  </header>

  <article class="onebox-body">
    <div class="aspect-image" style="--aspect-ratio:690/388;"><img src="https://cdn.oaistatic.com/assets/chatgpt-share-og-u7j5uyao.webp" class="thumbnail" width="690" height="388"></div>

<h3><a href="https://chatgpt.com/share/67c9d949-9fc8-8004-b3e9-fedab3cf0886" target="_blank" rel="noopener">ChatGPT - Stackless Interpreter Design</a></h3>

  <p>Shared via ChatGPT</p>


  </article>

  <div class="onebox-metadata">
    
    
  </div>

  <div style="clear: both"></div>
</aside>

            <p><small>2 posts - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/issues-with-writing-output-cells-directly/2366">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/issues-with-writing-output-cells-directly/2366</link>
          <pubDate>Thu, 06 Mar 2025 17:20:33 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2366</guid>
          <source url="https://rebol.metaeducation.com/t/issues-with-writing-output-cells-directly/2366.rss">Issues With Writing Output Cells Directly</source>
        </item>
        <item>
          <title>The Probably-False Economy of EVAL Consuming FRAME!</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>The earliest design usages for FRAME! were for things like ENCLOSE.</p>
<p>ENCLOSE builds the frame for the function you're enclosing, then passes the frame to a function that can manipulate the frame, and invoke it with EVAL (or not invoke it at all, if it wishes).</p>
<p>Here's a sort of simple historical example:</p>
<pre><code>/foo: func [a b] [
   let result: a + b
   a: b: ~&lt;whatever&gt;~  ; functions can modify args/locals for any reason
   return result
]

/bar: enclose foo/ (lambda [f [frame!]] [
    let b: f.b
    f.a: f.a * 10
    (eval f) + b
])

&gt;&gt; bar 100 10
== 1020  ; ((a * 10) + b) + b
</code></pre>
<p>You'll notice above that I didn't write:</p>
<pre><code>/bar: enclose foo/ (lambda [f [frame!]] [
    f.a: f.a * 10
    (eval f) + f.b
])
</code></pre>
<p>This is because EVAL of a FRAME! would "consume" the frame, and trying to use <strong><code>f.b</code></strong> after the EVAL would raise an error.  In other words, memory was not allocated as it usually would be for a new frame to do the call...but the fields of the frame were the actual values being used.</p>
<p>I overwrote <strong><code>a</code></strong> and <strong><code>b</code></strong> to make the point that once that EVAL is called, you can't rely on any particular state of a frame's variables.  In the general case, they can be changed to anything.</p>
<h2><a name="p-7936-eval-copy-f-is-easy-and-so-is-saving-variables-right-1" class="anchor" href="https://rebol.metaeducation.com#p-7936-eval-copy-f-is-easy-and-so-is-saving-variables-right-1"></a><code>(eval copy f)</code> is Easy, and So Is Saving Variables, Right?</h2>
<p>The premise I was going on was that it seemed like it would be wasteful...especially in scenarios like an ENCLOSE, to make another copy of the frame's data.</p>
<p>And I figured usually you wouldn't need to refer to anything from the frame's input state after you called it.</p>
<p>So you had two choices: either evaluate a copy of the frame, or save any variables you were interested in as locals.</p>
<h2><a name="p-7936-it-turns-out-to-be-incredibly-common-to-save-variables-2" class="anchor" href="https://rebol.metaeducation.com#p-7936-it-turns-out-to-be-incredibly-common-to-save-variables-2"></a>It Turns Out To Be Incredibly Common To Save Variables</h2>
<p>I didn't know when originally trying to optimize the feature how often an ENCLOSE would need to talk about the input fields after an EVAL call.</p>
<p>But empirically I'd say you need the fields at least half the time.  You actually want it more often than that when you consider debugging--you often want to print some information about the input parameters after you've done the EVAL.</p>
<h2><a name="p-7936-let-b-fb-costs-much-more-than-mallocmemcpy-3" class="anchor" href="https://rebol.metaeducation.com#p-7936-let-b-fb-costs-much-more-than-mallocmemcpy-3"></a><code>(let b: f.b)</code> Costs Much More Than <code>malloc()+memcpy()</code></h2>
<p>When you come down to it, relatively speaking: <em>Evaluator cycles are expensive</em>.  <em>Tuple lookup is expensive</em>.  <em>Assignment is expensive</em>.  <em>LET statements are expensive</em>.</p>
<p>That's because this is an interpreted language, and running code in the interpreter involves pushing and popping entities that represent interpreter stack levels.  There's all kinds of C data structures and layers of C function calls as the gears of the machinery turn...whether your operation be simple or complex.  That's just the name of the game... <strong>a + b</strong> in a generalized evaluator is going to be at least 100x more costly than adding two integers in C, which is basically just a single CPU instruction.</p>
<p>So if you have to do <em>any</em> mitigation of losing the frame data by adding interpreted code, not only are you having to junk up what you're writing...but you're also paying much more than you would have if the system had just gone ahead and made a copy.</p>
<h2><a name="p-7936-explaining-why-you-cant-is-lamer-than-it-just-works-4" class="anchor" href="https://rebol.metaeducation.com#p-7936-explaining-why-you-cant-is-lamer-than-it-just-works-4"></a>Explaining Why You Can't Is Lamer Than <em>"It Just Works"</em></h2>
<p>People understand that if they have a FRAME! for APPEND and they EVAL it, that the series is going to be mutated.</p>
<p>But they're going to understand less that the series field of the frame is not available at all to them after the call.</p>
<p>It's kind of a no-brainer to say that if the two approaches were at all comparable in speed or overall performance, that the more useful behavior should be the default.</p>
<p><em>And I actually believe the more useful behavior is faster in the general case...by avoiding additional intepreter cycles to save frame fields in variables.</em></p>
<h2><a name="p-7936-free-f-could-use-free-as-an-intrinsic-if-you-want-5" class="anchor" href="https://rebol.metaeducation.com#p-7936-free-f-could-use-free-as-an-intrinsic-if-you-want-5"></a><code>(free f)</code> Could Use FREE As An Intrinsic If You Want</h2>
<p><strong><a href="https://rebol.metaeducation.com/t/intrinsics-functions-without-frames/2050">FREE could be trivially made Intrinsic</a>.</strong></p>
<p>If profiling suggested that something like an ENCLOSE on a function with a large frame was affecting your bottom line by not freeing the frame, you could just free it after the EVAL.  That would leave behind nothing but a tiny useless stub (to avoid latent references in other cells from crashing the GC), so you'd get the same end result as the historical EVAL.</p>
<p>I'm betting that having EVAL be able to be intrinsic when it takes one argument, and making FREE intrinsic would be faster than trying to do some weird refinement like <strong><code>eval:free</code></strong> to fold both into one operation...because refinement processing has its own cost, which I <em>think</em> would be greater.</p>
<p>But an optimized <strong><code>eval-free</code></strong> might be worth making, I don't know.  However its mechanic would simply be to natively fold the free in after the EVAL, instead of trying to make EVAL take over the frame and use its memory.</p>
<p>My guess is that using EVAL-FREE won't be a benefit most of the time if you add any evaluator cycles to save a variable because of it.</p>
<h1><a name="p-7936-hence-eval-will-no-longer-consume-frame-6" class="anchor" href="https://rebol.metaeducation.com#p-7936-hence-eval-will-no-longer-consume-frame-6"></a>Hence, EVAL Will No Longer Consume FRAME!</h1>
<p>This makes the "action-is-frame" duality even more solid, because as frames are passed around in the system there won't be "consume frame vs. don't" flags involved.</p>
<p>You'll just either free the frame after you've applied it, or you won't.  <img src="https://rebol.metaeducation.com/images/emoji/twitter/man_shrugging.png?v=14" title=":man_shrugging:" class="emoji" alt=":man_shrugging:" loading="lazy" width="20" height="20"></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/the-probably-false-economy-of-eval-consuming-frame/2339">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/the-probably-false-economy-of-eval-consuming-frame/2339</link>
          <pubDate>Sun, 01 Dec 2024 00:50:53 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2339</guid>
          <source url="https://rebol.metaeducation.com/t/the-probably-false-economy-of-eval-consuming-frame/2339.rss">The Probably-False Economy of EVAL Consuming FRAME!</source>
        </item>
        <item>
          <title>A Pattern-Matching Optimized MAP (?)</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>In UPARSE, there's a MAP! of all the combinators.  You can map fundamental datatypes to combinator functions, or you can map literal elements to combinator functions (as keywords like SOME or ACROSS are)... or you can map literal elements to something to substitute as a rule.</p>
<p><em>(e.g. you can map a literal element to a parse rule block!, and it will use that block as the implementation of whatever you mapped.  You could map <strong><code>&lt;_&gt;</code></strong> to <strong><code>[opt some whitespace]</code></strong> if you wanted.)</em></p>
<h2><a name="p-7793-but-now-foobar-foo-and-foo-are-all-becoming-chainhttpsrebolmetaeducationcomtintroducingnew-chain-datatype2226-1" class="anchor" href="https://rebol.metaeducation.com#p-7793-but-now-foobar-foo-and-foo-are-all-becoming-chainhttpsrebolmetaeducationcomtintroducingnew-chain-datatype2226-1"></a>But now <code>FOO:BAR</code>, <code>FOO:</code> and <code>:(FOO)</code> are all becoming <a href="https://rebol.metaeducation.com/t/introducingnew-chain-datatype/2226">CHAIN!</a></h2>
<p>It's likely that <strong><code>foo:bar</code></strong> would be handled by the combinator mechanics themselves, to provide refinements to the combinator.  So it wouldn't be dispatched to the "CHAIN! combinator".</p>
<p>But we've been handling <strong><code>foo:</code></strong> and <strong><code>:(foo)</code></strong> very differently.  But they're both the same type: CHAIN!.</p>
<p>Problem is, there are no variadic combinators (yet?).  So you either map the CHAIN! to a combinator that takes an argument or you don't.  The <strong><code>foo:</code></strong> needs an argument, the <strong><code>:(foo)</code></strong> has been evaluating the group and splicing it as a rule to execute.</p>
<h2><a name="p-7793-cheap-solution-of-the-moment-hack-the-dispatch-2" class="anchor" href="https://rebol.metaeducation.com#p-7793-cheap-solution-of-the-moment-hack-the-dispatch-2"></a>Cheap Solution of the Moment: Hack The Dispatch</h2>
<p>The terrible but "let's keep things moving" answer is that if you have any sequence with a leading blank, it will try dispatching to <strong><code>:*</code></strong> or <strong><code>/*</code></strong> or <strong><code>.*</code></strong> ... e.g. those literal sequence patterns are what you put in the combinator table.</p>
<p>And if it has a trailing blank, you put <strong><code>*:</code></strong> or <strong><code>*/</code></strong> or <strong><code>*.</code></strong> and give those a combinator.</p>
<p>If both of those fall through, it goes to the plain CHAIN! or PATH! or TUPLE! combinator.</p>
<p>Shoddy...of course.  How to do better?</p>
<h2><a name="p-7793-could-a-map-like-structure-optimize-pattern-match-3" class="anchor" href="https://rebol.metaeducation.com#p-7793-could-a-map-like-structure-optimize-pattern-match-3"></a>Could A MAP!-Like Structure Optimize Pattern Match?</h2>
<p>As a non-compiled language, we can't do much to optimize <a href="https://rebol.metaeducation.com/t/destructure-dialect/1877">something like DESTRUCTURE</a> if the moment we encounter it is the first time we've seen it.</p>
<p>We <em>could</em> have a data structure that is internally optimized to pattern matching.  e.g. something like a MAP! but that has architecture inside of itself as a data structure that groups patterns that are close to each other together, and branches off at the appropriate points.</p>
<p>I don't know how to implement it, but I do know that if we turned the combinator map into a list of destructure rules that it tested one at a time in order, that that would be really slow... even if destructure were native.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/a-pattern-matching-optimized-map/2302">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/a-pattern-matching-optimized-map/2302</link>
          <pubDate>Sat, 28 Sep 2024 23:35:31 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2302</guid>
          <source url="https://rebol.metaeducation.com/t/a-pattern-matching-optimized-map/2302.rss">A Pattern-Matching Optimized MAP (?)</source>
        </item>
        <item>
          <title>Speed of UPARSE</title>
          <dc:creator><![CDATA[bradrn]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I’m just wondering if any benchmarking has been done on UPARSE. How does it compare to PARSE in Red or in Rebol? Or to parser combinators in Haskell? For that matter, how does it compare to an ordinary recursive-descent parser handwritten in Ren-C?</p>
<p>(Yes, I know UPARSE is unoptimised and slow. But it would be interesting to know <em>how</em> slow.)</p>
            <p><small>6 posts - 2 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/speed-of-uparse/2177">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/speed-of-uparse/2177</link>
          <pubDate>Fri, 29 Mar 2024 06:47:47 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2177</guid>
          <source url="https://rebol.metaeducation.com/t/speed-of-uparse/2177.rss">Speed of UPARSE</source>
        </item>
        <item>
          <title>Mapping from Series =&gt; Series By Co-Opting The Key Series</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>There was an unfinished idea in an old version of the interpreter.  It related to how to deal with problems like trying to make a copy of a block, and make sure any series with the same identity are only copied once in the new structure, and point to that one copied identity.</p>
<p>Rebol2 did not have this behavior:</p>
<pre><code>rebol2&gt;&gt; block: [a]
== [a]

rebol2&gt;&gt; original: reduce [block block]
== [[a] [a]]

rebol2&gt;&gt; append block 'b
== [a b]

rebol2&gt;&gt; original
== [[a b] [a b]]  ; both aliases see the append

rebol2&gt;&gt; duplicate: copy/deep original
== [[a b] [a b]]

rebol2&gt;&gt; append first duplicate 'c
== [a b c]

rebol2&gt;&gt; duplicate
== [[a b c] [a b]]  ; considered by many to be wrong: independent copies
</code></pre>
<p>This post isn't about whether that is right or wrong (and having such questions may seem to some as an indication of <em>"this language is madness! get me to Haskell"</em>, etc. But as I've said this is the game we're playing here so we roll with it.)</p>
<p>But to not get independent copies, you need a way to map series nodes to copies you've already created...so you can consult that mapping before making new copies.  And the direction that was being pursued by the old interpreter I am looking at was to actually do surgery on the originating series nodes, to alter them so they shifted out some of their content, such that they could be their own keys in the mapping.</p>
<p>Generally speaking, all the bits in a series stub are spoken for.  So it would seem there's nowhere to stow a pointer to the new series you are creating in it.  What the implementation was doing was pushing a 4 pointer cell on the data stack, writing one pointer's worth of information from the stub into that cell, then replacing that pointer slot in the stub with the stack index.  Then it wrote the new series into the cell...so the cell contained one stowed pointer from the original series and one pointer for the new series.</p>
<p>This meant the original series was now in a "weird" state, that things like the GC had to know about and tolerate.  Other operations looking for the missing information in the stub needed to be caught if they tried to get at it without following the stack index through to the stack cell.</p>
<p>Having the cells on the data stack meant it was not necessary to enumerate all the series stubs after a copy to "clean them up".  Otherwise, I'd imagine it may be possible to make some kind of guarantee that for any series appearing in source, the union of the bits in the source series and the bits of the copied series can hold all the information necessary to construct two valid series... e.g. one pointer's worth of information is always redundant in those two copies.  If you can get <em>two</em> pointers' worth of information redundant, the second could be used to chain a linked list as you go...removing the need for the stack cells to enumerate.</p>
<p>Though having the stack cells and no particular requirement of information redundancy in source series with their copies offers another benefit: being applicable for creating mappings that aren't copying-related.</p>
<p>Anyway, it was a little unfinished idea I ran across that I wanted to document.  I'm cleaning up the bootstrap executable to refresh it with something that will help <a href="https://rebol.metaeducation.com/t/rethinking-braces-as-an-array-type/1727">in the FENCE! migration</a>, and mercilessly deleting any code in the bootstrap executable that does not specifically benefit bootstrap... to reduce the instability surface, speed things up, and make it easier to debug the 6-year old executable if worst comes to worst.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/mapping-from-series-series-by-co-opting-the-key-series/2166">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/mapping-from-series-series-by-co-opting-the-key-series/2166</link>
          <pubDate>Wed, 06 Mar 2024 15:24:07 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2166</guid>
          <source url="https://rebol.metaeducation.com/t/mapping-from-series-series-by-co-opting-the-key-series/2166.rss">Mapping from Series =&gt; Series By Co-Opting The Key Series</source>
        </item>
        <item>
          <title>Optimizing Environment Lookup</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <h2><a name="p-7029-object-storage-object-frame-port-error-1" class="anchor" href="https://rebol.metaeducation.com#p-7029-object-storage-object-frame-port-error-1"></a>Object Storage (OBJECT!, FRAME!, PORT!, ERROR!)</h2>
<p>Rebol objects were designed as two parallel arrays, which we can call the "keylist" and the "varlist".  Originally these were entire cells, like this:</p>
<pre><code>obj: make object! [a: 10 b: 20]

            0     1     2
KEYLIST  [     |  a  |  b  ]   ; 4 platform pointers per cell

            0     1     2
VARLIST  [self |  10 |  20 ]   ; 4 platform pointers per cell
</code></pre>
<p>The idea is that the <code>[0]</code>th cell of the varlist contains an instance of the object itself.  This means if the implementation has a pointer to the varlist in its hand, it also has a cell instance of the object.  This also means you can find out from just the varlist what subtype it is (ERROR!, FRAME!, PORT!, etc.)</p>
<p>R3-Alpha used full 4-platform-pointer-sized WORD! cells for each element in the keylist, and left the [0]th cell empty.</p>
<p>Ren-C optimized this to just point to symbols.  So keylists are arrays of single pointers, and there is no [0]th element.</p>
<pre><code>                  0     1 
KEYLIST        [  a  |  b  ]   ; 1 platform pointer per cell

            0     1     2
VARLIST  [self |  10 |  20 ]   ; 4 platform pointers per cell
</code></pre>
<p>Keylists are shared among objects that are used as prototypes for each other, e.g. <strong>obj2: make obj [...]</strong>.  They will become un-shared if any of the objects are expanded.</p>
<p>(Object expansion is allowed in R3-Alpha and Ren-C, but not Rebol2 or Red).</p>
<h2><a name="p-7029-module-storage-module-2" class="anchor" href="https://rebol.metaeducation.com#p-7029-module-storage-module-2"></a>Module Storage (MODULE!)</h2>
<p>R3-Alpha used the same layout for modules containing hundreds of items as it did for objects.</p>
<p>Ren-C instead allocates small variable "stubs" for each variable in a module.  Each stub is 8 platform pointers in size.</p>
<ul>
<li>4 of those platform pointers are for the cell of the variable's value</li>
<li>1 pointer is for the symbol of the variable</li>
<li>1 pointer is to the module the variable is for</li>
<li>1 pointer to the next stub with the same symbol for another module</li>
<li>1 pointer-sized slot unused at this time</li>
</ul>
<p>These form a linked list of all the same-named variable instances in modules.  This list is pointed to by the symbol itself.</p>
<p>If we want to check if a WORD! cell has a variable in a module, the cell contains a pointer to the word's symbol.  We follow that, and get to the list of variables.  We can walk that list and see if there is an instance matching the module we are looking for.</p>
<h2><a name="p-7029-let-variables-3" class="anchor" href="https://rebol.metaeducation.com#p-7029-let-variables-3"></a>LET Variables</h2>
<p>At the moment, LET variables are similar to the stubs holding variables for a module... except they don't have an associated module.</p>
<h2><a name="p-7029-specifier-chains-are-linked-lists-of-contexts-or-containers-4" class="anchor" href="https://rebol.metaeducation.com#p-7029-specifier-chains-are-linked-lists-of-contexts-or-containers-4"></a>Specifier Chains Are Linked Lists Of Contexts -or- Containers</h2>
<p>Things like FRAME! or OBJECT! or MODULE! have one pointer for their "parent specifier".  So when you do something like:</p>
<pre><code> let x: 10
 obj: make object! [y: x + 10, z: x + 20]
</code></pre>
<p>The BLOCK! that object receives has a specifier put onto it... in this case, it will be a LET variable.  That LET variable presumably points up to something else (an enclosing function frame, or a module, or whatever).</p>
<p>The object creates its varlist, and then that varlist has a pointer to the LET.  It uses this as the edited specifier when running the body block of the object.</p>
<p>But if you later try to leverage that object elsewhere e.g. with <strong>overbind obj [...]</strong>, it wants to chain that object onto some other specifier.  However its parent link is already in use for the other chain.  So this means a little stub USE container is needed... which points at the object and provides a new slot to put a pointer in.</p>
<h2><a name="p-7029-looking-up-an-unbound-word-walks-this-chain-5" class="anchor" href="https://rebol.metaeducation.com#p-7029-looking-up-an-unbound-word-walks-this-chain-5"></a>Looking Up An Unbound Word Walks This Chain</h2>
<aside class="quote no-group quote-modified" data-username="bradrn" data-post="12" data-topic="1751">
<div class="title">
<div class="quote-controls"></div>
<img alt="" width="24" height="24" src="https://rebol.metaeducation.com/user_avatar/rebol.metaeducation.com/bradrn/48/365_2.png" class="avatar"><a href="https://rebol.metaeducation.com/t/rebol-and-scopes-well-why-not/1751/12">Rebol And Scopes: Well, Why Not?</a></div>
<blockquote>
<p>Though I do spy one low-hanging fruit…</p>
<aside class="quote no-group" data-username="hostilefork" data-post="11" data-topic="1751">
<div class="title">
<div class="quote-controls"></div>
<img alt="" width="24" height="24" src="https://rebol.metaeducation.com/user_avatar/rebol.metaeducation.com/hostilefork/48/421_2.png" class="avatar"><a href="https://rebol.metaeducation.com/t/rebol-and-scopes-well-why-not/1751/11">Rebol And Scopes: Well, Why Not?</a></div>
<blockquote>
<p>a linked list of objects</p>
</blockquote>
</aside>
<p>Might it not be quicker to use a hashmap or similar data structure?</p>
</blockquote>
</aside>
<p>It's not entirely obvious what to hash, here.  And it's not so much that any particular lookup is all that slow.  It's just that there's lots of them, and you can't reliably cache the answer between runs.</p>
<p><a href="https://rebol.metaeducation.com/t/semantics-and-optimization-of-copying-function-bodies/2119/2">One thing I cited to exploit</a> was the fact that when you copy a function body, you tend to wind up with elements that look up either in a module or in the frame of that function.</p>
<ul>
<li>
<p>Module lookup is relatively fast because there aren't all that many redundant names (e.g. there's typically only one APPEND and it's in LIB.)</p>
</li>
<li>
<p>Function frames are not allowed to expand.</p>
</li>
<li>
<p>You can use the space in the unbound elements to give an answer to something knowable--like "this isn't defined in the frame for function X" or "this is defined in the frame for function X at offset Y", that can let you skip along to searching in the module or beeline for the pointer to what you want in the frame.</p>
</li>
</ul>
<p>I'm sure this will help.  Will have to see how much.</p>
<h2><a name="p-7029-gc-load-is-a-big-problem-6" class="anchor" href="https://rebol.metaeducation.com#p-7029-gc-load-is-a-big-problem-6"></a>GC Load Is A Big Problem</h2>
<p>Ren-C's garbage collector has some interesting points, but it's still a mark-and-sweep strategy.</p>
<p>These specifier chains are being allowed to leak out, with every function call producing tons of them... and function frames have to be GC'd because you can't assume there are no extant references.  (Natives are an exception, they will free their frames when they end, but you can't do that with usermode functions because they use frames as specifiers in the blocks they run... and you don't know what happens to that block).</p>
<p>LETs are pretty bad too... a LET inside a loop creates a little piece of junk each time that needs to get cleaned up.</p>
<p>I think reference counting would be helpful, because most of these aren't referenced very long and aren't involved in cycles.  So reaching a 0 refcount would be a moment the memory could be reclaimed.  My guess is it would outweigh the cost of the reference counting by a fair bit.  But it's difficult to do reference counting correctly in C-like code (although having a C++ build variant it could be double-checked).</p>
            <p><small>4 posts - 2 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/optimizing-environment-lookup/2134">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/optimizing-environment-lookup/2134</link>
          <pubDate>Thu, 25 Jan 2024 03:27:25 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2134</guid>
          <source url="https://rebol.metaeducation.com/t/optimizing-environment-lookup/2134.rss">Optimizing Environment Lookup</source>
        </item>
        <item>
          <title>Semantics and Optimization of Copying Function Bodies</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Part of Rebol's design is that every execution frame has an order to its arguments, represented by an integer.</p>
<p>So for instance with IF:</p>
<ul>
<li>the CONDITION would be in frame <code>cell[1]</code></li>
<li>the BRANCH would be in frame <code>cell[2]</code></li>
</ul>
<h2><a name="p-6984-native-functions-use-the-ordering-1" class="anchor" href="https://rebol.metaeducation.com#p-6984-native-functions-use-the-ordering-1"></a>NATIVE Functions Use The Ordering</h2>
<p>Natives don't have to walk the keys of a frame, e.g. to find a slot that matches the symbol "BRANCH".  They are hardcoded to look directly at the index it should be in.</p>
<p><em>(R3-Alpha hardcoded these indices, and you can see that as <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/n-control.c#L620">D_ARG(1) and D_ARG(2)</a> in the implementation of IF.  Ren-C puts the native specs in comments, processed during the build to make macros.  These macros like <a href="https://github.com/metaeducation/ren-c/blob/0d2d7c39733f21e1088240ab2e5c7614072c8059/src/core/n-control.c#L151">ARG(condition) and ARG(branch)</a> resolve to the integers at compile-time.)</em></p>
<h2><a name="p-6984-interpreted-func-functions-use-indices-too-but-differently-2" class="anchor" href="https://rebol.metaeducation.com#p-6984-interpreted-func-functions-use-indices-too-but-differently-2"></a>Interpreted FUNC Functions Use Indices Too, But Differently</h2>
<p>When a FUNC is being generated, a table is made that maps from symbols for the arguments and locals to the integer for that symbol in the frame.</p>
<p>Then the BLOCK! passed as a body is walked through to find ANY-WORD!s that reference the arguments and locals, and the index is poked into the cells for those words.  Then, the binding pointer in the cell is set to point at the function definition.</p>
<p>That isn't enough information to look up the word--because the binding is relative to a function that is not running.  Hence it's called <strong>relative binding</strong>.  You need to somehow connect this to an instance of the function that is running and has its argument and local cells filled.</p>
<p>Historical Rebol &amp; Red make this connection dynamically by climbing the stack and looking for an instance of the function seen in the binding.  Clearly that doesn't permit indefinite extent (closure) over a local or argument, because when the function isn't running its variables can no longer be found.  But it also can be wrong if recursions interact with material bound to various instances of the same function.</p>
<p>Ren-C uses a better mechanism of <a href="https://rebol.metaeducation.com/t/relative-binding-and-frame-internals/1344">Specific Binding</a>, where information is slipped through instances of BLOCK!s and other arrays, saying which frame that view of the array is executing on behalf of.</p>
<h2><a name="p-6984-conflict-with-new-policy-of-leave-binding-alone-3" class="anchor" href="https://rebol.metaeducation.com#p-6984-conflict-with-new-policy-of-leave-binding-alone-3"></a>Conflict With New Policy of "Leave Binding Alone"</h2>
<p>The bias in the new proposal of binding is that any WORD!s already bound will be left-as is, with only some minor surgery on environments of blocks done by things like FUNC or FOR-EACH, to inject some new variables at the head of the lookup.</p>
<p>That may make it sound like these words which are bound relatively inside function bodies are more trouble than they are worth.  In the model, they're actually supposed to be thought of as unbound--so if they carry a secret binding, that could only be used as an optimization when they are actually intentionally combined with the right frame.</p>
<h2><a name="p-6984-the-optimization-is-actually-not-minor-4" class="anchor" href="https://rebol.metaeducation.com#p-6984-the-optimization-is-actually-not-minor-4"></a>The Optimization is Actually Not Minor</h2>
<p>Ren-C function frames are unusual, in that a single frame can be reused for several "phases" of a function composition.  e.g. you can ADAPT and SPECIALIZE and AUGMENT a function--even a native--and all these just reuse the same frame.  But during the phase, it only sees the fields of the frame it is supposed to be able to see.</p>
<p>For instance, if you specialize out the VALUE from append (e.g. by making it always append 5) and then try to ADAPT the resulting function, you won't even know that APPEND has a VALUE at all.  It will seem like it only has a series.</p>
<p>Being strict about enforcing this information hiding permits you to do unusual things, like once the VALUE is shielded in the inner function... you can AUGMENT the resulting functions with another argument named VALUE.  This process can proceed indefinitely.  There could be dozens of fields named VALUE in the frame, but only one visible at a time.</p>
<p>So checking whether a value is visible in the frame is more involved than just walking a list of symbols and comparing them (which isn't necessarily fast in and of itself).  The parameter definitions must be consulted also, to see if they line up with the running phase of the function in order to determine the visibility.</p>
<p>The time of function creation is a good moment to do this work, instead of going through it on every lookup.  And squeaking performance out of pure virtual binding is going to be hard... we need all the tricks we can get.</p>
<h2><a name="p-6984-copying-the-function-body-5" class="anchor" href="https://rebol.metaeducation.com#p-6984-copying-the-function-body-5"></a>Copying The Function Body</h2>
<p>R3-Alpha actually had a trick during bootstrap, where while loading the library functions it would use a special version of FUNC that did not copy the function body blocks.  It assumed none of them were composed out of parts that were exposed or would change, and if there were exceptions the library authors were supposed to be sophisticated enough to COPY or COPY/DEEP where needed.</p>
<p>(IIRC, it would contaminate these blocks by putting the relative binding information in them, so they would be seen as corrupt if anyone managed to get access to them.)</p>
<p>But after bootstrap was over, it would replace FUNC with a version that deeply copied the bodies.</p>
<p>It invites a lot of accidents in the historical world to have a FUNC that doesn't copy its body deeply.  But there are some new possibilities that might be able to avoid accidents while still covering a lot of cases.  For instance, it could deeply protect the blocks and make them immutable.  This way, if a user ever found themselves bit by it they could add in their own COPY or COPY/DEEP at the relevant places.  But 95% or more of the time, you'd not need to and the system could speed up.</p>
<p>In any case, it's interesting that the relative binding information wouldn't be corrupting the binding information any longer, because it's just an optimization for unbound values... and any PICKs or FOR-EACHs would see relatively bound words as unbound.</p>
            <p><small>4 posts - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/semantics-and-optimization-of-copying-function-bodies/2119">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/semantics-and-optimization-of-copying-function-bodies/2119</link>
          <pubDate>Tue, 16 Jan 2024 09:00:09 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2119</guid>
          <source url="https://rebol.metaeducation.com/t/semantics-and-optimization-of-copying-function-bodies/2119.rss">Semantics and Optimization of Copying Function Bodies</source>
        </item>
        <item>
          <title>Performance Implications of Antiform-FRAME!-is-Action</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Making the antiform state of frames be <a href="https://rebol.metaeducation.com/t/taking-action-on-function-vs-action/596/6"><em>"interpret this frame as an action when referenced through a WORD!"</em></a> is a <em>deep</em> change.  The implications haven't been fully absorbed, and there are some things I noticed that create troubles for optimization.</p>
<p>Looking again at a simple example:</p>
<pre><code>&gt;&gt; f: make frame! :append
== make frame! [
    series: ~
    value: ~
    part: ~
    dup: ~
    line: ~
]

&gt;&gt; f.value: 5

&gt;&gt; append5: anti f
== ~make frame! [
    series: ~
    value: 5
    part: ~
    dup: ~
    line: ~
]~  ; anti

&gt;&gt; append5 [a b c]
== [a b c 5]
</code></pre>
<p>That's cool, for sure.  And it removes the number of things we have to name in the system (I was particularly annoyed by having to differentiate "action" and "activation" when there was a separate ACTION! datatype).</p>
<h2><a name="p-6771-but-what-if-the-frame-is-modified-1" class="anchor" href="https://rebol.metaeducation.com#p-6771-but-what-if-the-frame-is-modified-1"></a>But What If The Frame Is Modified...?</h2>
<p>Say the user writes this:</p>
<pre><code>&gt;&gt; append5 (f.series: [d e f], [a b c])
== ???
</code></pre>
<p>When running the specialization <strong>append5</strong> we'd already decided that the series parameter wasn't specialized and needed to be fulfilled.  But the user is <em>specializing the series parameter during the fulfillment</em>.</p>
<p>At best, this is semantically messy (and can manifest as hard-to-comprehend behavior when the example is less obvious than this one).  At worst, the internal bookkeeping of the evaluator gets confused and it crashes due to having the slot it's filling changed out from under it.</p>
<h2><a name="p-6771-we-could-snapshot-the-frame-but-snapshots-arent-free-2" class="anchor" href="https://rebol.metaeducation.com#p-6771-we-could-snapshot-the-frame-but-snapshots-arent-free-2"></a>We Could Snapshot The Frame... But Snapshots Aren't Free</h2>
<p>If we are forced to make a snapshot of the frame state at the start of execution, then that means making a copy, which takes time and space.</p>
<p>The space isn't actually the problem...because we can just put the snapshot in the frame space we're already making for the function call.</p>
<p>Where we pay is that it effectively adds an extra traversal of the arguments.  We're traversing the space to make the copy (with some slots unspecialized and needing to be fulfilled).  Then we're traversing the space again to do the fulfillment of the unspecialized arguments.</p>
<p>With no snapshot, we could leave the memory for the frame cells completely garbage at the outset... and fill them as we go with either a fulfillment or a specialization.  <strong>Empirically, avoiding the snapshot could save as much as 5% of the average total runtime of the interpreter.</strong></p>
<h2><a name="p-6771-another-issue-no-moment-to-cache-optimizations-3" class="anchor" href="https://rebol.metaeducation.com#p-6771-another-issue-no-moment-to-cache-optimizations-3"></a>Another Issue: No Moment To Cache Optimizations</h2>
<p>When there was a separate "make an action! out of this frame" step, the action was a way of saying "freeze!" on the arguments, so they could no longer be changed.  So it avoids the problem of changes during fulfillment.</p>
<p>But it did something else: it meant properties of the arguments could be studied...to remember things like <em>"what's the first unspecialized argument slot"</em>.</p>
<p>(It might seem that it would be easy to find the first unspecialized argument slot.  But this isn't just a search from the beginning of the frame, because it's possible to reorder arguments or have partial specializations.  So it's tricky.)</p>
<h2><a name="p-6771-how-about-just-freezing-the-frames-if-theyre-executed-4" class="anchor" href="https://rebol.metaeducation.com#p-6771-how-about-just-freezing-the-frames-if-theyre-executed-4"></a>How About Just Freezing The Frames If They're Executed?</h2>
<p>The simplest idea here is just to say that once you invoke a frame, it's frozen...you can't change its fields anymore.  Then that freezing process can do the caching of properties that accelerate action execution, and all is well.</p>
<p>If you want to keep a frame mutable, then COPY it before making it an antiform.  The issue is just that changes to the frame won't be seen by the isotope you made.</p>
<h2><a name="p-6771-do-technicalities-like-this-make-me-question-frame-as-action-5" class="anchor" href="https://rebol.metaeducation.com#p-6771-do-technicalities-like-this-make-me-question-frame-as-action-5"></a>Do Technicalities Like This Make Me Question FRAME!-as-Action?</h2>
<p>Well, it's a delicate balance of choices, and I'm still feeling it out.</p>
<p>The worry is that the number of concerns being exposed to users is such that the properties of "the kind frame you can run is different enough that you basically have to think of it as a different type".</p>
<p>To avoid that in this case, it's better to do the freezing implicitly vs. saying that you need a special routine like RUNS to bless a frame as runnable.  This way, RUNS can be understood as simply taking in a plain frame! and giving back an isotopic frame!... something you could easily write yourself.</p>
<p>Crafting a uniformity of experience with FRAME! so it doesn't feel like it's making up for a missing ACTION! type is certainly something to continue to be mindful of.  But performance needs to be minded too at some point, so I'm doing what I can about that.</p>
            <p><small>5 posts - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/performance-implications-of-antiform-frame-is-action/2083">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/performance-implications-of-antiform-frame-is-action/2083</link>
          <pubDate>Wed, 20 Dec 2023 02:09:21 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2083</guid>
          <source url="https://rebol.metaeducation.com/t/performance-implications-of-antiform-frame-is-action/2083.rss">Performance Implications of Antiform-FRAME!-is-Action</source>
        </item>
        <item>
          <title>Executable Size circa 2023...and tweaking INLINE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>A modern Ren-C non-debug executable on Linux--with https and the libuv filesystem and networking code (which supports asynchronous file I/O etc.) is about 1.7 MB when it is built at an O2 level of optimization (optimize for speed).</p>
<p>When built at Os optimization it's about 1.2 MB, sacrificing 40% of the speed to get the compression.  (In the modern era, most people would say that the extra size isn't a big deal to get that much of a speed improvement.)</p>
<p>By comparison, an R3-Alpha Linux executable is about 0.56 MB at O2.  And a Red CLI-only binary on Linux is about 1.0 MB.</p>
<h2><a name="p-6728-why-has-size-gone-up-1" class="anchor" href="https://rebol.metaeducation.com#p-6728-why-has-size-gone-up-1"></a>Why Has Size Gone Up?</h2>
<p>I've looked under the hood at the differences with R3-Alpha to see what accounts for the disparity with modern Ren-C.  libuv accounts for a couple 100k, and is worth it--it would be especially so if taking advantage of things like the async file I/O.</p>
<p>But the rest just generally comes down to the fact that it's about twice as much code.  If you enjoy using ADAPT or ENCLOSE or SPECIALIZE, well, there's code that implements it.  And it's a deeper, safer, far more advanced codebase that just does more.</p>
<h2><a name="p-6728-i-actually-pared-out-about-600k-by-tweaking-inlining-2" class="anchor" href="https://rebol.metaeducation.com#p-6728-i-actually-pared-out-about-600k-by-tweaking-inlining-2"></a>I Actually Pared Out About 600K By Tweaking Inlining</h2>
<p>When I started looking at size, the O2 binary was like 2.4 MB.  That was more than I expected, so I decided to look under the hood into why.</p>
<p>I used Google's tool <a href="https://github.com/google/bloaty">Bloaty McBloatface</a> to get some insight, and to my surprise...some rather small functions had a disproportionate amount of code attributed to them.</p>
<p>It turned out that this was due to putting functions in header files and inlining them with <code>static inline</code>.  When I moved 5 of these functions into the .c files instead of the .h files, that saved 400k in one blow... and the executable only got 0.4% slower (four tenths of a percent) as a result.</p>
<p>Then I managed to make it so the C++ build was about 140K lighter by changing the <code>static inline</code> on the remaining functions to a macro of INLINE that's either <code>inline</code> in the C++ build, or <code>static inline</code> in the C build.</p>
<p>I guess the takeaway here is that even if you notice that something is getting bigger due to good reasons of having more code, it always pays to look under the hood a bit when you can.  A few hours of work can get some low-hanging fruit.</p>
<p>(Another takeaway is that being able to build a C codebase as C++--if you want to--continuously pays dividends...)</p>
<p>Here's some notes on the INLINE macro:</p>
<pre data-code-wrap="plaintext"><code class="lang-plaintext">
//=//// INLINE MACRO FOR LEVERAGING C++ OPTIMIZATIONS /////////////////////=//
//
// "inline" has a long history in C/C++ of being different on different
// compilers, and took a long time to get into the standard.  Once it was in
// the standard it essentially didn't mean anything in particular about
// inlining--just "this function is legal to appear in a header file and be
// included in multiple source files without generating conflicts."  The
// compiler makes no particular promises about actually inlining the code.
//
// R3-Alpha had few inline functions, but mostly used macros--in unsafe ways
// (repeating arguments, risking double evaluations, lacking typechecking.)
// Ren-C reworked the code to use inline functions fairly liberally, even
// putting fairly large functions in header files to give the compiler the
// opportunity to not need to push or pop registers to make a call.
//
// However, GCC in C99 mode requires you to say `static inline` or else you'll
// get errors at link time.  This means that every translation unit has its
// own copy of the code.  A study of the pathology of putting larger functions
// in headers as inline with `static inline` on them found that about five
// functions were getting inlined often enough to add 400K to the executable.
// Moving them out of .h files and into .c files dropped that size, and was
// only about *0.4%* slower (!) making it an obvious win to un-inline them.
//
// This led to experimentation with C++ builds just using `inline`, which
// saved a not-insignificant 8% of space in an -O2 build, as well as being ever
// so slightly faster.  Even if link-time-optimization was used, it still
// saved 3% on space.
//
// The long story short here is that plain `inline` is better if you can use
// it, but you can't use it in gcc in C99 mode (and probably not other places
// like TinyC compiler or variants). So this clunky INLINE macro actually
// isn't some pre-standards anachronism...it has concrete benefits.
//
#if CPLUSPLUS_11
    #define INLINE inline
#else
    #define INLINE static inline
#endif
</code></pre>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/executable-size-circa-2023-and-tweaking-inline/2061">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/executable-size-circa-2023-and-tweaking-inline/2061</link>
          <pubDate>Tue, 21 Nov 2023 05:02:05 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2061</guid>
          <source url="https://rebol.metaeducation.com/t/executable-size-circa-2023-and-tweaking-inline/2061.rss">Executable Size circa 2023...and tweaking INLINE</source>
        </item>
        <item>
          <title>Intrinsics: Functions without Frames</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Redbol's historical type system really had only one design point: <em>be fast</em>.  There were 64 fundamental datatypes, and parameters of a function could either accept each datatype or not.  So a simple bitset of 64 bits was stored alongside each parameter, and checked when the function was called.  That was it.</p>
<p>Ren-C's richer design explodes the number of "types" in the system.  Not only are there more fundamental types, but antiform isotopes like <strong><code>~null~</code></strong> are variations on WORD!, but you don't want every function that takes a WORD! to take nulls...and you don't want to have the type checking be so broad as to take <strong><code>[antiform!]</code></strong> just because you want to be able to take nulls (because that would include splices, packs, etc.)</p>
<p>It's not just this reason that Redbol's type checking was too simple, but it forced my hand in coming up with some sort of answer.  <em>I couldn't think of any better idea than Lisp, which does type checking via functions ("predicates").</em>  So I rigged it up where if you want to say a function can take an integer or null, you can write <strong><code>[null? integer!]</code></strong>  You can freely mix LOGIC-returning functions with fundamental types, and we're no longer stuck with the 64 fundamental type limit.</p>
<h2><a name="p-6680-isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1" class="anchor" href="https://rebol.metaeducation.com#p-6680-isnt-it-slow-to-call-a-list-of-functions-for-typechecking-1"></a>Isn't It Slow To Call A List of Functions For Typechecking?</h2>
<p>It can be.  And in particular, it can be if you have to go through calling those functions twice.</p>
<p>Why twice?  Because of "coercion".  For example, if you pass a pack to a function that expects packs, you'll get the meta-pack:</p>
<pre><code>&gt;&gt; foo: func [^x [pack?]] [probe x]

&gt;&gt; foo pack [1 "hi"]
~['1 '"hi"]~
</code></pre>
<p>But if your function didn't want packs, but wanted the type the pack decays to, it has to work for that as well:</p>
<pre><code>&gt;&gt; bar: func [^x [integer?]] [probe x]

&gt;&gt; bar pack [1 "hi"]
'1 
</code></pre>
<p><em>Did the function want the meta form or the meta-decayed form?</em>  There's no way of knowing for sure in advance.  The method chosen is to offer the meta form first, and if that doesn't match then the decayed form is offered.</p>
<p>It didn't know before walking through the block of functions to typecheck that a pack wouldn't have been accepted.  So it had to go through offering the pack, and then offering the integer.</p>
<h2><a name="p-6680-but-i-noticed-something-about-these-functions-2" class="anchor" href="https://rebol.metaeducation.com#p-6680-but-i-noticed-something-about-these-functions-2"></a>But I Noticed Something About These Functions...</h2>
<p>Typically these functions are very simple:</p>
<ul>
<li>
<p>They take one argument.</p>
</li>
<li>
<p>They can't fail.</p>
</li>
<li>
<p>They don't require recursive invocations of the evaluator.</p>
</li>
</ul>
<p>This led me to wonder how hard it would be to define a class of actions whose implementations were a simple C function with an input value and output value.  If you weren't in a scenario where you needed a full FRAME!, you could reach into the ACTION's definition and grab the simple C function out of it.  All these functions would use the same dispatcher--that would be a simple matter of proxying the first argument of a built frame to pass it to this C function.</p>
<p>I decided to call these <strong>"intrinsics"</strong>, which is named after a <a href="https://en.wikipedia.org/wiki/Intrinsic_function">trick compilers use</a> when they see certain function calls that they implement those functions via direct code inlining.  It's not a perfect analogy, but it's similar in spirit.</p>
<h2><a name="p-6680-it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3" class="anchor" href="https://rebol.metaeducation.com#p-6680-it-wasnt-all-that-hard-to-implement-relatively-speaking-roll_eyes-3"></a>It Wasn't All That Hard To Implement (relatively speaking <img src="https://rebol.metaeducation.com/images/emoji/twitter/roll_eyes.png?v=14" title=":roll_eyes:" class="emoji" alt=":roll_eyes:" loading="lazy" width="20" height="20"> )</h2>
<p>All of the native function implementations were assumed to have the same type signature, taking a frame as an argument.  I took away that assumption and added an /INTRINSIC refinement to the NATIVE function generator.  If it was an intrinsic, then the C function in the native table would take a single value argument and an output slot to write to.</p>
<p>So it's still one C function per native.  But if it's an intrinsic, then the function is not a dispatcher... the Intrinsic_Dispatcher() is used, and the C function is poked into the properties of the function.</p>
<p>Callsites that want to optimize for intrinsics just look to see if an action has the Intrinsic_Dispatcher(), and if so they have to take responsibility for procuring an argument and type checking it.  But if they do, they can just call the C function directly with no frame overhead.</p>
<p><strong>This helps make the switchover to functions in type spec blocks much more palatable.</strong>  It's never going to be as fast as the bitset checking, but it's fast enough to allow things to make progress.</p>
            <p><small>2 posts - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/intrinsics-functions-without-frames/2050">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/intrinsics-functions-without-frames/2050</link>
          <pubDate>Sun, 15 Oct 2023 17:25:32 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2050</guid>
          <source url="https://rebol.metaeducation.com/t/intrinsics-functions-without-frames/2050.rss">Intrinsics: Functions without Frames</source>
        </item>
        <item>
          <title>Is The Script Compression Feature Necessary?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>R3-Alpha introduced the option that when you SAVE a script, you can ask that it be compressed.</p>
<p>It doesn't compress the header...just the body of the script.  There were two options for how this body could be compressed after the header: either as a Base64 BINARY literal ("script compression"), or directly as gzip'd bits ("raw compression").</p>
<p>As an example:</p>
<pre><code>&gt;&gt; data: save/compress blank [1 &lt;two&gt; "three"] 'script
== #{
    5245424F4C205B0A202020204F7074696F6E733A205B636F6D70726573735D0A
    5D0A3634237B483473494141414141414141436A4E5573436B707A3764545543
    724A4B45704E56654943414E425746325951414141417D
}

&gt;&gt; print as text! data
REBOL [
     Options: [compress]
]
64#{H4sIAAAAAAAACjNUsCkpz7dTUCrJKEpNVeICANBWF2YQAAAA}

&gt;&gt; [body header]: load data
== [1 &lt;two&gt; "three"
]

&gt;&gt; body
== [1 &lt;two&gt; "three"
]

&gt;&gt; header
== make object! [
    Title: "Untitled"
    File: ~null~
    Name: ~null~
    Type: 'script
    Version: ~null~
    Date: ~null~
    Author: ~null~
    Options: [compress]
    Description: ~null~
]
</code></pre>
<h2><a name="p-6665-rebol2-didnt-have-it-red-doesnt-have-it-1" class="anchor" href="https://rebol.metaeducation.com#p-6665-rebol2-didnt-have-it-red-doesnt-have-it-1"></a>Rebol2 Didn't Have It, Red Doesn't Have It...</h2>
<p>Arguments that it helps with transmitting over networks don't hold up much these days, because the HTTP protocol itself does compression.</p>
<p>Plus, keeping scripts in compressed form is an annoying form of opaqueness on a language that's supposed to be about simplicity.</p>
<p>I've kept it around just because there were tests for it, and it exercised compression code (including showcasing a really bad design method of trying to decompress garbage to see if it was the raw compressed form, causing a crazy memory allocation).  But I'm not sure what the compelling use case for this feature is.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/is-the-script-compression-feature-necessary/2044">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/is-the-script-compression-feature-necessary/2044</link>
          <pubDate>Thu, 27 Jul 2023 23:20:14 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-2044</guid>
          <source url="https://rebol.metaeducation.com/t/is-the-script-compression-feature-necessary/2044.rss">Is The Script Compression Feature Necessary?</source>
        </item>
        <item>
          <title>Python Speedup Proposals</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>The original "CPython" implementation is in some ways similar to Rebol/Red... though these days Ren-C is more aligned with the stackless Python implementation...which is also written in C, but distinct from CPython.</p>
<p><em>(But Ren-C's design overall is a lot less comparable to anything, due to the number of very "alien" ideas in it, but that I think are what gives it more notable properties.)</em></p>
<p>In any case, despite running an interpreter loop and doing kind-of-what-Rebol-does, they've committed themselves to doing some speedup proposals and apparently it's paying off:</p>
<p><a href="https://devblogs.microsoft.com/python/python-311-faster-cpython-team/" class="inline-onebox">A Team at Microsoft is Helping Make Python Faster - Microsoft for Python Developers Blog</a></p>
<p>Some of their proposals involve JIT-compiling things (which they know won't work on restrictive platforms like iOS).  But they apparently have done a lot of tweaks besides that which have turned out beneficial.  Because it's a C interpreter there might be something applicable to be learned by looking at their "Stage 1" and "Stage 2" changes.</p>
<blockquote>
<h3>Stage 1 -- Python 3.10</h3>
<p>The key improvement for 3.10 will be an adaptive, specializing interpreter. The interpreter will adapt to types and values during execution, exploiting type stability in the program, without needing runtime code generation.</p>
<h3>Stage 2 -- Python 3.11</h3>
<p>This stage will make many improvements to the runtime and key objects. Stage two will be characterized by lots of "tweaks", rather than any "headline" improvement. The planned improvements include:</p>
<ul>
<li>Improved performance for integers of less than one machine word.</li>
<li>Improved peformance for binary operators.</li>
<li>Faster calls and returns, through better handling of frames.</li>
<li>Better object memory layout and reduced memory management overhead.</li>
<li>Zero overhead exception handling.</li>
<li>Further enhancements to the interpreter</li>
<li>Other small enhancements.</li>
</ul>
<h3>Stage 3 -- Python 3.12 (requires runtime code generation)</h3>
<p>Simple "JIT" compiler for small regions. Compile small regions of specialized code, using a relatively simple, fast compiler.</p>
<h3>Stage 4 -- Python 3.13 (requires runtime code generation)</h3>
<p>Extend regions for compilation. Enhance compiler to generate superior machine code.</p>
</blockquote>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/python-speedup-proposals/1992">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/python-speedup-proposals/1992</link>
          <pubDate>Sun, 27 Nov 2022 10:05:19 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1992</guid>
          <source url="https://rebol.metaeducation.com/t/python-speedup-proposals/1992.rss">Python Speedup Proposals</source>
        </item>
        <item>
          <title>Boot Footprint: Giant String Literal vs. Encap?</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>One thing you can do with C is embed literal data.  This is how R3-Alpha ships with its mezzanine functions "built in", the prep process stores everything in a big compressed array of bytes called (misleadingly) <strong><code>Native_Specs</code></strong>:</p>
<p><a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/core/b-init.c#L166" class="inline-onebox">rebol/src/core/b-init.c at 25033f897b2bd466068d7663563cd3ff64740b94 · rebol/rebol · GitHub</a></p>
<p>The name being <code>Native_Specs</code> might suggest it was the contents of <a href="https://github.com/rebol/rebol/blob/25033f897b2bd466068d7663563cd3ff64740b94/src/boot/natives.r">%natives.r</a>.  But it's actually a lot more, with glued-together source code... including all of the contents of the <a href="https://github.com/rebol/rebol/tree/master/src/mezz">%base-xxx.r, %sys-xxx.r, and %mezz-xxx.r</a> files.  So I renamed it to <code>Boot_Block_Compressed</code>.</p>
<p>But it doesn't embed the files as-is... it LOADs them and SAVEs them using an already-built version of R3.  This round-tripping removes the comments and normalizes the spacing.  It also actually scrambled it with CLOAK for whatever reason--a waste of time because you could read all the code with SOURCE if you felt like it.  :-/</p>
<p><em>(Ren-C doesn't use an old-R3's LOAD+SAVE to strip out comments, because it would lock down the format.  Your hands would be tied on adding or changing lexical forms in the sys/base/mezzanine.  So it has its own STRIPLOAD function that does a light stripping out of comments and spaces for this glue-files-together purpose)</em></p>
<h2><a name="p-6438-is-embedding-big-fat-c-constants-supported-by-the-standard-1" class="anchor" href="https://rebol.metaeducation.com#p-6438-is-embedding-big-fat-c-constants-supported-by-the-standard-1"></a>Is Embedding Big Fat C Constants Supported By The Standard?</h2>
<p>C compilers are only <em>required</em> to allow you to build in string literals that are <a href="https://stackoverflow.com/a/11488687">509 characters in C89, and 4095 characters in C99</a>.  They can allow more, but don't have to.</p>
<p>So I recall R3-Alpha having problems when you turn up <code>--pedantic</code> warning levels by using a syntax like:</p>
<pre><code>const char Native_Specs[] = "\x01\x02\x03...";
</code></pre>
<p>That warning went away when I changed it to:</p>
<pre><code>const unsigned char Boot_Block_Compressed[] = { 0x01, 0x02, 0x03 ...};
</code></pre>
<p>Regarding the problem of hitting length limits, Ren-C actually breaks things up a bit more...because each extension has its own constant declaration like this for its Rebol portion.</p>
<p>Because this code is decompressed and scanned once--and then tossed--there's probably a number of experiments that could be done.  What if the blob were loaded as mutable data, and then used as some kind of buffer for another purpose?  Is there some way to help hint to the OS that you really are only going to use the information only once so it will throw out the page from memory?  Or will the right thing happen to scan it and use it just once?</p>
<p>Long story short--it hasn't been a problem, even with the TCC build.  So it has been taken for granted that it works acceptably.</p>
<h2><a name="p-6438-but-would-encapping-be-better-2" class="anchor" href="https://rebol.metaeducation.com#p-6438-but-would-encapping-be-better-2"></a>But Would Encapping Be Better?</h2>
<p>One vision of how the boot would work is that it would only load enough to get de-encapping working.  Then the de-encapping would be how all the blobs for the "built-in" extensions were extracted.</p>
<p><em>This seems like an interesting vision,</em> because if someone gave you a big fat Ren-C and you wanted any skinnier version, you could basically ask it to cut everything out you don't want and give you a new EXE.  You could roll it up with any customizations you like.</p>
<p>But if you're using any "real" form of encapping (e.g. manipulating the resource portions of a Linux ELF file or a Windows PE file) this gets complicated.  And Ren-C's encap facilities are <a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb">written in usermode</a>...so that expects things like file I/O and PARSE of a BINARY!, etc.  I also assume that unzip facilities would be part of encapping.  So you need a reasonably runnable system just to get to that point.</p>
<p><strong>I've punted on worrying too much about this, because of the focus on the web build.</strong></p>
<p>It would be a bad investment of limited resources to handwrite and maintain encapping code in C, just so that encapping can be the means by which more of the bootstrap can be done with encap.</p>
<h2><a name="p-6438-script-code-is-easy-to-encap-exedll-code-is-not-3" class="anchor" href="https://rebol.metaeducation.com#p-6438-script-code-is-easy-to-encap-exedll-code-is-not-3"></a>Script Code Is Easy to Encap, EXE/DLL Code Is Not</h2>
<p>So the "easy" part would be changing the build to go in two steps.</p>
<p>The first step would make an r3-one.exe that is capable of augmenting itself with encapped data.  The second step would ask that r3 to fold in various scripts and resources to make an r3-two.exe that had more things in it...such as a console.</p>
<p>This isn't that far out to accomplish.  <strong>The hard part is when what you're encapping isn't script data, but compiled and executable C code...like bits from a DLL.</strong>  e.g. encapping "extensions".</p>
<p>What some people do in this situation is to actually glue the DLL file into the executable, but extract it to the filesystem and load the extracted version.  If you Google around for "using a DLL as an embedded resource" you'll find people who've done such things...but the answers you find will be from over a decade ago, because no one cares about how they ship such things anymore.</p>
<h2><a name="p-6438-making-encap-a-dependency-is-probably-unwise-4" class="anchor" href="https://rebol.metaeducation.com#p-6438-making-encap-a-dependency-is-probably-unwise-4"></a>Making Encap A Dependency Is Probably Unwise...</h2>
<p>It isn't going to be a terribly big win for bootstrap if it can't be used to pull out or put in extensions.</p>
<p>I don't think it's wise to pursue handcrafted C de-encapping.  In fact there's no way I'd be writing any kind of encap code right now if it weren't already made.  Kind of the only reason we have the usermode encapping around is because Atronix was using it, but I was trying to keep the feature but cut it out of the C.  It hasn't been tossed entirely because it functions as test code.</p>
<p>We <em>could</em> make a token two-step build (the phase one executable, that uses the phase one to build a phase two with encapped data in it).</p>
<p>But it seems what we might want more is an easy option to not build in encapping whatsoever, and have more control over options at build time than the current list of extensions.</p>
<p>For the limited audience looking at desktop builds--I imagine the answer will be that if you want a differently-sized r3.exe, you do it with a C compiler and ticking different boxes.  Or you build everything as a DLL and accept it's not all one file.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/boot-footprint-giant-string-literal-vs-encap/1977">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/boot-footprint-giant-string-literal-vs-encap/1977</link>
          <pubDate>Sat, 24 Sep 2022 01:57:39 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1977</guid>
          <source url="https://rebol.metaeducation.com/t/boot-footprint-giant-string-literal-vs-encap/1977.rss">Boot Footprint: Giant String Literal vs. Encap?</source>
        </item>
        <item>
          <title>Influences On Startup Time And Memory Use</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Right now it's not ideal to be focusing on things like startup time and memory use.  There are a lot of fundamental features being rethought--and recall that rules of optimizing code at the cost of clarity and flexibility of design are:</p>
<ul>
<li>
<p>Rule <span class="hashtag-raw">#1:</span> Don't Do It</p>
</li>
<li>
<p>Rule <span class="hashtag-raw">#2</span> (Experts Only) Don't Do It... Yet.</p>
</li>
</ul>
<p>...BUT, the issues can't be ignored forever.  And it's reasonable for one to ask why there's been a dramatic increase in boot time and memory use between the build being used for bootstrap and a current commit.</p>
<p>So it's worth having a thread here to track some of what's involved.</p>
<h2><a name="p-6414-encap-detection-1" class="anchor" href="https://rebol.metaeducation.com#p-6414-encap-detection-1"></a>ENCAP Detection</h2>
<p>By default we still run encap detection on all desktop builds, scanning the executable.  On Windows I think Shixin's version loads the whole binary into memory, and on Linux it still does quite a lot.</p>
<p>You can skip the detection by using <code>--no-encap</code>.</p>
<p><a href="https://github.com/metaeducation/ren-c/blob/master/scripts/encap.reb" class="inline-onebox">ren-c/scripts/encap.reb at master · metaeducation/ren-c · GitHub</a></p>
<p>But the encap and de-encapping tools will still be bundled in the executable.  They're not an extension, so if you don't want to pay for that...you need to entirely remove <a href="https://rebol.metaeducation.com/t/boot-footprint-giant-string-literal-vs-encap/1977">early-boot modules like encap and unzip</a> which are <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/src/main/prep-main.reb#L45">built in another way</a></p>
<p>Obviously platform-specific C code would be faster and lighter than PARSE.  And there was some before, but it entangled things in the core with FILE I/O...and it was dedicated finicky C for a purpose we're not really focusing on, especially in the web build.</p>
<p>The decision to move encapping to userspace tools was mine, and not something I regret.  But since we're not using it, all it's really doing is acting as a test.  I've made a separate thread to talk about the fate of Encap, and whether we should depend on it more or distance from it further:</p>
<p><a href="https://rebol.metaeducation.com/t/boot-footprint-giant-string-literal-vs-encap/1977" class="inline-onebox">Boot Footprint: Giant String Literal vs. Encap?</a></p>
<h2><a name="p-6414-a-big-cost-is-going-to-come-from-uparse-2" class="anchor" href="https://rebol.metaeducation.com#p-6414-a-big-cost-is-going-to-come-from-uparse-2"></a>A Big Cost Is Going To Come From UPARSE</h2>
<p>UPARSE right now is an elaborate exercise of the ability to build complex feature-filled dialects in userspace.  And it does so at great cost to the evaluator.</p>
<p>Of course the plan is to cut that down, because COMBINATORs are just functions.  They could be written as natives.  And even more importantly, the process of <em>combinating itself</em> needs to be native.</p>
<p>I have done some experiments with this:</p>
<p><a href="https://rebol.metaeducation.com/t/progress-on-nativizing-parser-combinators/1636" class="inline-onebox">Progress on Nativizing Parser Combinators</a></p>
<p>But those experiments are currently inactive, because the design needed more work.  And it's easier to churn through that work with userspace code.</p>
<p>What can we do about it? Well until UPARSE goes through an optimization phase, we can just use PARSE3 in boot...or at least for whatever subsetted codebase is in this metric.  The main thing is just to get it measured so we know how much of this is known UPARSE-ism vs. other unknowns.  I'm going to bet it's a lot...even though it's not used all that terribly much in boot, it's going to be big.</p>
<p>Cutting it out for the moment would at least help focus on the next bigger things.</p>
<h2><a name="p-6414-another-pain-point-is-going-to-be-getset-atop-pickpoke-3" class="anchor" href="https://rebol.metaeducation.com#p-6414-another-pain-point-is-going-to-be-getset-atop-pickpoke-3"></a>Another Pain Point Is Going to be GET+SET Atop PICK+POKE</h2>
<p>I spent quite a while working through what a GET and SET and PICK and POKE actually were.  Ultimately I concluded:</p>
<ul>
<li>
<p>GETs are just sequences of individual PICK steps (where a GET of a WORD! starts the chain with the binding of the word, and PICKs the word out of that object)</p>
</li>
<li>
<p>SETs are a sequence of PICK steps which are kept track of...followed by POKE.  That POKE can return nothing (in which case you're done) or it can return an adjusted value.  If the value needed to be adjusted that means it then gets POKE'd back into the cell back in the chain, and this ripples back so long as the bitpattern in cells need to be adjusted.</p>
</li>
</ul>
<p>I haven't gone back to this prototype and optimized it.  That means it quite literally is building evaluation chains of PICK and POKE every time it does tuple processing (what would be "path picking", e.g. variables out of objects).  I wasn't sure if this was the answer or not, so it seemed best to keep it general to be able to play with it.</p>
<p>It's tough to know how much "hardening" should be done on this.  It's nice to be able to hijack and hook and bend things.  I think I still want to consider it to be calls to PICK and POKE, but we can do those calls via frames built just for those functions...and not generic evaluation.  I'll have to look at it.</p>
<h2><a name="p-6414-each-extension-adds-memory-use-but-also-has-startup-code-4" class="anchor" href="https://rebol.metaeducation.com#p-6414-each-extension-adds-memory-use-but-also-has-startup-code-4"></a>Each Extension Adds Memory Use, But Also Has Startup Code</h2>
<p>By default the desktop includes every extension, even for making animated GIFs...as well as currently</p>
<p>If one wants to make a non-kitchen-sink test build of Ren-C...obviously use <code>debug: none</code>, and <a href="https://github.com/metaeducation/ren-c/blob/02d1ba2c6e2a8b5fc689d4d6684435ae369a528d/configs/default-config.r#L23">chopping extensions out with <strong><code>-</code></strong> instead of <strong><code>+</code></strong></a>, for starters.  Note that extensions can be <a href="https://github.com/metaeducation/ren-c/actions/runs/3056527403/jobs/4930783888#step:22:3">built as separate DLL/.so with <strong><code>*</code></strong></a></p>
<h2><a name="p-6414-other-factors-need-managing-on-a-case-by-case-basis-5" class="anchor" href="https://rebol.metaeducation.com#p-6414-other-factors-need-managing-on-a-case-by-case-basis-5"></a>Other Factors Need Managing On a Case-by-Case Basis</h2>
<p>Those would be among the only things that can be done without <em>some</em> attention to the C, which hasn't been vetted for this metric in years.  But it isn't a priority right at this exact moment--there are much more important things.</p>
<p><em>(If you want some of my general philosophy about why Ren-C will be competitive with R3-Alpha despite "increased complexity", then seeing some <a href="https://rebol.metaeducation.com/t/the-now-even-more-special-specialize/588">old stats on SPECIALIZE might be illuminating</a>)</em></p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/influences-on-startup-time-and-memory-use/1972">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/influences-on-startup-time-and-memory-use/1972</link>
          <pubDate>Sun, 18 Sep 2022 22:11:11 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1972</guid>
          <source url="https://rebol.metaeducation.com/t/influences-on-startup-time-and-memory-use/1972.rss">Influences On Startup Time And Memory Use</source>
        </item>
        <item>
          <title>Rethinking The Stale Bit: Invisibility In The Isotopic Age</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Every evaluation step is asked to target an output cell.  Before the evaluation, a single bit is set on that cell to say it is "stale".  So if the evaluation doesn't write to it--and confirms that wasn't a mistake--then the old result is left around to recover.</p>
<p>The ability to recover the previous result with the flip of a bit is used for invisibility.  It's used not just in the evaluator when it goes step by step, but also in things like ANY and ALL.</p>
<pre><code>&gt;&gt; 1 + 2 comment "in the evaluator"
== 3

&gt;&gt; all [1 + 2 comment "here too"]
== 3
</code></pre>
<p><em>Doing it this way has sacrificed some features.</em>  For instance, you can't make an "invisible" infix function:</p>
<pre><code>&gt;&gt; foo: infix func [left] [
       print ["Left was" left]
       return void
   ]

&gt;&gt; 304 1020 foo
Left was 1020
== 1020
</code></pre>
<p>The 1020 from the previous evaluation was used as an argument.  But after "consuming all its arguments" the product of FOO could not leave the 304 there.  If each evaluation step was to a temporary cell, that temporary cell could be used to fill the infix slot of FOO... and 304 could be left.</p>
<p><em>Doing it this way has also required acrobatics to accomplish non-negotiable features.</em>  The related problem of making it possible for an infix function to perceive voidness on the left hand side requires stale bit mechanics that aren't for the faint of heart... e.g. to differentiate these two cases:</p>
<pre><code>&gt;&gt; (else [print "Won't work"])
** Error

&gt;&gt; () else [print "Will work"]
Will work
</code></pre>
<h2><a name="p-6386-is-all-the-bit-fiddling-worth-it-vs-copying-1" class="anchor" href="https://rebol.metaeducation.com#p-6386-is-all-the-bit-fiddling-worth-it-vs-copying-1"></a>Is All The Bit-Fiddling Worth It Vs. Copying?</h2>
<p>Considering the small size of cells (4 platform pointers), the logic to test and clear the "stale" bit may seem to add overhead and complexity that isn't saving that much.  Instead, every evaluation could be done into a temporary slot...and then if not invisible, the 4 pointers could be moved.</p>
<p>This is actually a bit misleading--because copying cells is actually a bit more expensive in the general case.  Cell format flags have to be checked, bindings may need to be managed, and if a reference count mechanic is implemented this could make it all worse.</p>
<p>Less copying is desirable, and it seems neat to have achieved invisibility thus far without needing an extra eval per-eval-step.</p>
<blockquote>
<p><em>"So if the evaluation doesn't write to it--and confirms that wasn't a mistake..."</em></p>
</blockquote>
<p>This is one of the main reasons I've stuck with the current method.  It's useful for debug purposes to know if a native just forgot to write an output cell anyway.  So I figured: <em>"so long as the output cell is going to have a flag on it saying it hasn't been written to yet, why not make that flag able to coexist with the previous value...and hence avoid a mechanic of needing to copy every time?"</em></p>
<h2><a name="p-6386-but-isotopes-mean-its-time-for-change-2" class="anchor" href="https://rebol.metaeducation.com#p-6386-but-isotopes-mean-its-time-for-change-2"></a>But Isotopes Mean It's Time For Change</h2>
<p>Early on I observed that there was no way to get this to work:</p>
<pre><code>&gt;&gt; 1000 + 20 if true [comment "hi"]
== 1020  ; not possible
</code></pre>
<p>The IF had to produce something as a proxy for VOID that wasn't void... in order to signal a taken branch (we want THEN to run).</p>
<p>But even if that proxy was able to <em>decay</em> to a void state, it was too late.  It had overwritten the output.  Today that proxy is a parameter pack with a meta-void in it: <strong><code>~[~]~</code></strong>.</p>
<p>There's more stuff with parameter packs that <em>should</em> work, like this:</p>
<pre><code> &gt;&gt; 1000 + 20 [x @y]: pack [304 void]
 == 1020
</code></pre>
<p><em>And isotopic objects that represent lazy evaluations should be able to produce void, too.</em>  They're a proxy for behavior, and if you pick and choose behaviors that could be accomplished with a normal result that a REIFY method on a lazy object can't, you're saying they're not as powerful.</p>
<p><strong>These features tip the scales.</strong>  And really, the circuitous nature of void infix handling was already tipping them.</p>
<p>The concerns over copying are mostly addressed by something I'm calling "cell movement"; this means we can really get closer to the 4 platform pointer copies, because you're destroying the old cell in the process.  So if techniques like reference counting came along, you're not adding and removing them--you're just letting the new cell take over the resources of the old.</p>
<p>Plus, detecting whether a cell has been written to or not is a generic debug feature now that has easy coverage.</p>
<p><em>The stale bit is thus on the chopping block.</em>  So expect more robust void-related behavior coming soonish.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963</link>
          <pubDate>Thu, 08 Sep 2022 21:34:36 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1963</guid>
          <source url="https://rebol.metaeducation.com/t/rethinking-the-stale-bit-invisibility-in-the-isotopic-age/1963.rss">Rethinking The Stale Bit: Invisibility In The Isotopic Age</source>
        </item>
        <item>
          <title>Incomplete TRANSCODEs: Actually an Optimization Problem</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C has a multi-return interface for TRANSCODE.  Without /NEXT, you get the whole thing:</p>
<pre><code>&gt;&gt; transcode "abc def"
== [abc def]
</code></pre>
<p>With the /NEXT refinement, it will go one item at a time.  But the return convention is that you receive back a the remainder as the primary return result, and the transcoded value is the second:</p>
<pre><code>&gt;&gt; [pos value]: transcode/next "abc def"
== " def"

&gt;&gt; pos
== " def"

&gt;&gt; value
== abc
</code></pre>
<p>Of course, with multi-return you can ask for the overall return result to be the synthesized value:</p>
<pre><code>&gt;&gt; [pos @value]: transcode/next "abc def"
== abc
</code></pre>
<p>You don't even have to name things if you don't want them!</p>
<pre><code>&gt;&gt; [_ @]: transcode/next "abc def"
== abc
</code></pre>
<p>And you can just use a regular SET-WORD! to get just the primary result.</p>
<pre><code>&gt;&gt; pos: trancode/next "abc def"
== " def"
</code></pre>
<p><strong>You know that you're at the end of the input when it returns NULL.</strong>  This means there was no value synthesized, and you're done.</p>
<pre><code>&gt;&gt; [pos /value]: transcode/next ""
== ~null~  ; anti

&gt;&gt; pos
== ~null~  ; anti

&gt;&gt; value
== ~null~  ; anti
</code></pre>
<p>Writing foolproof loops to process items are a breeze:</p>
<pre><code>while [[utf8 /item]: transcode utf8]
    print mold item
]
</code></pre>
<p>The leading slash on <code>/item</code> is necessary when you want to accommodate the case where transcode didn't produce any item.  Because then it doesn't return a 2-parameter pack, it just returns a pure null.  This is required for clean interoperability with THEN and ELSE...because nulls in packs are considered to be "something" vs. "nothing".  Multi-return unpacking requires you to demonstrate consciousness when you are trying to unpack more items than you're getting, hence the slash is needed when trying to unpack a singular null into two slots.</p>
<p>On the plus side, if you are expecting that there must be a transcoded item, then you get a free check by eliminating the slash...it will then cause an error if the item isn't produced!</p>
<p><img src="https://rebol.metaeducation.com/images/emoji/twitter/+1.png?v=14" title=":+1:" class="emoji only-emoji" alt=":+1:" loading="lazy" width="20" height="20"></p>
<h2><a name="p-6311-this-runs-circles-around-red-and-r3-alpha-1" class="anchor" href="https://rebol.metaeducation.com#p-6311-this-runs-circles-around-red-and-r3-alpha-1"></a>This Runs Circles Around Red and R3-Alpha</h2>
<p>For starters: neither support strings as input--because the scanner is built for reading UTF-8 files...and both R3-Alpha and Red unpack strings into fixed-width encodings.  So if you have string input, you have to pay for a copy encoded as UTF-8 via TO BINARY!.  (<a href="https://rebol.metaeducation.com/t/realistically-migrating-rebol-to-utf8-everywhere/374">Ren-C's UTF-8 Everywhere</a> wins again!)</p>
<p>R3-Alpha unconditionally returns a block with the last element as a remainder, whether you ask for one item via /NEXT or not:</p>
<pre><code>r3-alpha&gt;&gt; transcode to binary! "abc def"
== [abc def #{}]

r3-alpha&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

r3-alpha&gt;&gt; transcode/next to binary! ""
== [#{}]
</code></pre>
<p>So if you were transcoding an entire input, you have to TAKE/LAST an always-empty binary off of the result.</p>
<p>But you are using /NEXT you have to PICK out the element from the start of the array and the remainder from the end.  But you need to notice the exception of no-value-produced where the block is length 1 instead of 2.</p>
<p>That's awkward, but as usual... <em>Red somehow manages to make an incompatible interface that is as much worse as it is better:</em></p>
<p>The better part is that if you don't ask for /NEXT you just get the block back, like in Ren-C:</p>
<pre><code>red&gt;&gt; transcode to binary! "abc def"
== [abc def]
</code></pre>
<p>But the /NEXT interface is outright broken:</p>
<pre><code>red&gt;&gt; transcode/next to binary! "abc def"
== [abc #{20646566}]

red&gt;&gt; transcode/next to binary! ""
== [[] #{}]
</code></pre>
<p>It might look better because you don't have to guess about which position to find the remainder in--it's always in the second slot.  But it has a fatal flaw: you can't distinguish the result state of scanning <code>"[]"</code> and any string with nothing but comments and whitespace.</p>
<p>Consider this very basic loop to scan one item at a time and print it:</p>
<pre><code>red&gt;&gt; utf8: to binary! "abc def"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print mold item
]
abc
def
</code></pre>
<p>You get two items.  But what if you had something that was--say--a comment:</p>
<pre><code>red&gt;&gt; utf8: to binary! "; I'm just a comment"

red&gt;&gt; while [not tail? utf8] [
     set [item utf8] transcode/next utf8
     print ["Item is:" mold item]
]
Item is: []
</code></pre>
<p>You get one spurious item.  (They chose BLOCK! for the item, but it wouldn't matter what it was--a NONE! would be just as bad, you're just losing the distinction between empty strings and <code>"#[none]"</code> then.)</p>
<p><strong>If I were prescribing a solution for Red I'd suggest approximating Ren-C's solution as closely as possible.</strong></p>
<p>When /NEXT is used have it take a variable to write the transcoded value into.  Then return the position.  If the scan turns out to have no product, return NONE.  For consistency with Ren-C you might set the transcoded value to NONE as well <em>(it doesn't matter, because the return of none signals whatever it is isn't meaningful...so use UNSET! if you want.)</em></p>
<pre><code>while [utf8: transcode/next utf8 'item] [
    print mold item
]
assert [none? item]  ; or unset, or whatever
</code></pre>
<p>Not as nice as the multi-returns, and you can't duck out of passing the variable if you aren't interested.  But... <img src="https://rebol.metaeducation.com/images/emoji/twitter/man_shrugging.png?v=14" title=":man_shrugging:" class="emoji" alt=":man_shrugging:" loading="lazy" width="20" height="20"></p>
<h2><a name="p-6311-ren-c-also-thrashes-r3-alpha-and-red-in-error-handling-2" class="anchor" href="https://rebol.metaeducation.com#p-6311-ren-c-also-thrashes-r3-alpha-and-red-in-error-handling-2"></a>Ren-C Also <em>Thrashes</em> R3-Alpha and Red In Error Handling</h2>
<p>Ren-C TRANSCODE has these potential behaviors:</p>
<ul>
<li>
<p>RETURN a BLOCK! (if plain TRANSCODE)</p>
</li>
<li>
<p>RETURN a PACK of the <strong>~[remainder value]~</strong> if TRANSCODE/NEXT) -or- RETURN NULL if no value was transcoded from the input (empty string, comments, just spaces, etc.)</p>
<ul>
<li>
<p>Having remainder as the primary return means you can check the default result in a loop for truthiness and loop easily using WHILE or whatever.</p>
</li>
<li>
<p>Returning pure NULL when no value is transcoded means you can react to there being nothing to transcode with THEN and ELSE, etc.</p>
</li>
</ul>
</li>
<li>
<p>It can do a "hard FAIL"</p>
<ul>
<li>
<p>This would happen if you asked something fundamentally incoherent...like asking to TRANSCODE an input that was non-UTF-8...like a GOB!, or something like that</p>
</li>
<li>
<p>Such errors are only interceptible by a special SYS.UTIL.RESCUE method--they are not supposed to be easy to gloss over and unlikely to have meaningful mitigation.  So only special sandboxing situations (like writing consoles that print out the error) are supposed to trap them.</p>
</li>
</ul>
</li>
<li>
<p>It can RETURN an <em>antiform ERROR!</em> ("raised error") if something went wrong in the transcoding process itself</p>
<ul>
<li>
<p>This would be something like a syntax error, like  if you asked <strong>transcode "a bc 1&amp;x def"</strong></p>
</li>
<li>
<p>These will be promoted to a hard FAIL if the immediate caller doesn't do something to specially process them.</p>
</li>
<li>
<p>You can casually ignore or intercept these, because you can be confident that it was a formal return result of the thing you just called--not some deeper problem like a random typo or other issue.</p>
</li>
</ul>
</li>
</ul>
<p>I won't rehash the entire <a href="https://rebol.metaeducation.com/t/fail-vs-return-raise-the-new-age-of-definitional-failures/1852">"why definitional errors are foundational"</a> post, but TRANSCODE was one of the first functions that had to be retrofitted to use them.</p>
<pre><code>&gt;&gt; transcode "a bc 1&amp;x def" except e -&gt; [print ["Error:" e.id]]
Error: scan-invalid
</code></pre>
<p><strong>The definitionality is extremely important!</strong>  I spent a long time today because in the bootstrap shim I had a variation of transcode...parallel to this in R3-Alpha:</p>
<pre><code>r3-alpha&gt;&gt; transcode: func [input] [
               prnit "My Transcode Wrapper"  ; oops, typo
               return transcode input
           ]

r3-alpha&gt;&gt; if not attempt [transcode to binary! "abc def"] [print "Bad input"]
Bad input
</code></pre>
<p><em><strong>But the input isn't bad!!!</strong></em>  This leads to a nightmare of trying to figure out what was going wrong.  Today's particular nightmare was when tinkering with the shim implementation of TRANSCODE.  A bug in the shim was leading to silently skipping work that should have been done, because the caller wanted to be tolerant of bad transcode input.</p>
<p><strong>There's simply no practical way of working on code of any complexity without something like definitional failures, and experience has proven this day after day.</strong></p>
<h2><a name="p-6311-getting-incomplete-results-via-r3-alphas-error-3" class="anchor" href="https://rebol.metaeducation.com#p-6311-getting-incomplete-results-via-r3-alphas-error-3"></a>Getting Incomplete Results Via R3-Alpha's /ERROR</h2>
<p>R3-Alpha offered this feature:</p>
<pre><code>/error -- Do not cause errors - return error object as value in place
</code></pre>
<p>The intended use is that you might want the partial input of what had been successfully scanned so far.  If the code went and raised an error, you could trap that error.  But you wouldn't have any of the scanned items.</p>
<p>It would put it any ERROR! as the next-to-last item in the block, with the remainder after that:</p>
<pre><code>&gt;&gt; transcode/error to binary! "a bc 1&amp;x def"
== [abc make error! [
    code: 200
    type: 'Syntax
    id: 'invalid
    arg1: "pair"
    arg2: "1&amp;x"
    arg3: none
    near: "(line 1) a bc 1&amp;x def"
    where: [transcode]
] #{20646566}]

&gt;&gt; to string! #{20646566}
== " def"  ; wait...why isn't 1&amp;x part of the "remainder"
</code></pre>
<p>It's clumsy to write the calling code (or to read it...testing to see if the next-to-last-item is an ERROR! and reacting to that.</p>
<p><em>(Also: What if there was some way to represent ERROR! values literally in source?  This would conflate with such a block that was valid...but just incidentally had an ERROR! and then a BINARY! in the last positions.)</em></p>
<p>But the thing that had me most confused about it was the remainder.  Notice above you don't get  <code>1&amp;x</code> as the start of the stuff it couldn't understand.</p>
<p>Was it trying to implement some kind of recoverable scan?  What would that even mean?  <img src="https://rebol.metaeducation.com/images/emoji/twitter/thinking.png?v=14" title=":thinking:" class="emoji" alt=":thinking:" loading="lazy" width="20" height="20"></p>
<p><strong>Ultimately I think this was just a leaking of an implementation detail as opposed to any reasonable attempt at recoverable scanner.</strong>  It only didn't tell you where the exact tail of the successfully scanned material was because it did not know.</p>
<p>The scanning position is based on token consumptions, and so if you started something like a block scan and it saw a <strong>[</strong> then it forgets where it was before that.  Then if something inside the block goes bad, it will just give you a remainder position somewhere inside that--<em>completely forgetting about how many nesting levels it was in</em>.</p>
<p>So what you were getting was a crappier implementation of scanning one by one, and remembering where you were before the last bad scan:</p>
<pre><code>pos: input
error: null
block: collect [
   while [true] [
       keep [pos @]: transcode pos else [
           break
       ] except e -&gt; [
           error: e
           break
       ]
   ]
]
</code></pre>
<p>That gives you a proper version, setting error if something happened and giving you the block intact.</p>
<h2><a name="p-6311-so-finally-we-see-its-an-optimization-problem-4" class="anchor" href="https://rebol.metaeducation.com#p-6311-so-finally-we-see-its-an-optimization-problem-4"></a>So Finally... We See It's An Optimization Problem</h2>
<p>Question is if there's some way of folding this into TRANSCODE, so it's doing the looping and collecting efficiently for you.  What would the interface be like that gave you back the error, and how would you know to remember to check it?</p>
<p>The problem is that when you return a raised definitional error from a function, that's the only thing you return.  How would you return partial results (and maybe a resumption position) as well?</p>
<p>A /TRAP refinement could cause another variation in how the return results are given:</p>
<pre><code>&gt;&gt; [error block]: transcode/trap "a bc"
; null

&gt;&gt; block
== [a bc]

&gt;&gt; [error block]: transcode/trap "a bc 1&amp;x def"
== make error! [...]

&gt;&gt; block
== [a bc]
</code></pre>
<p>Having the error be first seems good, lining up with TRAP.  Then the block as the second result.</p>
<p>That's not bad, but it would require some implementation reworking that I don't have time for.  Problem is that how the scanner is written now it clears all the stack out when an error gets raised, and there'd have to be some flag to tell it to persist the data stack accruals despite unwinding the level stack.  It's not rocket science it's just not important right now.</p>
<h2><a name="p-6311-answer-for-now-kill-off-error-5" class="anchor" href="https://rebol.metaeducation.com#p-6311-answer-for-now-kill-off-error-5"></a>Answer For Now: Kill Off /ERROR</h2>
<ul>
<li>
<p>The answer /ERROR has been giving back in error cases for the remainder is sketchy and conflates potential literal error scanning with a scanning error.</p>
</li>
<li>
<p>You can get the behavior 100% reliably just by intercepting errors going one transcode item at a time.</p>
<ul>
<li>Bear in mind that one-at-a-time is only going one <em>top-level</em> item at a time.  If you scan a block with 1000 items in it, that's one transcode step.  So we're not really talking about that many steps most of the time with regards to the scale of a file.</li>
</ul>
</li>
<li>
<p>This is a good opportunity to write tests of item-by-item scanning with error handling</p>
</li>
<li>
<p>Red added a bunch of refinements on transcode [/next /one /prescan /scan /part /into /trace], and they didn't pick up /error themselves</p>
</li>
</ul>
<p>Speaking of adding lots of refinements: I also want to get away in general from investments in weird C scanner code and hooks (<em>especially</em> if it's just an optimization).</p>
<p>What we should be investing in is more fluid mixture of PARSE of strings/binary with the scanner.  e.g. we should have ways of knowing what line number you're at during the parse for any combinator, and just generally pushing on that.  Adding TRANSCODE parameters up the wazoo isn't a winning strategy.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/incomplete-transcodes-actually-an-optimization-problem/1940">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/incomplete-transcodes-actually-an-optimization-problem/1940</link>
          <pubDate>Mon, 22 Aug 2022 15:09:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1940</guid>
          <source url="https://rebol.metaeducation.com/t/incomplete-transcodes-actually-an-optimization-problem/1940.rss">Incomplete TRANSCODEs: Actually an Optimization Problem</source>
        </item>
        <item>
          <title>Optimizing TRANSCODE Usage in String/Binary PARSE</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>As written, the DATATYPE! combinator in UPARSE may do wasteful value loading when operating on string input.</p>
<p>Consider this case.</p>
<pre><code>&gt;&gt; parse "[some big block ...] 10" [collect some [keep integer! | block!]]
== [10]
</code></pre>
<p><em>Pretty impressive that it works.</em>  (Red will only do this on BINARY! input, but Ren-C's UTF-8 everywhere allows it to do it on strings too!)</p>
<p>But at the combinator level, it's wasteful.  What happens is:</p>
<ul>
<li>
<p>Hitting the INTEGER! combinator, causing it to scan the next element, loading <strong><code>[some big block ...]</code></strong> as a series into memory.</p>
<ul>
<li>It then checks the type, notices it's not an integer, and the INTEGER! combinator gives back a rejection...so the BLOCK! combinator goes to the next alternate.</li>
</ul>
</li>
<li>
<p>It hits the BLOCK! combinator and scans the block again.</p>
<ul>
<li>
<p>This time it matches, so the parser returns success and the synthesized block</p>
</li>
<li>
<p><em>But the block isn't actually desired</em>, so it is thrown away</p>
</li>
</ul>
</li>
<li>
<p>The next iteration scans the INTEGER! and keeps it.</p>
</li>
</ul>
<h2><a name="p-6309-why-does-it-work-this-way-1" class="anchor" href="https://rebol.metaeducation.com#p-6309-why-does-it-work-this-way-1"></a>Why Does It Work This Way?</h2>
<p>It's based on TRANSCODE, and does basically exactly what I said:</p>
<pre><code>[item remainder]: transcode input except e -&gt; [return raise e]

if datatype != type of item [
    return raise ["Could not TRANSCODE" datatype "from input"]
]
return item
</code></pre>
<p>If we could pass in a datatype to TRANSCODE when using the /NEXT option (e.g. requesting a remainder, as we are above) then it could short-circuit and we wouldn't need that test.</p>
<h2><a name="p-6309-red-has-looked-at-this-kind-of-problem-2" class="anchor" href="https://rebol.metaeducation.com#p-6309-red-has-looked-at-this-kind-of-problem-2"></a>Red Has Looked At This Kind of Problem</h2>
<p>There are a bunch of new arguments to Red's TRANSCODE function:</p>
<pre><code>USAGE:
     TRANSCODE src

DESCRIPTION: 
     Translates UTF-8 binary source to values.
     Returns one or several values in a block. 

ARGUMENTS:
     src          [binary! string!]
     {UTF-8 input buffer; string argument will be UTF-8 encoded.}

REFINEMENTS:
     /next        =&gt; Translate next complete value (blocks as single value).
     /one         =&gt; Translate next complete value, returns the value only.
     /prescan     =&gt; Prescans only, do not load values. Returns guessed type.
     /scan        =&gt; Scans only, do not load values. Returns recognized type.
     /part        =&gt; Translates only part of the input buffer.
         length       [integer! binary!] "Length in bytes or tail position."
     /into        =&gt; Optionally provides an output block.
        dst          [block!] 
     /trace       =&gt; 
        callback     [function! [
                        event [word!]
                        input [binary! string!]
                        type [word! datatype!]
                        line [integer!]
                        token
                        return: [logic!]
                      ]] 

RETURNS:
    [block!]
</code></pre>
<p>I'm not sure exactly how useful the /PRESCAN option is (what good is a "guess" of the type?)  But the /SCAN option would offer some bit of efficiency.</p>
<p>It would mean instead of one call to TRANSCODE followed by a datatype test, there'd be two calls</p>
<ul>
<li>
<p>The first as TRANSCODE/SCAN to get the datatype (but not synthesize a value from it)</p>
</li>
<li>
<p>A second call to scan again and get the value</p>
</li>
</ul>
<p>We assume the idle mode of scanning without producing anything can be fast.</p>
<p>I would suggest the scan feature be <strong>transcode/types</strong> so it worked more generally, not just with /NEXT.</p>
<pre><code>&gt;&gt; transcode/types [1 a [b]]
== [#[datatype! integer!] #[datatype! word!] #[datatype! block!]]
</code></pre>
<p><sub><em>(When I figure out the story of datatypes, there are going to be a lot of forum posts fixing up the above ugly notation.)</em></sub></p>
<h2><a name="p-6309-but-what-about-the-synthesis-of-unused-values-3" class="anchor" href="https://rebol.metaeducation.com#p-6309-but-what-about-the-synthesis-of-unused-values-3"></a>But What About The Synthesis Of Unused Values?</h2>
<p>This is a bit of a pickle.  <em>We don't know if you're going to use the product or not.</em></p>
<p>UPARSE's design has values bubbling out the top, and no line of communication to be aware of whether what it produces will be used:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! group!] 
== (b)
</code></pre>
<p>You might think that when the block! rule is going to be run, UPARSE could notice it wasn't at the end and send some kind of signal to the BLOCK! combinator that it doesn't have to synthesize an output.  But there's no a-priori psychic power saying that GROUP! hasn't been configured to evaluate to void.  Until the combinator gets looked up and run, it's potentially the same situation as this:</p>
<pre><code>&gt;&gt; uparse "[a] (b)" [block! void] 
== [a]
</code></pre>
<h2><a name="p-6309-it-seems-we-have-two-choices-4" class="anchor" href="https://rebol.metaeducation.com#p-6309-it-seems-we-have-two-choices-4"></a>It Seems We Have Two Choices</h2>
<ol>
<li>
<p>We can assume that a plain DATATYPE! intends to synthesize a value, and use a different combinator to say you only want to match the type:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [scan block!]
== #[datatype! block!]  ; cheap (but useful) return value, no series synthesis

&gt;&gt; uparse "[a b c]" [block!]
== [a b c]
</code></pre>
</li>
<li>
<p>We can reverse it and say that by default it does the cheap thing, and you have to explicitly ask to get the expensive thing:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
== #[datatype! block!]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Looked at in isolation, it might seem like (2) would be the obvious winner.</p>
<p>The thorn is that this would be a pretty notable divergence from how array parsing works, which I would basically call non-negotiable:</p>
<pre><code>&gt;&gt; uparse [[a b c]] [x: block!]

&gt;&gt; x
== [a b c]
</code></pre>
<p>So is there actually an option 3?</p>
<ol start="3">
<li>
<p>Make lone datatype! an error, and have two distinct operations for transcoding:</p>
<pre><code>&gt;&gt; uparse "[a b c]" [block!]
** Error: On string input, use either TRANSCODE BLOCK! or SCAN BLOCK!

&gt;&gt; uparse "[a b c]" [transcode block!]
== [a b c]

&gt;&gt; uparse "[a b c]" [scan block!]
== [a b c]
</code></pre>
</li>
</ol>
<p>Urg.  That kind of sucks.</p>
<p><strong>I think the answer is to accept option (1) being suboptimal performance, allowing those who are performance-minded to tune it.</strong>  There's no overt harm by scanning things you throw away, it's just wasteful.</p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/optimizing-transcode-usage-in-string-binary-parse/1939">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/optimizing-transcode-usage-in-string-binary-parse/1939</link>
          <pubDate>Sun, 21 Aug 2022 19:10:10 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1939</guid>
          <source url="https://rebol.metaeducation.com/t/optimizing-transcode-usage-in-string-binary-parse/1939.rss">Optimizing TRANSCODE Usage in String/Binary PARSE</source>
        </item>
        <item>
          <title>&quot;Sub-Cell&quot; Addressing In DATE! and TIME!</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>As we are familiar, DATE! can have a TIME! component:</p>
<pre><code>&gt;&gt; d: now
== 21-Nov-2021/18:56:45-5:00 

&gt;&gt; type of d
== #[datatype! date!]

&gt;&gt; t: d.time
== 18:56:45

&gt;&gt; type of t
== #[datatype! time!]
</code></pre>
<p>Although TIME! can exist as a separate cell and value type, <em>a DATE! doesn't store a time cell inside of it</em>.  It packs the date and time information into a single cell.</p>
<p>Hence when you say <strong>d.time</strong> above, a <em>new</em> TIME! value has to be synthesized.  There's not a whole cell worth of time to hand a pointer back to...its woven into the bits of the DATE!.</p>
<p><strong>That might not sound like much of an issue, but it creates the problem I refer to as <em>"sub-cell addressing"</em>.</strong></p>
<p><em>If you've missed everything I've griped about with this so far</em>, it means that when you want to see a behavior like the following:</p>
<pre><code>&gt;&gt; d.time.hour: 12
== 12

&gt;&gt; date
== 21-Nov-2021/12:56:45-5:00  ; we want hour updated
</code></pre>
<p>We run into the problem that if <strong>d.time</strong> <em>synthesizes</em> a value, then a naive picking process of <strong>(d.time).hour: 12</strong> would only be able to manipulate the bits in the synthesized time.  That wouldn't change <strong>d</strong>.  <em>What the user actually wanted was to update the bits of a time that was folded into the implementation of the date.</em></p>
<h2><a name="p-5667-rebol-lacks-the-vocabulary-to-do-this-in-an-obvious-way-1" class="anchor" href="https://rebol.metaeducation.com#p-5667-rebol-lacks-the-vocabulary-to-do-this-in-an-obvious-way-1"></a>Rebol Lacks The Vocabulary To Do This In An Obvious Way</h2>
<p>The smallest units that Rebol speaks in terms of are the <strong><code>Cell</code></strong> and the <strong><code>Stub</code></strong>.</p>
<p><em>(If you need a refresher on these, my <a href="https://www.youtube.com/watch?v=6nsKTpArTCE">2019 conference video tech talk</a> explains them.)</em></p>
<p>While Stubs often hold allocation tracking information for series, they are also the size of two Cells (by design), and the pool which holds them can hold <strong><code>Pairing</code></strong> nodes that are two Cells that can be tracked by the GC just like any other Stub.</p>
<p><strong>It would appear we could be able to simplify matters if we changed the combination of DATE! and TIME! to point to a 2-cell node.</strong></p>
<pre><code>DATETIME! cell
[  ]                DATE!           TIME!
  --&gt; points to [ 21-Nov-2021 | 18:56:45-5:00 ]  (2 cells)
</code></pre>
<p><em>(Whether the "zone" is part of a time or lives in the datetime would depend on whether you wanted to write <code>d.zone: -5:00</code> or <code>d.time.zone: -5:00</code>, I don't know if it ever makes sense to speak of a time with a zone independent of a datetime or not.)</em></p>
<p>Breaking things up this way, we can say that <strong>d.time</strong> implicates a cell.  And we can have some operation that acts on a cell (let's say POKE) like:</p>
<pre><code> &gt;&gt; poke 18:56:45 'hour 12
 == 12:56:45
</code></pre>
<h2><a name="p-5667-hang-on-date-time-and-datetime-are-immediate-2" class="anchor" href="https://rebol.metaeducation.com#p-5667-hang-on-date-time-and-datetime-are-immediate-2"></a>Hang On: DATE!, TIME! (and DATETIME!) are IMMEDIATE!</h2>
<p>We still have a bit of a problem here with our smallest units of representation.  Presumably we don't want this:</p>
<pre><code>&gt;&gt; d1: 21-Nov-2021/18:56:45-5:00 

&gt;&gt; d2: d1

&gt;&gt; d1.time.hour: 12
== 12

&gt;&gt; d1
== 21-Nov-2021/12:56:45-5:00

&gt;&gt; d2
== 21-Nov-2021/12:56:45-5:00  ; don't want d2 to change (right?)
</code></pre>
<p>But we also don't want to be needlessly copying the 2-cell node each time a date is assigned.  So it would be a <em>copy-on-write</em> mechanic.</p>
<p>If we're working with a cell-based granularity, then we wind up in a somewhat similar situation to what we had before...where the tuple processing has to propagate backwards.  e.g. when you have the POKE that changes the cell bits for the TIME! to make a new TIME! cell, there has to be some memory going back to the DATETIME! in order to tell it to make a new node and write the cell into the copy.</p>
<p>Does framing this in terms of cells offer any benefit over letting the DATETIME! be a higher-level entity that does a more specific folding of the TIME! cell into its bits?  This is a question I've been trying to answer, and haven't had an easy time of answering.</p>
<p><strong>One thing it would do to use a cell-based protocol is that it could generalize properties that had flags on cells, such as being PROTECT'ed.</strong>  Without the picking protocol requiring each step to go through a cell, the system cannot fiddle these bits in a known way.  So just as the DATE! folds the TIME! into it in some arbitrary way, the protect bit would have to go through this through a complex protocol also.</p>
<p>What I do know is that my current generalized solution is rather complex and slow--and doesn't answer how to do things like PROTECT.  We're seeing a slowdown from many different angles and I am trying to figure out what the best tradeoff is in terms of simplicity and generality.  It's not easy.</p>
            <p><small>3 posts - 2 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/sub-cell-addressing-in-date-and-time/1765">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/sub-cell-addressing-in-date-and-time/1765</link>
          <pubDate>Thu, 02 Dec 2021 16:57:13 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1765</guid>
          <source url="https://rebol.metaeducation.com/t/sub-cell-addressing-in-date-and-time/1765.rss">&quot;Sub-Cell&quot; Addressing In DATE! and TIME!</source>
        </item>
        <item>
          <title>Simple Objects vs. What The People Want</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>Ren-C has a more streamlined version of how R3-Alpha implemented simple OBJECT!s, but it's really mostly the same <em>(though MODULE! has changed significantly)</em></p>
<p>An OBJECT! is just two parallel lists, which I have called the <strong>"keylist"</strong> and the <strong>"varlist"</strong>.</p>
<p>So if you say something like:</p>
<pre><code>obj: make object! [
    x: 1 + 2
    y: 10 + 20
]
</code></pre>
<p>You will get:</p>
<pre><code>keylist: {symbol(x) symbol(y)}
varlist: [*V0* 3 30]
</code></pre>
<p>The first slot in a varlist is used for some tracking information.  So:</p>
<ul>
<li><code>keylist[0]</code> is the key for <code>varlist[1]</code></li>
<li><code>keylist[1]</code> is the key for <code>varlist[2]</code></li>
</ul>
<h2><a name="p-5582-you-get-a-new-keylist-with-every-make-object-1" class="anchor" href="https://rebol.metaeducation.com#p-5582-you-get-a-new-keylist-with-every-make-object-1"></a>You Get A New Keylist With Every MAKE OBJECT!</h2>
<p>Nothing in the system goes around looking for common patterns in your object creation to notice that you've made several objects with the same keys.</p>
<pre><code>collect [
    count-up i 1000 [
        keep make object! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>You just made 1000 objects, and all of them have their own copy of the keylist <code>{symbol(X) symbol(Y)}</code>.  Ren-C made this overhead cost less than 1/4 as much as R3-Alpha, but it's still kind of lame.</p>
<p><strong>The only way you avoid making a new keylist is if you do object inheritance.</strong></p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20]
    ]
]
</code></pre>
<p>This time, there's 1000 objects all sharing a single keylist.</p>
<p><strong>If you expand keys at all, that will result in a new keylist...</strong></p>
<p>You spoil the optimization if you put anything additional in your derived object:</p>
<pre><code>point!: make object! [x: y: null]
collect [
    count-up i 1000 [
        keep make point! [x: i * 10, y: i * 20, z: i * 30]
    ]
]
</code></pre>
<p>There's no inheritance mechanism that makes use of the common sublist.  So this puts you at <em>1001</em> keylists, because your keylist for the original point! never gets used.</p>
<p><strong>Object Expansion via APPEND disconnects shared keylists</strong></p>
<p>R3-Alpha allowed you to add fields to an object.  If you did so, you would lose any sharing that it had taken advantage of before.</p>
<pre><code>p: make point! [x: 10 y: 20]  ; reuses point!'s keylist
append p [z: 30]  ; oop, not anymore...gets its own keylist.
</code></pre>
<p><strong>Comparisons Are Difficult</strong></p>
<p>Because there's no global mechanism of canonization of keylists, you get entirely different-looking objects by creating the fields in different orders.</p>
<pre><code>obj1: make object! [x: 10 y: 20]
obj2: make object! [y: 20 x: 10]
</code></pre>
<p>These objects have been considered to be not equal historically.  Because comparisons are done by walking the fields in order.  So obj1 &lt;&gt; obj2 in this case.</p>
<p>However, if you create an object via inheritance so it shares a keylist, that will standardize the order of the fields:</p>
<pre><code>point1: make point! [x: 10 y: 20]
point2: make point! [y: 20 x: 10]
</code></pre>
<p>Here we will have point1 = point2, since their shared keylist forces the order of x and y to whatever it was in POINT!.</p>
<h2><a name="p-5582-there-are-fancier-ways-of-dealing-with-this-2" class="anchor" href="https://rebol.metaeducation.com#p-5582-there-are-fancier-ways-of-dealing-with-this-2"></a>There Are Fancier Ways Of Dealing With This</h2>
<p><strong>If you're willing to say that the order of keys in objects shouldn't matter...</strong> then you can rethink the data structures to exploit commonalities in the patterns of keys that are created.</p>
<p>The V8 JavaScript engine approaches this with <strong><a href="https://richardartoul.github.io/jekyll/update/2015/04/26/hidden-classes.html">Hidden Classes</a></strong>.</p>
<p>But there's really always some other way of approaching the problem.  The way modules work in "Sea of Words" is an example of a structure that seems to work reasonably well for modules--but wouldn't work as well for lots of little objects.</p>
<h2><a name="p-5582-todays-frame-depends-on-this-non-fancy-way-3" class="anchor" href="https://rebol.metaeducation.com#p-5582-todays-frame-depends-on-this-non-fancy-way-3"></a>Today's FRAME! Depends On This Non-Fancy Way</h2>
<p>Right now, when a native runs it does so with a concept of the order of the arguments and refinements that gets baked into the C code directly.  IF knows that the condition is argument 1 and that the branch is argument 2, and it looks directly in slots 1 and 2 of the varlist of the frame to find those variables.</p>
<p>This is pretty foundational to the idea of the language, and is part of what gives it an appealing "simple-ness".</p>
<p>Ren-C has come along and permitted higher level mechanisms like specialization and adaptation, but everything is always getting resolved in a way that each step in a function's composition works on putting information into the exact numbered slot that the lower levels expect it to be in.</p>
<h2><a name="p-5582-binding-has-depended-on-this-non-fancy-way-4" class="anchor" href="https://rebol.metaeducation.com#p-5582-binding-has-depended-on-this-non-fancy-way-4"></a>Binding Has Depended On This Non-Fancy Way</h2>
<p>A premise in Rebol has been that you can make a connection between a variable and an object that has a key with the name of that variable, and once that connection is made it will last.  This rule is why there's been dodginess about deleting keys in objects or rearranging them...and why R3-Alpha permits adding new variables but not removing any.</p>
<pre><code> obj: make object! [x: 10 y: 20]
 code: [x + y]
 bind code obj
</code></pre>
<p>If you write something like the above, you are annotating the X inside of CODE with (obj field <span class="hashtag-raw">#1</span>), and the Y inside of CODE with (obj field <span class="hashtag-raw">#2</span>).  So nothing can happen with obj that can break that.</p>
<p><strong>This isn't strictly necessary.</strong>  It could have annotated X and Y with just (obj) and then gone searching each time it wanted to find it.  This would permit arbitrary rearrangement of OBJ, inserting and removing keys.  It could even remove X or Y and then tell you it couldn't find them anymore.</p>
<p>There are compromises as well.  The binding could be treated as a potentially fallible cache...it could look in that slot position (if it's less than the total keylist size) and see if the key matched.  If not, it could fall back on searching and then update with the slot where it saw the field.</p>
<p>(Of course this means you have to look at the keylist instead of just jumping to where you want to be in the varlist, and locality is such that they may not be close together; so having to look at the keylist <em>at all</em> will bring you a slowdown.)</p>
<h2><a name="p-5582-but-what-is-the-goal-here-5" class="anchor" href="https://rebol.metaeducation.com#p-5582-but-what-is-the-goal-here-5"></a>But What Is The Goal, Here?</h2>
<p>I've mentioned how the FRAME! design pretty much seems to go along well with the naive ordering of object fields.</p>
<p>I guess this is where your intuition comes in as to what represents "sticking to the rules of the game".  <em>And I think that hardcoding of positions into the executable of where to find the argument cells for natives is one of the rules.</em></p>
<p>This suggests that all functions hardcode the positions of their arguments--even usermode functions.  I'm okay with this.</p>
<p>So then we get to considering the question about OBJECT!.</p>
<ul>
<li>
<p>A lot of languages force you to predefine the structure of an object before creating instances.  And defining that structure is a good place to define its interfaces.  If Rebol wants to go in a more formal direction (resembling a Rust/Haskell/C++) then you might suggest you <em>always</em> make a base structure...and you can only have the fields named in it.</p>
</li>
<li>
<p>Other languages (like JavaScript) are more freeform, and as mentioned can look for the relationships after-the-fact.  Order of fields does not matter.</p>
</li>
</ul>
<p>It's clear that Rebol's userbase so far are people who would favor better implementation of the JavaScript model over going to more strictness.  I think there'd be a pretty good reception of a model where you could create objects with <strong>{...}</strong> and where fields could be added or removed as people saw fit.  If behind-the-scenes the system was optimizing access to those objects, that would presumably be preferable to this idea that you had to be responsible for declaring prototypes to get efficiencies (that would instantly disappear if you added another field).</p>
<p>But the mechanics definitely get more complicated.  :-/</p>
            <p><small>8 posts - 3 participants</small></p>
            <p><a href="https://rebol.metaeducation.com/t/simple-objects-vs-what-the-people-want/1745">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/simple-objects-vs-what-the-people-want/1745</link>
          <pubDate>Mon, 18 Oct 2021 06:45:29 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1745</guid>
          <source url="https://rebol.metaeducation.com/t/simple-objects-vs-what-the-people-want/1745.rss">Simple Objects vs. What The People Want</source>
        </item>
        <item>
          <title>Sea of Words is now in Beta (or something?)... Some Numbers</title>
          <dc:creator><![CDATA[hostilefork]]></dc:creator>
          <category>Optimization</category>
          <description><![CDATA[
            <p>I've gotten Sea of Words through the test suite and Bootstrap...and running the scenarios that have GitHub Actions (rebol-httpd, rebol-odbc, rebol-whitespacers.)  And I got it working in the web console too, of course!</p>
<p><em>(Note: If the web console seems sluggish these days don't blame Sea of Words...I started using UPARSE in it, and right now UPARSE is in full-on experimental mode.  It's a beast, so using it at all--even on trivial samples--will be resource intensive.)</em></p>
<h1><a name="p-5405-some-easy-to-get-numbers-for-the-moment-1" class="anchor" href="https://rebol.metaeducation.com#p-5405-some-easy-to-get-numbers-for-the-moment-1"></a>Some Easy-To-Get Numbers For The Moment</h1>
<p><strong>These numbers should be taken with a grain of salt...</strong> they don't measure everything, and some things shift around in ways that are hard to quantify.  But they're better than nothing.</p>
<p>(Note: I actually had to fix a bug in the evaluation count that was giving wild answers.  R3-Alpha lacked a double-check on its optimized method of incrementing the total evaluation count without needing to so every time in the loop...)</p>
<h2><a name="p-5405-prior-to-sea-of-words-2" class="anchor" href="https://rebol.metaeducation.com#p-5405-prior-to-sea-of-words-2"></a>Prior to Sea of Words</h2>
<p>Here is a report from a freshly booted desktop build on Windows, which avoids trying to read the executable into memory:</p>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 123702
    series-made: 53630
    series-freed: 27122
    series-expanded: 728
    series-bytes: 3460161
    series-recycled: 25447
    made-blocks: 33403
    made-objects: 191
    recycles: 3
]
</code></pre>
<h2><a name="p-5405-after-sea-of-words-3" class="anchor" href="https://rebol.metaeducation.com#p-5405-after-sea-of-words-3"></a>After Sea of Words</h2>
<pre><code>&gt;&gt; stats/profile
== make object! [
    evals: 138386
    series-made: 72860
    series-freed: 39697
    series-expanded: 706
    series-bytes: 3270669
    series-recycled: 20856
    made-blocks: 52054
    made-objects: 221
    recycles: 3
]
</code></pre>
<p>On the bright side, <strong>Sea of Words is not only a watershed moment in binding/modules, it's also saving 189K or so of memory</strong>, even just here in its first debut.</p>
<p>You may be wondering why there are so many more blocks.  The answer is that they're very <em>tiny</em> optimized stub blocks, used to hold individual variables that are floating in the "sea".  This is expected and purposeful.  As shown, the total memory use went down...</p>
<h2><a name="p-5405-this-is-really-only-a-beginning-4" class="anchor" href="https://rebol.metaeducation.com#p-5405-this-is-really-only-a-beginning-4"></a>This is Really Only A Beginning</h2>
<p>While the abilities that just came into play are a tremendous step for making a "real" and usable module system, there is significantly more left.  I'll be posting more on those issues after some <img src="https://rebol.metaeducation.com/images/emoji/twitter/zzz.png?v=14" title=":zzz:" class="emoji" alt=":zzz:" loading="lazy" width="20" height="20"></p>
            <p><small>1 post - 1 participant</small></p>
            <p><a href="https://rebol.metaeducation.com/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678">Read full topic</a></p>
          ]]></description>
          <link>https://rebol.metaeducation.com/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678</link>
          <pubDate>Sun, 22 Aug 2021 11:45:46 +0000</pubDate>
          <discourse:topicPinned>No</discourse:topicPinned>
          <discourse:topicClosed>No</discourse:topicClosed>
          <discourse:topicArchived>No</discourse:topicArchived>
          <guid isPermaLink="false">rebol.metaeducation.com-topic-1678</guid>
          <source url="https://rebol.metaeducation.com/t/sea-of-words-is-now-in-beta-or-something-some-numbers/1678.rss">Sea of Words is now in Beta (or something?)... Some Numbers</source>
        </item>
  </channel>
</rss>
